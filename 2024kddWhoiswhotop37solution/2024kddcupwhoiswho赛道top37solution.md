## 前言

感谢主办方举办的这次比赛,也感谢各位参赛选手的努力。

本次比赛官网:https://www.biendata.xyz/competition/ind_kdd_2024/

本次比赛完整数据集:https://www.kaggle.com/datasets/yunsuxiaozi/2024kddcupwhoiswho

Hello,大家好,我是yunsuxiaozi(匀速小子).

这是我第一次参加kddcup的比赛,我参加的是Whoiswho赛道的同名消歧比赛。我在A榜排名$31/117\approx 26.5\% (top27\%)$,B榜排名$37/53\approx 69.8\%(top70\%)$。

由于我在比赛中并不算前排,以下内容也不是什么好的solution,所以以下内容更多的是写给我自己看的,应该也没有什么人看吧。

本次比赛打到目前的排名我还是比较满意的,毕竟是第一次参加KDDcup的比赛。在我前面的大佬都是比赛圈著名的人物。和他们相比,我比赛经验不足,知识面也没他们广,所以比不过也是很正常的。

## top37solution介绍:

我用的是纯数据挖掘的做法。我这里总共有3个文件,一个是训练了一个论文分类器,一个是构造数据的文件,一个是模型训练和推理的文件。

我的fasttext的文件是训练了一个论文分类器,对论文按学科来进行分类。在https://www.biendata.xyz/forum/view_post_category/1034618/这个讨论中,它提到可以用大语言模型将论文的领域概括为一个特征,但是我不擅长搞大语言模型,就用这个数据集https://www.kaggle.com/datasets/Cornell-University/arxiv/data自己训练了一个论文分类器,根据论文的摘要给论文加上学科的特征。(注:这好像是违规操作,但是我排名靠后反正也没有奖金就不在乎这种小事了,训练出来的模型准确率只有80%,可能用大语言模型搞准确率还高,效果还好,如果读者有需要可以用大语言模型来搞。)

我的data文件根据训练数据和测试数据构造特征,这里构造特征还是花了很长时间,所以和模型的训练分了2个文件,不想浪费Kaggle的GPU。特征工程主要也是在这个文件中完成,具体做了什么也写的很详细了,有兴趣可以看看.

我的model文件就是模型的训练和推理,这里用了2组lgb模型来进行训练。在前面还做了一些特征工程是因为后续也在尝试修改代码以达到更好的效果。





## 几个疑惑的问题思考后的结果(以下仅代表个人观点)

### 1.要不要在lgb模型的训练中使用early_stop?

答:我的回答是不要。本次比赛的评估指标是对每个author单独计算auc值,然后加权。虽然比赛方提供的训练数据有14万条,但是具体到author只有700多个。如果用交叉验证,每折验证集的author只有100个左右,数据量太少了。这100个author无法代表真实世界中无穷无尽的样本,如果用这些样本评估而早停可能会欠拟合,所以我个人认为不要。

### 2.模型的训练过程中要不要使用加权,也就是用比赛的评估指标训练模型?

答:经过实验发现效果并不好,我也思考了原因,通过对每个author的oof的auc作图后发现,一个author的论文数量越少,auc相对来说越低,甚至低于0.5,而论文数量越少错误的数量也会相对越少。因此,错误多的样本论文数量多,论文数量多的样本auc好,如果给auc好的样本大权重,auc差的样本小权重,明显是不合理的。最终还是选择不用加权来训练模型。

### 3.要不要使用模型融合?

答:经过实验效果并不好,xgb和catboost效果比不上lgb模型,虽然模型融合线下oof提高了很多,但是提交上去还是不行,可能是线下选择blending权重的时候过拟合,最后还是选择只使用lgb模型。
