{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03f93274",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.00697,
     "end_time": "2024-06-29T03:08:29.211294",
     "exception": false,
     "start_time": "2024-06-29T03:08:29.204324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Created by <a href=\"https://github.com/yunsuxiaozi\">yunsuxiaozi</a> 2024/06/29\n",
    "\n",
    "\n",
    "#### 这是2024年高校大数据挑战赛的baseline,你可以在<a href=\"https://github.com/yunsuxiaozi/AI-and-competition\">AI and competition</a>里获取更多比赛的baseline。本次比赛官网如下:<a href=\"https://www.heywhale.com/org/bdc/competition/area/6662bf9a8d6c97c5d0c6bb10/leaderboard\">2024bdc</a>\n",
    "\n",
    "#### 本次比赛官方所给的baseline是最新的论文iTransformer,分数非常高,大部分的选手都已经采用,并且在它的基础上改进取得了更好的成绩。我这里使用2维的CNN来做一个简单的baseline,分数不高,仅供参考。\n",
    "\n",
    "\n",
    "#### 本次比赛的数据集已经被老师上传到Kaggle了,数据集链接如下:<a href=\"https://www.kaggle.com/datasets/bruceqdu/bigdata2024\">2024bdc dataset</a>。这里也直接在Kaggle上运行程序,如果想使用我的baseline可以将数据的路径改成你自己的路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d6327e",
   "metadata": {
    "papermill": {
     "duration": 0.004427,
     "end_time": "2024-06-29T03:08:29.223014",
     "exception": false,
     "start_time": "2024-06-29T03:08:29.218587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.导入必要的python库,这里不多做解释,注释也已经写的很清楚了。固定随机种子是为了保证模型可以复现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57c59ea4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T03:08:29.234153Z",
     "iopub.status.busy": "2024-06-29T03:08:29.233746Z",
     "iopub.status.idle": "2024-06-29T03:08:34.822077Z",
     "shell.execute_reply": "2024-06-29T03:08:34.820835Z"
    },
    "papermill": {
     "duration": 5.597729,
     "end_time": "2024-06-29T03:08:34.825504",
     "exception": false,
     "start_time": "2024-06-29T03:08:29.227775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np#矩阵运算与科学计算的库\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch#深度学习库,pytorch\n",
    "import torch.nn as nn#neural network,神经网络\n",
    "import torch.nn.functional as F#神经网络函数库\n",
    "import torch.optim as optim#一个实现了各种优化算法的库\n",
    "import gc#垃圾回收模块\n",
    "import warnings#避免一些可以忽略的报错\n",
    "warnings.filterwarnings('ignore')#filterwarnings()方法是用于设置警告过滤器的方法，它可以控制警告信息的输出方式和级别。\n",
    "\n",
    "#设置随机种子\n",
    "import random\n",
    "def seed_everything(seed):\n",
    "    torch.backends.cudnn.deterministic = True#将cuda加速的随机数生成器设为确定性模式\n",
    "    torch.backends.cudnn.benchmark = False#关闭CuDNN框架的自动寻找最优卷积算法的功能，以避免不同的算法对结果产生影响\n",
    "    torch.manual_seed(seed)#pytorch的随机种子\n",
    "    np.random.seed(seed)#numpy的随机种子\n",
    "    random.seed(seed)#python内置的随机种子\n",
    "seed_everything(seed=2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2832a524",
   "metadata": {
    "papermill": {
     "duration": 0.00481,
     "end_time": "2024-06-29T03:08:34.835399",
     "exception": false,
     "start_time": "2024-06-29T03:08:34.830589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.导入数据。这里有必要对数据进行详细的说明。\n",
    "\n",
    "- global_data:shape为(5848, 4, 9, 3850)\n",
    "\n",
    " 5848是时间,数据是2019年1月-2020年12月每3小时一次,2年总共有731天,每天有8次记录,故731\\*8=5848\n",
    " \n",
    " 4是4个特征,即:十米高度的矢量纬向风速10U，正方向为东方(m/s);十米高度的矢量经向风速10V，正方向为北方(m/s);两米高度的温度值T2M（℃）;均一海平面气压MSL（Pa）\n",
    " \n",
    " 9是9个网格,即:左上、上、右上、左、中、右、左下、下、右下.\n",
    " \n",
    " 3850就是3850个站点。\n",
    " \n",
    "- temp:shape为(17544, 3850, 1)\n",
    " \n",
    " 17544为5848\\*3,就是把数据变成1小时1次记录\n",
    " \n",
    " 3850 是3850个站点\n",
    " \n",
    " 1我感觉这个维度完全多余。\n",
    " \n",
    "- wind:shape为(17544, 3850, 1),解释和temp一样。\n",
    "\n",
    "\n",
    "#### 对数据的处理:\n",
    "\n",
    "- 首先需要将global_data在时间上变成1小时记录一次,这里由于就是baseline,所以将1个数据复制3次,如果后续改进,可以尝试用插值来搞? 9这个维度是方位,由于每个方位检测到的4个特征应该都是相关性比较高的,所以这里考虑直接对它们求平均处理。这样就将global_data变成(17544,4,1,3850)。由于后续的处理是根据每个站点前144个小时的特征预测下一个小时的特征,所以,这里将数据变成(3850,17544,4),具体怎么变见代码。\n",
    "\n",
    "- 对于temp和wind的处理就是将(17544,3850,1)变成(3850,17544,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc4bff99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T03:08:34.847716Z",
     "iopub.status.busy": "2024-06-29T03:08:34.846307Z",
     "iopub.status.idle": "2024-06-29T03:09:33.224982Z",
     "shell.execute_reply": "2024-06-29T03:09:33.223566Z"
    },
    "papermill": {
     "duration": 58.387764,
     "end_time": "2024-06-29T03:09:33.227889",
     "exception": false,
     "start_time": "2024-06-29T03:08:34.840125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_data.shape:(5848, 4, 1, 3850)\n",
      "global_data_hour.shape:(3850, 17544, 4)\n",
      "temp.shape:(3850, 17544, 1)\n",
      "wind.shape:(3850, 17544, 1)\n"
     ]
    }
   ],
   "source": [
    "path='/kaggle/input/bigdata2024/global/'\n",
    "#每3个小时,(温度、湿度、风速、风向),(左上、上、右上、左、中、右、左下、下、右下),3850个站点\n",
    "# (5848, 4, 9, x)\n",
    "global_data=np.load(path+\"global_data.npy\").mean(axis=-2,keepdims=True)\n",
    "print(f\"global_data.shape:{global_data.shape}\")\n",
    "#将3个小时变成1个小时  (5848*3, 4, 1, x)\n",
    "global_data_hour=np.repeat(global_data, 3, axis=0)\n",
    "    \n",
    "del global_data\n",
    "gc.collect()#手动触发垃圾回收,强制回收由垃圾回收器标记为未使用的内存\n",
    "\n",
    "# (5848*3, 4, 9, x)->(x,5848*3,36)\n",
    "global_data_hour=global_data_hour.transpose(3,0,1,2)\n",
    "#(x,5848*3,36)\n",
    "global_data_hour=global_data_hour.reshape(len(global_data_hour),-1,4)\n",
    "print(f\"global_data_hour.shape:{global_data_hour.shape}\")\n",
    "\n",
    "#每个小时,每个站点的温度 (17544, x, 1)->(x,17544,1)\n",
    "temp=np.load(path+\"temp.npy\").transpose(1,0,2)\n",
    "print(f\"temp.shape:{temp.shape}\")\n",
    "#每个小时,每个站点的风速 (17544, x, 1)->(x,17544,1)\n",
    "wind=np.load(path+\"wind.npy\").transpose(1,0,2)\n",
    "print(f\"wind.shape:{wind.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31677f9b",
   "metadata": {
    "papermill": {
     "duration": 0.004886,
     "end_time": "2024-06-29T03:09:33.238090",
     "exception": false,
     "start_time": "2024-06-29T03:09:33.233204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.数据的采样。\n",
    "\n",
    "#### 我们之前得到的特征是global_data:(3850,17544,4),temp和wind(3850,17544,1),我们这里的idea是用前144个时刻的所有特征预测下一个时刻的temp和wind,所以先拼接一个总特征(3850,17544,6),然后再构造X和y1,y2。由于全部数据的数据量巨大,所以这里对数据进行采样,采样的概率为0.0125,因为0.015我试过,超内存了。对数据进行标准化是神经网络必要的数据预处理,train_mean和train_std也要保存,因为提交的时候对测试数据也要进行同样的操作。最后数据处理完的维度X:(len(X),144\\*6),y1,y2:(len(X),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5984b296",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T03:09:33.250303Z",
     "iopub.status.busy": "2024-06-29T03:09:33.249872Z",
     "iopub.status.idle": "2024-06-29T03:11:02.343053Z",
     "shell.execute_reply": "2024-06-29T03:11:02.338170Z"
    },
    "papermill": {
     "duration": 89.139295,
     "end_time": "2024-06-29T03:11:02.382451",
     "exception": false,
     "start_time": "2024-06-29T03:09:33.243156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_feats.shape:(3850, 17544, 6)\n",
      "X.shape:(836534, 864),y1.shape:(836534, 1),y2.shape:(836534, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(x,17544,38)\n",
    "train_feats=np.concatenate((global_data_hour,temp,wind),axis=-1)\n",
    "print(f\"train_feats.shape:{train_feats.shape}\")\n",
    "#(x,17544,1),(x,17544,1)\n",
    "label1,label2=temp,wind\n",
    "\n",
    "def get_train_data(train_feats,label1,label2):#(x,17544,38),(x,17544,1),(x,17544,1)\n",
    "    X,y1,y2=[],[],[]\n",
    "    #每个站点\n",
    "    for si in range(train_feats.shape[0]):\n",
    "        for ti in range(train_feats.shape[1]-144):\n",
    "            if np.random.rand()<0.0125:#这里再进行采样\n",
    "                #si个站点ti:ti+144个时刻的所有特征\n",
    "                X.append(train_feats[si][ti:ti+144].reshape(-1))\n",
    "                y1.append(label1[si][ti+144])\n",
    "                y2.append(label2[si][ti+144])\n",
    "    X,y1,y2=np.array(X),np.array(y1),np.array(y2)\n",
    "    return X,y1,y2\n",
    "X,y1,y2=get_train_data(train_feats,label1,label2)\n",
    "train_mean=X.mean(axis=0)\n",
    "train_std=X.std(axis=0)\n",
    "np.save(\"train_mean.npy\",train_mean)\n",
    "np.save(\"train_std.npy\",train_std)\n",
    "X=(X-train_mean)/train_std\n",
    "print(f\"X.shape:{X.shape},y1.shape:{y1.shape},y2.shape:{y2.shape}\")\n",
    "del global_data_hour,temp,wind,train_feats\n",
    "gc.collect()#手动触发垃圾回收,强制回收由垃圾回收器标记为未使用的内存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2457e5b",
   "metadata": {
    "papermill": {
     "duration": 0.016848,
     "end_time": "2024-06-29T03:11:02.421534",
     "exception": false,
     "start_time": "2024-06-29T03:11:02.404686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.BaselineModel\n",
    "\n",
    "#### 这里搭建了一个简单的CNN作为baseline。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c211ad7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T03:11:02.474862Z",
     "iopub.status.busy": "2024-06-29T03:11:02.469753Z",
     "iopub.status.idle": "2024-06-29T03:11:02.521214Z",
     "shell.execute_reply": "2024-06-29T03:11:02.515412Z"
    },
    "papermill": {
     "duration": 0.087265,
     "end_time": "2024-06-29T03:11:02.530069",
     "exception": false,
     "start_time": "2024-06-29T03:11:02.442804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(BaselineModel,self).__init__()\n",
    "        self.conv=nn.Sequential(\n",
    "                  #1*24*36->16*24*36\n",
    "                  nn.Conv2d(in_channels=1,out_channels=16,kernel_size=3,stride=1,padding=1),\n",
    "                  nn.BatchNorm2d(16),\n",
    "                  #16*24*36->16*12*18\n",
    "                  nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "                  nn.GELU(),\n",
    "                  #16*12*18->32*12*18\n",
    "                  nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1,padding=1),\n",
    "                  nn.BatchNorm2d(32),\n",
    "                  #32*12*18->64*12*18\n",
    "                  nn.Conv2d(in_channels=32,out_channels=64,kernel_size=5,stride=1,padding=2),\n",
    "                  nn.BatchNorm2d(64),\n",
    "                  #64*12*18->64*6*9\n",
    "                  nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "                  nn.GELU(),\n",
    "                  #64*6*9->128*6*9\n",
    "                  nn.Conv2d(in_channels=64,out_channels=128,kernel_size=5,stride=1,padding=2),\n",
    "                  nn.BatchNorm2d(128),\n",
    "                  #128*6*9->128*3*4\n",
    "                  nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "                  nn.GELU(),\n",
    "        )\n",
    "        self.head=nn.Sequential(\n",
    "                nn.Linear(128*3*4,128),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(128,256),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(256,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.conv(x)\n",
    "        x=x.reshape(x.shape[0],-1)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9711e1",
   "metadata": {
    "papermill": {
     "duration": 0.019414,
     "end_time": "2024-06-29T03:11:02.567288",
     "exception": false,
     "start_time": "2024-06-29T03:11:02.547874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.模型的训练\n",
    "\n",
    "#### date就是说这是6月29日的第二次提交。这里之所以将864reshape成(1,24,36)只是想将数据搞得尽可能正方形一点,好使用CNN来卷积,模型训练使用的是MSE,评估指标使用的是官方的评估指标,由于是对temp和wind搞了2个模型,所以没有看最终指标的分数。可能是因为我用train_test_split存在数据泄露的情况,线下跑出来的指标好低,和线上完全对不上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bec8d8dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T03:11:02.605931Z",
     "iopub.status.busy": "2024-06-29T03:11:02.605059Z",
     "iopub.status.idle": "2024-06-29T09:49:19.548480Z",
     "shell.execute_reply": "2024-06-29T09:49:19.543938Z"
    },
    "papermill": {
     "duration": 23896.967762,
     "end_time": "2024-06-29T09:49:19.554212",
     "exception": false,
     "start_time": "2024-06-29T03:11:02.586450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,name:temp\n",
      "train_loss:39.63254928588867\n",
      "valid_loss:6.957380294799805,metric:0.042841896378927866\n",
      "epoch:1,name:temp\n",
      "train_loss:6.404951572418213\n",
      "valid_loss:4.364864349365234,metric:0.026876498608155396\n",
      "epoch:2,name:temp\n",
      "train_loss:3.9735267162323\n",
      "valid_loss:3.3389687538146973,metric:0.020568538182798583\n",
      "epoch:3,name:temp\n",
      "train_loss:3.416158437728882\n",
      "valid_loss:3.1424412727355957,metric:0.019356603391767334\n",
      "epoch:4,name:temp\n",
      "train_loss:3.0788538455963135\n",
      "valid_loss:2.7104780673980713,metric:0.01669682685350834\n",
      "epoch:5,name:temp\n",
      "train_loss:2.764535903930664\n",
      "valid_loss:3.2275211811065674,metric:0.0198797962722501\n",
      "epoch:6,name:temp\n",
      "train_loss:2.61907696723938\n",
      "valid_loss:3.0715019702911377,metric:0.018926934101076896\n",
      "epoch:7,name:temp\n",
      "train_loss:2.4271206855773926\n",
      "valid_loss:2.6960549354553223,metric:0.016605464776724428\n",
      "epoch:8,name:temp\n",
      "train_loss:2.1649467945098877\n",
      "valid_loss:2.8013415336608887,metric:0.017257515111020413\n",
      "epoch:9,name:temp\n",
      "train_loss:2.076241970062256\n",
      "valid_loss:2.7667369842529297,metric:0.01704180363123001\n",
      "epoch:0,name:wind\n",
      "train_loss:2.634408950805664\n",
      "valid_loss:1.8498305082321167,metric:0.293155302079099\n",
      "epoch:1,name:wind\n",
      "train_loss:1.574277400970459\n",
      "valid_loss:1.508863091468811,metric:0.23910268708680157\n",
      "epoch:2,name:wind\n",
      "train_loss:1.4538973569869995\n",
      "valid_loss:1.4260954856872559,metric:0.22600743773084295\n",
      "epoch:3,name:wind\n",
      "train_loss:1.382198452949524\n",
      "valid_loss:1.442484736442566,metric:0.22861156092489276\n",
      "epoch:4,name:wind\n",
      "train_loss:1.3340543508529663\n",
      "valid_loss:1.3945986032485962,metric:0.22101595006495284\n",
      "epoch:5,name:wind\n",
      "train_loss:1.3144776821136475\n",
      "valid_loss:1.4613676071166992,metric:0.23158700964706677\n",
      "epoch:6,name:wind\n",
      "train_loss:1.3275376558303833\n",
      "valid_loss:1.4318352937698364,metric:0.22690022686209754\n",
      "epoch:7,name:wind\n",
      "train_loss:1.2891463041305542\n",
      "valid_loss:1.4204884767532349,metric:0.22511097144144243\n",
      "epoch:8,name:wind\n",
      "train_loss:1.273412823677063\n",
      "valid_loss:1.396796464920044,metric:0.22136685331401934\n",
      "epoch:9,name:wind\n",
      "train_loss:1.2555644512176514\n",
      "valid_loss:1.3794482946395874,metric:0.2186393584855981\n"
     ]
    }
   ],
   "source": [
    "date='0629_2'\n",
    "def loss_fn(y_true,y_pred):#torch.tensor\n",
    "    return torch.mean((y_true-y_pred)**2)\n",
    "def metric(y_true,y_pred):#np.array\n",
    "    return np.mean((y_true-y_pred)**2)/np.var(y_true)\n",
    "\n",
    "def train(X,y,batch_size=1024,num_epochs=5,name='wind'):#传入的是np.array的数据,name是wind还是temp\n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.2, random_state=2024,shuffle=False)\n",
    "    #模型设置\n",
    "    model=BaselineModel()\n",
    "    #优化器设置\n",
    "    optimizer=optim.Adam(model.parameters(),lr=0.000025,betas=(0.5,0.999))\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"epoch:{epoch},name:{name}\")\n",
    "        #模型设置为训练状态\n",
    "        model.train()\n",
    "        #将梯度清空\n",
    "        optimizer.zero_grad()\n",
    "        #每次训练之前先打乱顺序\n",
    "        random_index=np.arange(len(train_X))\n",
    "        np.random.shuffle(random_index)\n",
    "        train_X,train_y=train_X[random_index],train_y[random_index]\n",
    "        train_loss=0.0\n",
    "        for idx in range(0,len(train_X),batch_size):\n",
    "            train_X1=torch.Tensor(train_X[idx:idx+batch_size]).reshape(-1,1,24,36)\n",
    "            train_y1=torch.Tensor(train_y[idx:idx+batch_size])\n",
    "            train_pred=model(train_X1)\n",
    "            loss=loss_fn(train_y1,train_pred)\n",
    "            #反向传播\n",
    "            loss.backward()\n",
    "            #优化器进行优化(梯度下降,降低误差)\n",
    "            optimizer.step()\n",
    "            train_loss+=loss\n",
    "        print(f\"train_loss:{train_loss/(len(train_X)//batch_size)}\")\n",
    "         #模型设置为评估模式\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss=0.00\n",
    "            valid_preds=np.zeros(len(valid_y))\n",
    "            for idx in range(0,len(valid_X),batch_size):\n",
    "                valid_X1=torch.Tensor(valid_X[idx:idx+batch_size]).reshape(-1,1,24,36)\n",
    "                valid_y1=torch.Tensor(valid_y[idx:idx+batch_size])\n",
    "                valid_pred=model(valid_X1)\n",
    "                loss=loss_fn(valid_y1,valid_pred)\n",
    "                valid_loss+=loss\n",
    "                valid_preds[idx:idx+batch_size]=valid_pred.detach().numpy().reshape(-1)\n",
    "            print(f\"valid_loss:{valid_loss/(len(valid_X)//batch_size)},metric:{metric(valid_y.reshape(-1),valid_preds)}\")\n",
    "        torch.cuda.empty_cache()\n",
    "    torch.save(model.state_dict(),f\"{date}{name}.pth\")\n",
    "train(X,y1,batch_size=128,num_epochs=10,name='temp')\n",
    "train(X,y2,batch_size=128,num_epochs=10,name='wind')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea200c4",
   "metadata": {
    "papermill": {
     "duration": 0.010082,
     "end_time": "2024-06-29T09:49:19.574455",
     "exception": false,
     "start_time": "2024-06-29T09:49:19.564373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6.模型的预测\n",
    "\n",
    "\n",
    "#### 这里就是线下按照测试数据的大小随机生成测试数据来跑一下,看看能不能跑通模型,这段代码也可以写入提交的index.py文件里。\n",
    "\n",
    "#### 由于我对代码也一直在改动,注释里的内容也不一定是现在的版本的注释,各位看懂就好,不要在意注释中的错误。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2fdbb11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-29T09:49:19.606817Z",
     "iopub.status.busy": "2024-06-29T09:49:19.603948Z",
     "iopub.status.idle": "2024-06-29T09:51:32.859984Z",
     "shell.execute_reply": "2024-06-29T09:51:32.858690Z"
    },
    "papermill": {
     "duration": 133.286768,
     "end_time": "2024-06-29T09:51:32.873149",
     "exception": false,
     "start_time": "2024-06-29T09:49:19.586381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cenn_data.shape:(71, 56, 4, 1, 60)\n",
      "temp_lookback.shape:(71, 168, 60, 1)\n",
      "wind_lookback.shape:(71, 168, 60, 1)\n",
      "input.shape:(102240, 864)\n",
      "input.shape:(102240, 864)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((71, 24, 60, 1), (71, 24, 60, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#测试数据\n",
    "x=60#60个观测站点\n",
    "#71个不连续的周,56(每3个小时测一次),观测4个特征,9个观测方位,x个站点\n",
    "cenn_data=np.random.randn(71,56,4,9,x).mean(axis=-2,keepdims=True)#真实情况是np.load加载的\n",
    "print(f\"cenn_data.shape:{cenn_data.shape}\")\n",
    "#将3个小时变成1个小时  (71,168,4,1,x)\n",
    "cenn_data_hour=np.repeat(cenn_data, 3, axis=1)\n",
    "cenn_data_hour=cenn_data_hour.transpose(0,4,1,2,3)#71*x*168*4*9\n",
    "cenn_data_hour=cenn_data_hour.reshape(71*x,168,4)\n",
    "\n",
    "\n",
    "#cenn/temp_lookback.npy 71个不连续的周  1个小时一次  x站上一周的温度\n",
    "temp_lookback=np.random.randn(71,168,x,1)\n",
    "print(f\"temp_lookback.shape:{temp_lookback.shape}\")\n",
    "temp_lookback=temp_lookback.transpose(0,2,1,3)#71,x,168,1\n",
    "temp_lookback=temp_lookback.reshape(71*x,168,1)\n",
    "#cenn/wind_lookback.npy 71个不连续的周  1个小时一次  x站上一周的风速\n",
    "wind_lookback=np.random.randn(71,168,x,1)\n",
    "print(f\"wind_lookback.shape:{wind_lookback.shape}\")\n",
    "wind_lookback=wind_lookback.transpose(0,2,1,3)#71,x,168,1\n",
    "wind_lookback=wind_lookback.reshape(71*x,168,1)\n",
    "\n",
    "#71*x个站点,168小时,38个特征\n",
    "total_feats=np.concatenate((cenn_data_hour,temp_lookback,wind_lookback),axis=-1)\n",
    "\n",
    "def predict(feats,model_name='temp.pth',batch_size=128):\n",
    "    model = BaselineModel()\n",
    "    model.load_state_dict(torch.load(model_name))\n",
    "    #预测24个小时的用的是71*x个站点(i:i-24)时刻的36个特征\n",
    "    data=[]\n",
    "    for i in range(24):\n",
    "        data.append(feats[:,i:i-24,].reshape(feats.shape[0],-1))\n",
    "    data=np.array(data)#24个小时,71*x个站点 【i:i-24时刻*36个特征】\n",
    "    #【24小时*71*x个站点】*【i:i-24时刻*36个特征】\n",
    "    data=data.reshape(-1,data.shape[-1])\n",
    "    print(f\"input.shape:{data.shape}\")\n",
    "    data=(data-train_mean)/train_std\n",
    "    #test_preds=【24小时*71*x个站点】*【预测值】\n",
    "    test_preds=np.zeros(len(data))\n",
    "    for idx in range(0,len(data),batch_size):\n",
    "        data1=torch.Tensor(data[idx:idx+batch_size]).reshape(-1,1,24,36)\n",
    "        test_preds[idx:idx+batch_size]=model(data1).detach().numpy().reshape(-1)\n",
    "    test_preds=test_preds.reshape(24,71,x,-1)\n",
    "    #71个周,24小时,x个站点的预测值\n",
    "    test_preds=test_preds.transpose(1,0,2,3)\n",
    "    return test_preds\n",
    "test_preds1=predict(feats=total_feats,model_name=f'{date}temp.pth',batch_size=128)\n",
    "test_preds2=predict(feats=total_feats,model_name=f'{date}wind.pth',batch_size=128)\n",
    "test_preds1.shape,test_preds2.shape"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5226298,
     "sourceId": 8711844,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24192.07278,
   "end_time": "2024-06-29T09:51:37.464462",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-29T03:08:25.391682",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
