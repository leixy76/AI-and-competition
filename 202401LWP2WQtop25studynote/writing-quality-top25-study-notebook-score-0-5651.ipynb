{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ddd9601",
   "metadata": {
    "papermill": {
     "duration": 0.006076,
     "end_time": "2024-01-11T08:48:42.461315",
     "exception": false,
     "start_time": "2024-01-11T08:48:42.455239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Created by yunsuxiaozi\n",
    "\n",
    "#### This article learned about <a href=\"https://www.kaggle.com/competitions/linking-writing-processes-to-writing-quality/discussion/466771\">the top 25 open-source solutions</a>,and organized and annotated the code for easy reading.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f357180e",
   "metadata": {
    "papermill": {
     "duration": 0.005307,
     "end_time": "2024-01-11T08:48:42.472418",
     "exception": false,
     "start_time": "2024-01-11T08:48:42.467111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b57eab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-11T08:48:42.486453Z",
     "iopub.status.busy": "2024-01-11T08:48:42.485596Z",
     "iopub.status.idle": "2024-01-11T08:48:46.194645Z",
     "shell.execute_reply": "2024-01-11T08:48:46.193595Z"
    },
    "papermill": {
     "duration": 3.719161,
     "end_time": "2024-01-11T08:48:46.197393",
     "exception": false,
     "start_time": "2024-01-11T08:48:42.478232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd#导入csv文件的库\n",
    "import numpy as np#进行矩阵运算的库\n",
    "import polars as pl#和pandas类似,但是处理大型数据集有更好的性能.\n",
    "#用于对一组元素计数,一个存在默认值的字典,访问不存在的值时抛出的是默认值\n",
    "from collections import Counter,defaultdict\n",
    "import re#用于正则表达式提取\n",
    "from scipy.stats import skew, kurtosis#统计分析和概率分布导入偏度和峰度\n",
    "\n",
    "#model\n",
    "from lightgbm import LGBMRegressor#导入lgbm回归器\n",
    "from catboost import CatBoostRegressor#catboost回归器\n",
    "from xgboost import XGBRegressor#导入XGB回归器\n",
    "\n",
    "#KFold是直接分成k折,StratifiedKFold还要考虑每种类别的占比\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#设置随机种子,保证模型可以复现\n",
    "import random\n",
    "seed=2024\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "import warnings#避免一些可以忽略的报错\n",
    "warnings.filterwarnings('ignore')#filterwarnings()方法是用于设置警告过滤器的方法，它可以控制警告信息的输出方式和级别。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9724fd",
   "metadata": {
    "papermill": {
     "duration": 0.005601,
     "end_time": "2024-01-11T08:48:46.208836",
     "exception": false,
     "start_time": "2024-01-11T08:48:46.203235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### The function used to obtain the content of the essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45cb29e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-11T08:48:46.222504Z",
     "iopub.status.busy": "2024-01-11T08:48:46.221720Z",
     "iopub.status.idle": "2024-01-11T08:48:46.235497Z",
     "shell.execute_reply": "2024-01-11T08:48:46.234303Z"
    },
    "papermill": {
     "duration": 0.023096,
     "end_time": "2024-01-11T08:48:46.237636",
     "exception": false,
     "start_time": "2024-01-11T08:48:46.214540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_Essays(df):\n",
    "    #传入的是其中一个id的论文,取出id\n",
    "    USER_ID = df[\"id\"].iloc[0]\n",
    "    #这个id和论文内容相关的几列\n",
    "    textInputDf = df[['activity', 'cursor_position', 'text_change']]\n",
    "    #对论文没有变化的动作不用管\n",
    "    currTextInput = textInputDf[textInputDf.activity != 'Nonproduction']\n",
    "    essayText = \"\"#开始重构论文内容.\n",
    "    for Input in currTextInput.values:#取出一个'activity', 'cursor_position', 'text_change'\n",
    "        #input[0]是这个id的activity\n",
    "        if Input[0] == 'Replace':\n",
    "            #text_change按照' => '分开 replaceTxt:[' qqq qqqqq ', ' ']\n",
    "            replaceTxt = Input[2].split(' => ')#应该是A=>B的操作\n",
    "            #input[1]是鼠标位置,是一个数字 鼠标位置-len()\n",
    "            #这是一个字符串的转换操作,由replaceTxt[0]转成replaceTxt[1] \n",
    "            essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] +essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "            continue\n",
    "        if Input[0] == 'Paste':#在Input[1]的位置粘贴Input[2]\n",
    "            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "            continue\n",
    "        if Input[0] == 'Remove/Cut':#删除剪切 在Input[1]的位置删除Input[2]\n",
    "            essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n",
    "            continue\n",
    "        #如果是Move from\n",
    "        if \"M\" in Input[0]:\n",
    "            #[284, 292] To [282, 290] 把[284, 292]这8行移动到[282,290]\n",
    "            croppedTxt = Input[0][10:]\n",
    "            #from和to的4个数字分开.\n",
    "            splitTxt = croppedTxt.split(' To ')\n",
    "            valueArr = [item.split(', ') for item in splitTxt]\n",
    "            moveData = (int(valueArr[0][0][1:]), \n",
    "                        int(valueArr[0][1][:-1]), \n",
    "                        int(valueArr[1][0][1:]), \n",
    "                        int(valueArr[1][1][:-1]))\n",
    "            #位置不相等,如果位置相等,等于什么都没有做\n",
    "            if moveData[0] != moveData[2]:\n",
    "                #行号小于 \n",
    "                if moveData[0] < moveData[2]:\n",
    "                    essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] +\\\n",
    "                    essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n",
    "                #行号大于\n",
    "                else:\n",
    "                    essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] +\\\n",
    "                    essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n",
    "            continue\n",
    "        #由于continue会跳过这个循环,这里应该是在Input[1]的位置'Input'了Input[2]\n",
    "        essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]     \n",
    "            \n",
    "    return USER_ID, essayText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b221bed4",
   "metadata": {
    "papermill": {
     "duration": 0.005592,
     "end_time": "2024-01-11T08:48:46.249122",
     "exception": false,
     "start_time": "2024-01-11T08:48:46.243530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### A function used to obtain features of words, sentences, and paragraphs in the essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52bc36fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-11T08:48:46.263977Z",
     "iopub.status.busy": "2024-01-11T08:48:46.263102Z",
     "iopub.status.idle": "2024-01-11T08:48:46.284677Z",
     "shell.execute_reply": "2024-01-11T08:48:46.283846Z"
    },
    "papermill": {
     "duration": 0.032181,
     "end_time": "2024-01-11T08:48:46.287092",
     "exception": false,
     "start_time": "2024-01-11T08:48:46.254911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "AGGREGATIONS = ['count','min','max','first','last', 'median','sum','std']\n",
    "\n",
    "#将论文转成单词\n",
    "def word_feats(df):\n",
    "    essay_df = df\n",
    "    #对空格,\\n,句号问号感叹号,逗号进行匹配,得到一个拆分后的列表.\n",
    "    essay_df['word'] = essay_df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!|\\\\,',x))\n",
    "    # essay1 [1,2,3] essay2[4,5] ->5行 essay1 1  // essay1 2 // essay1 3 // essay2 1 // essay2 2\n",
    "    essay_df = essay_df.explode('word')\n",
    "    #求出每个单词的长度\n",
    "    essay_df['word_len'] = essay_df['word'].apply(lambda x: len(x))\n",
    "    #去掉单词长度为0的数据,应该是标点符号和换行符之间,换行符与空格之间之类的.\n",
    "    word_df = essay_df[essay_df['word_len'] != 0]\n",
    "   \n",
    "    #根据id计算单词长度的统计学变量\n",
    "    word_agg_df = word_df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n",
    "    #比如('mean','word_len')->'mean_word_len'\n",
    "    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "    word_agg_df['id'] = word_agg_df.index\n",
    "#     for word_l in [5, 6, 7, 8, 9, 10, 11, 12]:\n",
    "#         #ge 就是Latex里>=的符号,筛选出word_len>=word_l的行,根据id进行统计,提取每个计数的第0行\n",
    "#         word_agg_df[f'word_len_ge_{word_l}_count'] = word_df[word_df['word_len'] >= word_l].groupby(['id']).count().iloc[:, 0]\n",
    "#         #如果有缺失值就填充为0\n",
    "#         word_agg_df[f'word_len_ge_{word_l}_count'] = word_agg_df[f'word_len_ge_{word_l}_count'].fillna(0)\n",
    "    #重置索引\n",
    "    word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "    return word_agg_df\n",
    "\n",
    "#句子特征\n",
    "def sent_feats(df):\n",
    "    essay_df = df#传入的df就是论文的df\n",
    "    #对句子按照. ? !进行拆分. 得到一个拆分后的列表.\n",
    "    essay_df['sent'] = essay_df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    # essay1 [1,2,3] essay2[4,5] ->5行 essay1 1  // essay1 2 // essay1 3 // essay2 1 // essay2 2\n",
    "    essay_df = essay_df.explode('sent')\n",
    "    #将换行符'\\n'变成空白字符 strip 去除行头和行尾的空白字符.\n",
    "    essay_df['sent'] = essay_df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "    #统计一下每个句子的长度 \n",
    "    essay_df['sent_len'] = essay_df['sent'].apply(lambda x: len(x))\n",
    "    #求一下每个句子单词的个数.\n",
    "    essay_df['sent_word_count'] = essay_df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "    #去掉那些句子长度为0的数据\n",
    "    df = essay_df[essay_df.sent_len!=0].reset_index(drop=True)\n",
    "    \n",
    "    #统计句子长度的统计学变量和每个句子词数的统计学变量\n",
    "    sent_agg_df = pd.concat(\n",
    "        [df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS), df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1\n",
    "    )\n",
    "    #比如('mean','sent_len')->'mean_sent_len'\n",
    "    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "    sent_agg_df['id'] = sent_agg_df.index\n",
    "    \n",
    "#     # New features intoduced here: https://www.kaggle.com/code/mcpenguin/writing-processes-to-quality-baseline-v2\n",
    "#     for sent_l in [50, 60, 75, 100]:\n",
    "#         #ge 就是Latex里>=的符号,筛选出sent_len>=sent_l的行,根据id进行统计,提取每个计数的第0行\n",
    "#         sent_agg_df[f'sent_len_ge_{sent_l}_count'] = df[df['sent_len'] >= sent_l].groupby(['id']).count().iloc[:, 0]\n",
    "#         #如果有缺失值就填充为0\n",
    "#         sent_agg_df[f'sent_len_ge_{sent_l}_count'] = sent_agg_df[f'sent_len_ge_{sent_l}_count'].fillna(0)\n",
    "    #重置索引\n",
    "    sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "    #一句话里词的个数的count,其实就是有多少句话,也就是sent_len的count.重复了,故去掉.\n",
    "    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "    #sent_len_count其实就是有多少句话,故rename.\n",
    "    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "    return sent_agg_df\n",
    "\n",
    "def parag_feats(df):\n",
    "    essay_df = df\n",
    "    #按照'\\n'划分成段落 [1,2,3]\n",
    "    essay_df['paragraph'] = essay_df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    #[论文1 [段落1 段落2,……]->[论文1 段落1 // 论文1 段落2]\n",
    "    essay_df = essay_df.explode('paragraph')\n",
    "    #统计段落的长度\n",
    "    essay_df['paragraph_len'] = essay_df['paragraph'].apply(lambda x: len(x)) \n",
    "    #统计每个段落的词数\n",
    "    essay_df['paragraph_word_count'] = essay_df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "    essay_df['paragraph_sent_count'] = essay_df['paragraph'].apply(lambda x: len(re.split('\\\\.|\\\\?|\\\\!',x)))\n",
    "    #将段落长度>2的文本留下来.\n",
    "    df = essay_df[essay_df.paragraph_len>2].reset_index(drop=True)\n",
    "\n",
    "    paragraph_agg_df = pd.concat(\n",
    "        [df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS), \n",
    "         df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS),\n",
    "         df[['id','paragraph_sent_count']].groupby(['id']).agg(AGGREGATIONS)\n",
    "        ], axis=1\n",
    "    ) \n",
    "    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "    paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "    #段落词数的数量就是段落的数量,也就是paragraph_len_count,段落句子数的数量同理,故drop\n",
    "    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\", \"paragraph_sent_count_count\"], inplace=True)\n",
    "    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n",
    "    return paragraph_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e892b749",
   "metadata": {
    "papermill": {
     "duration": 0.005593,
     "end_time": "2024-01-11T08:48:46.298681",
     "exception": false,
     "start_time": "2024-01-11T08:48:46.293088",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Text Readability Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b022ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-11T08:48:46.313041Z",
     "iopub.status.busy": "2024-01-11T08:48:46.312608Z",
     "iopub.status.idle": "2024-01-11T08:48:46.321604Z",
     "shell.execute_reply": "2024-01-11T08:48:46.320534Z"
    },
    "papermill": {
     "duration": 0.019137,
     "end_time": "2024-01-11T08:48:46.323823",
     "exception": false,
     "start_time": "2024-01-11T08:48:46.304686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#文本的自动可读性指数 旨在衡量文本的可理解性.输出是理解课文所需的美国年级水平的近似表示.\n",
    "#https://www.nhooo.com/note/qa0tpe.html\n",
    "#初步理解:相同词数的情况下,句子越少,说明句子相对来说会很长,越长越不容易理解.words/sentence就会越大.\n",
    "#字符数相同的情况下,词数越多,单词越短,短的单词可能简单,所以就好理解.characters/words变小.\n",
    "#数值小就好理解,数值大就不好理解.具体的公式可能用数据做过实验得出?\n",
    "def ARI(txt):\n",
    "    characters=len(txt)\n",
    "    words=len(re.split(' |\\\\n|\\\\.|\\\\?|\\\\!|\\,',txt))#空格,换行符,句号,问号,感叹号,逗号分开.\n",
    "    sentence=len(re.split('\\\\.|\\\\?|\\\\!',txt))#句号,问号,感叹号分开的句子.\n",
    "    ari_score=4.71*(characters/words)+0.5*(words/sentence)-21.43\n",
    "    return ari_score\n",
    "\"\"\"\n",
    "http://www.supermagnus.com/mac/Word_Counter/index.html\n",
    "McAlpine EFLAW© Test\n",
    "     (W + SW) / S\n",
    "McAlpine EFLAW© Readability\n",
    "     Scale:\n",
    "     1-20: Easy\n",
    "     21-25: Quite Easy\n",
    "     26-29: Mildly Difficult\n",
    "     ≥ 30: Very Confusing\n",
    "     S:total sentences\n",
    "     W:total words\n",
    "\"\"\"\n",
    "def McAlpine_EFLAW(txt):\n",
    "    W=len(re.split(' |\\\\n|\\\\.|\\\\?|\\\\!|\\,',txt))#空格,换行符,句号,问号,感叹号,逗号分开.\n",
    "    S=len(re.split('\\\\.|\\\\?|\\\\!',txt))#句号,问号,感叹号分开的句子.\n",
    "    mcalpine_eflaw_score=(W+S*W)/S\n",
    "    return mcalpine_eflaw_score\n",
    "\"\"\"\n",
    "https://readable.com/readability/coleman-liau-readability-index/\n",
    "\n",
    "=0.0588*L-0.296*S-15.8\n",
    "L是每100个单词有多少个字母,S是平均每100个单词有多少句子.\n",
    "\"\"\"\n",
    "def CLRI(txt):\n",
    "    characters=len(txt)\n",
    "    words=len(re.split(' |\\\\n|\\\\.|\\\\?|\\\\!|\\,',txt))#空格,换行符,句号,问号,感叹号,逗号分开.\n",
    "    sentence=len(re.split('\\\\.|\\\\?|\\\\!',txt))#句号,问号,感叹号分开的句子.\n",
    "    L=100*characters/words\n",
    "    S=100*sentence/words\n",
    "    clri_score=0.0588*L-0.296*S-15.8\n",
    "    return clri_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a875b8a3",
   "metadata": {
    "papermill": {
     "duration": 0.005716,
     "end_time": "2024-01-11T08:48:46.335544",
     "exception": false,
     "start_time": "2024-01-11T08:48:46.329828",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Extracting features from the text chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704ba671",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-11T08:48:46.349621Z",
     "iopub.status.busy": "2024-01-11T08:48:46.349143Z",
     "iopub.status.idle": "2024-01-11T08:48:46.364135Z",
     "shell.execute_reply": "2024-01-11T08:48:46.362882Z"
    },
    "papermill": {
     "duration": 0.025264,
     "end_time": "2024-01-11T08:48:46.366775",
     "exception": false,
     "start_time": "2024-01-11T08:48:46.341511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#获取文本块的特征.\n",
    "def get_text_chunk_features(df):\n",
    "    punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    #计算论文的长度\n",
    "    df['text_length'] = df['essay'].apply(len)\n",
    "\n",
    "    #计算换行符的个数.\n",
    "    df['num_newlines'] = df['essay'].apply(lambda x: x.count('\\n'))\n",
    "    #文本可读性的3个指标\n",
    "    df['automated_readability_index'] = df['essay'].apply(ARI)\n",
    "    df['mcalpine_eflaw'] = df['essay'].apply(McAlpine_EFLAW)        \n",
    "    df['coleman_liau'] = df['essay'].apply(CLRI)\n",
    "\n",
    "    #字符‘q’在总字符数中的占比,其他的可能是空格和标点符号\n",
    "    df['repetitiveness'] = df['essay'].apply(lambda x: x.count('q') / max(len(x), 1))\n",
    "\n",
    "    #平均词长度=所有的词的总长度/总词数\n",
    "    df['avg_word_length'] = df['essay'].apply(lambda x: sum(len(word) for word in x.split()) / max(1, len(x.split())))\n",
    "    #单词紧急多样性 set(x.split())=['qq','qqq','qqqq'……],len(x.split())是论文总词数\n",
    "    df['word_lexical_diversity'] = df['essay'].apply(lambda x: len(set(x.split())) / len(x.split()))\n",
    "        \n",
    "    #单引号和双引号的使用次数.\n",
    "    df['num_s_quotations'] = df['essay'].apply(lambda x: x.count(\"'\"))\n",
    "    df['num_d_quotations'] = df['essay'].apply(lambda x: x.count('\"'))\n",
    "    #问号、感叹号,逗号,句号的使用次数\n",
    "    df['qm_count'] = df['essay'].apply(lambda x: x.count('?'))\n",
    "    df['excm_count'] = df['essay'].apply(lambda x: x.count('!'))\n",
    "    df['comma_count'] = df['essay'].apply(lambda x: x.count(','))\n",
    "    df['dot_count'] = df['essay'].apply(lambda x: x.count('.'))\n",
    "    #冒号和分号的使用次数\n",
    "    df['num_prelist_count'] = df['essay'].apply(lambda x: x.count(':')) +\\\n",
    "                            df['essay'].apply(lambda x: x.count(\";\"))    \n",
    "    \n",
    "    #空白字符例如:空格,换行符,制表符后面有一个句号的错误.\n",
    "    df[\"space_n_dot_mistake\"] = df['essay'].apply(lambda x: len(re.findall(r'\\s\\.', x)))\n",
    "    #空白字符例如:空格,换行符,制表符后面有一个逗号的错误.\n",
    "    df[\"space_n_comma_mistake\"] = df['essay'].apply(lambda x: len(re.findall(r'\\s\\,', x)))\n",
    "    #空白字符例如:空格,换行符,制表符前面有一个句号的错误.\n",
    "    df[\"comma_n_nonspace_mistake\"] = df['essay'].apply(lambda x: len(re.findall(r'\\,\\S', x)))\n",
    "    #空白字符例如:空格,换行符,制表符前面有一个逗号的错误.\n",
    "    df[\"dot_n_nonspace_mistake\"] = df['essay'].apply(lambda x: len(re.findall(r'\\.\\S', x)))\n",
    "    #总共有几个错误,对错误做一个汇总.\n",
    "    df[\"total_punc_mistake\"] = (\n",
    "        df[\"space_n_dot_mistake\"] +\n",
    "        df[\"space_n_comma_mistake\"] +\n",
    "        df[\"comma_n_nonspace_mistake\"] +\n",
    "        df[\"dot_n_nonspace_mistake\"]\n",
    "    )\n",
    "    #标点符号犯错误的次数/4种标点符号的使用次数(问号和感叹号出现次数不多)\n",
    "    df[\"punc_mistake_ratio\"] = df[\"total_punc_mistake\"] / (df['qm_count'] +\n",
    "                                                           df['excm_count'] +\n",
    "                                                           df['comma_count'] +\n",
    "                                                           df['dot_count'])\n",
    "\n",
    "    #统计不同的单词种类. \n",
    "    df['unique_word_count'] = df['essay'].apply(lambda x: len(set(re.findall(r'\\w+', x.lower()))))\n",
    "    \n",
    "    #统计每种标点符号出现的次数和\n",
    "    df['punctuation_count'] = df['essay'].apply(lambda x: sum(x.count(p) for p in punctuation)) \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f6065a",
   "metadata": {
    "papermill": {
     "duration": 0.005793,
     "end_time": "2024-01-11T08:48:46.379045",
     "exception": false,
     "start_time": "2024-01-11T08:48:46.373252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Standardize the essays, and then call the function mentioned above. This is the feature extraction of the essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfa68d44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-11T08:48:46.393190Z",
     "iopub.status.busy": "2024-01-11T08:48:46.392775Z",
     "iopub.status.idle": "2024-01-11T08:48:46.402403Z",
     "shell.execute_reply": "2024-01-11T08:48:46.401203Z"
    },
    "papermill": {
     "duration": 0.019986,
     "end_time": "2024-01-11T08:48:46.405039",
     "exception": false,
     "start_time": "2024-01-11T08:48:46.385053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#对一段文本txt进行处理的函数\n",
    "def standardize_text(txt):\n",
    "    txt = re.sub(r'\\t' , '', txt)#将文本中的制表符替换成空字符串\n",
    "    txt = re.sub(r'\\n {1,}' , '\\n', txt)#换行符后面如果有空格,去掉\n",
    "    txt = re.sub(r' {1,}\\n' , '\\n', txt)#换行符前面有空格的话去掉\n",
    "    txt = re.sub(r'\\n{2,}' , '\\n', txt)#如果有2个以上的换行符,替换成单个换行符\n",
    "    txt = re.sub(r' {2,}' , ' ', txt)#如果有出现连续2个空格,替换成单个空格.\n",
    "    txt = txt.strip()#开头和结尾如果有空白字符去掉\n",
    "    return txt#返回经过处理的文本\n",
    "def TextProcessor(inp_df):\n",
    "\n",
    "    for rowi in range(len(inp_df)):\n",
    "        #如果一篇论文删除空格后什么都没有,替换为'q',好心假装你写了东西.\n",
    "        if inp_df.loc[rowi, \"essay\"].replace(\" \", \"\") == \"\":\n",
    "            inp_df.loc[rowi, \"essay\"] = \"q\"   \n",
    "    \n",
    "    #对传入的论文去掉一些多余的字符.\n",
    "    inp_df[\"essay\"] = inp_df[\"essay\"].apply(lambda x: standardize_text(txt=x))\n",
    "    \n",
    "    #获取文本块的相关特征.\n",
    "    print(\"creating complete features\")\n",
    "    inp_df = get_text_chunk_features(inp_df) \n",
    "    #获取文本的单词,句子,段落特征\n",
    "    wf_df = word_feats(inp_df)\n",
    "    sf_df = sent_feats(inp_df)\n",
    "    pf_df = parag_feats(inp_df)\n",
    "    #将论文\n",
    "    inp_df = inp_df.merge(wf_df, how=\"left\", on=\"id\")\n",
    "    inp_df = inp_df.merge(sf_df, how=\"left\", on=\"id\")\n",
    "    inp_df = inp_df.merge(pf_df, how=\"left\", on=\"id\")\n",
    "    #提取好特征,把论文,词语,句子,段落去掉\n",
    "    inp_df.drop([\"essay\", \"word\", \"sent\", \"paragraph\"],axis=1,inplace=True)\n",
    "    \n",
    "    return inp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27e95ac",
   "metadata": {
    "papermill": {
     "duration": 0.00589,
     "end_time": "2024-01-11T08:48:46.417184",
     "exception": false,
     "start_time": "2024-01-11T08:48:46.411294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### The following is feature extraction for logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "540b7f04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-11T08:48:46.432311Z",
     "iopub.status.busy": "2024-01-11T08:48:46.431696Z",
     "iopub.status.idle": "2024-01-11T08:48:46.466719Z",
     "shell.execute_reply": "2024-01-11T08:48:46.465466Z"
    },
    "papermill": {
     "duration": 0.046119,
     "end_time": "2024-01-11T08:48:46.469354",
     "exception": false,
     "start_time": "2024-01-11T08:48:46.423235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#数值列,activity,event和text_change的重要类别\n",
    "num_cols = ['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count', 'event_id']\n",
    "activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "text_changes = ['q', ' ', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "\n",
    "#df表格的colname列统计values的count.\n",
    "def count_by_values(df, colname, values):\n",
    "    #maintain_order=True保持原有顺序\n",
    "    fts = df.select(pl.col('id').unique(maintain_order=True))\n",
    "    for i, value in enumerate(values):\n",
    "        #根据每个id判断colname是不是value并统计个数,rename成colname_i_cnt\n",
    "        tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n",
    "        #加上这个特征\n",
    "        fts  = fts.join(tmp_df, on='id', how='left') \n",
    "    return fts\n",
    "#暂停状态的特征聚合\n",
    "def pause_stat_aggregator(df, prefix=\"iw\"):\n",
    "    temp = df.group_by(\"id\").agg(\n",
    "           #根据id计算time_diff的最大,中位数,均值,最小值,方差,sum\n",
    "           pl.max('time_diff').alias(f\"{prefix}_max_pause_time\"),\n",
    "           pl.median('time_diff').alias(f\"{prefix}_median_pause_time\"),\n",
    "           pl.mean('time_diff').alias(f\"{prefix}_mean_pause_time\"),\n",
    "           pl.min('time_diff').alias(f\"{prefix}_min_pause_time\"),\n",
    "           pl.std('time_diff').alias(f\"{prefix}_std_pause_time\"),\n",
    "           pl.sum('time_diff').alias(f\"{prefix}_total_pause_time\"),\n",
    "           # time_diff在(0.5,1) (1,2) (2,3) >3的count.\n",
    "           pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') <= 1)).count().alias(f\"{prefix}_pauses_half_sec\"),\n",
    "           pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') <= 2)).count().alias(f\"{prefix}_pauses_1_sec\"),\n",
    "           pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') <= 3)).count().alias(f\"{prefix}_pauses_2_sec\"),\n",
    "           pl.col('time_diff').filter(pl.col('time_diff') > 3).count().alias(f\"{prefix}_pauses_3_sec\")\n",
    "    )\n",
    "    return temp  \n",
    "\n",
    "def dev_feats(df):\n",
    "    \n",
    "    print(\"< Count by values features >\")\n",
    "    \n",
    "    #统计activity,text_change,down_event,up_event这几个类别型变量的count down_event和up_event基本一样.\n",
    "    feats = count_by_values(df, 'activity', activities)\n",
    "    feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left') \n",
    "    feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left') \n",
    "\n",
    "    print(\"< Numerical columns features >\")\n",
    "\n",
    "    #对action_time求和,对数值型变量求均值,方差,中位数,最小值,最大值,50%的数字 没有pl.mean(num_cols).suffix('_mean')和pl.quantile(num_cols, 0.5).suffix('_quantile')\n",
    "    temp = df.group_by(\"id\").agg(pl.sum('action_time').suffix('_sum'),pl.std(num_cols).suffix('_std'),\n",
    "                                 pl.median(num_cols).suffix('_median'), pl.min(num_cols).suffix('_min'),\n",
    "                                 pl.max(num_cols).suffix('_max'),\n",
    "                                 )\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "    \n",
    "    print(\"< Categorical columns features >\")\n",
    "    #类别型变量求了n_unique,加入特征.\n",
    "    temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n",
    "    feats = feats.join(temp, on='id', how='left') \n",
    "    \n",
    "    \n",
    "    print(\"< Creating pause features >\")   \n",
    "    #up_time向后移动命名为up_time_lagged\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    #按秒来计算停歇的时间 time_diff\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    #按空格键,句号,Enter(换行) ,分别是单词,句子,段落之间的标志.\n",
    "    temp = temp.with_columns((pl.col(\"up_event\") == \"Space\").alias(\"is_space\"))\n",
    "    temp = temp.with_columns((pl.col(\"up_event\") == \".\").alias(\"is_dot\"))\n",
    "    temp = temp.with_columns((pl.col(\"up_event\") == \"Enter\").alias(\"is_enter\"))\n",
    "\n",
    "    temp = temp.with_columns(\n",
    "        #根据id计算单词之间的累加和,如果缺失向后填充\n",
    "        pl.col(\"is_space\").cumsum().shift().backward_fill().over(\"id\").alias(\"word_id\"),\n",
    "        #根据id计算句子之间的累加和,如果缺失向后填充        \n",
    "        pl.col(\"is_dot\").cumsum().shift().backward_fill().over(\"id\").alias(\"sentence_id\"),   \n",
    "        #根据id计算段落之间的累加和,如果缺失向后填充   \n",
    "        pl.col(\"is_enter\").cumsum().shift().backward_fill().over(\"id\").alias(\"paragraph_id\"),    \n",
    "    )\n",
    "    \n",
    "    #选择 activity为Input和Remove/cut的数据\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "\n",
    "    #All in 全部数据计算time_diff的统计特征\n",
    "    iw_df = pause_stat_aggregator(df=temp, prefix=\"iw\")\n",
    "\n",
    "    # Between-words pauses   每个'id'的每个‘word_id’的第一个time_diff  计算统计特征\n",
    "    bww_df = temp.group_by(\"id\", \"word_id\").agg(pl.col(\"time_diff\").first())\n",
    "    bww_df = pause_stat_aggregator(df=bww_df, prefix=\"bww\")\n",
    "\n",
    "    # Between-sentences pauses 每个'id'的每个‘sentence_id’的第一个time_diff  计算统计特征\n",
    "    bws_df = temp.group_by(\"id\", \"sentence_id\").agg(pl.col(\"time_diff\").first())\n",
    "    bws_df = pause_stat_aggregator(df=bws_df, prefix=\"bws\")\n",
    "\n",
    "    # Between-paragraphs pauses 每个'id'的每个‘paragraph_id’的第一个time_diff  计算统计特征\n",
    "    bwp_df = temp.group_by(\"id\", \"paragraph_id\").agg(pl.col(\"time_diff\").first())\n",
    "    bwp_df = pause_stat_aggregator(df=bwp_df, prefix=\"bwp\")\n",
    "\n",
    "    #将所有特征拼接在一起\n",
    "    feats = (feats.join(iw_df, on=\"id\", how=\"left\")\n",
    "                 .join(bww_df, on=\"id\", how=\"left\")\n",
    "                 .join(bws_df, on=\"id\", how=\"left\")\n",
    "                 .join(bwp_df, on=\"id\", how=\"left\")\n",
    "            )\n",
    "    \n",
    "    feats=feats.to_pandas()\n",
    "    \n",
    "    return feats\n",
    "\n",
    "#统计每秒有几个['Input', 'Remove/Cut']的行为.\n",
    "def get_keys_pressed_per_second(logs):\n",
    "    #logs中为['Input', 'Remove/Cut']的event_id的个数\n",
    "    temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n",
    "    #每个id最小的down_time和最大的up_time\n",
    "    temp_df_2 = logs.groupby(['id']).agg(min_down_time=('down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n",
    "    #按照id融合在一起\n",
    "    temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n",
    "    #每秒有几个event_id\n",
    "    temp_df['keys_per_second'] = temp_df['keys_pressed'] / ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n",
    "    return temp_df[['id', 'keys_per_second']]\n",
    "\n",
    "    \n",
    "\n",
    "def burst_features(df, burst_type=\"p\"):\n",
    "    #up_time向后移动命名为up_time_lagged\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    #按秒来计算停歇的时间 time_diff\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    #如果P-burst,统计的是activity=Input,如果是R-burst,统计的是Remove/cut\n",
    "    if burst_type == \"p\":\n",
    "        temp = temp.with_columns(pl.col('activity').is_in(['Input']))\n",
    "    elif burst_type == \"r\":\n",
    "        temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n",
    "    #action_time按秒来计算\n",
    "    temp = temp.with_columns((pl.col('action_time') / 1000).alias(\"action_time_s\"))\n",
    "    #up_time按照秒来计算\n",
    "    temp = temp.with_columns((pl.col('up_time') / 1000).alias(\"up_time_s\"))\n",
    "    #增加f'{burst_type}_burst_group'列,activity!='Input'是nan.\n",
    "    #activity=='Input',有具体数值,上一个是Input的话,数值一样,上一个是nan的话,在上一次出现‘Input’的数值上+2.\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"activity\")).then(pl.col(\"activity\").rle_id()).alias(f'{burst_type}_burst_group'))\n",
    "    #去掉nan的行.就是去掉activity不是'Input'.\n",
    "    temp = temp.drop_nulls()\n",
    "\n",
    "    #每个id,每个f\"{burst_type}_burst_group\" ==1,==3,==5…… \n",
    "    temp = temp.group_by(\"id\", f\"{burst_type}_burst_group\").agg(\n",
    "                                    #出现了几个activity\n",
    "                                    pl.count('activity').alias(f'{burst_type}_burst_group_keypress_count'),\n",
    "                                    #action_time的求和,均值,方差\n",
    "                                    pl.sum('action_time_s').alias(f'{burst_type}_burst_group_timespent'),\n",
    "                                    pl.mean('action_time_s').alias(f'{burst_type}_burst_keypress_timespent_mean'),\n",
    "                                    pl.std('action_time_s').alias(f'{burst_type}_burst_keypress_timespent_std'),\n",
    "                                    #这个burst_goup的up_time的最小值和最大值,即:第一次和最后一次.\n",
    "                                    pl.min('up_time_s').alias(f'{burst_type}_burst_keypress_timestamp_first'),\n",
    "                                    pl.max('up_time_s').alias(f'{burst_type}_burst_keypress_timestamp_last')\n",
    "    )\n",
    "\n",
    "    #每个id都有很多f\"{burst_type}_burst_group\",故还需要求统计特征.\n",
    "    temp = temp.group_by(\"id\").agg(\n",
    "                #每个id出现连续的burst_group的求和,均值,方差,最大值.\n",
    "                pl.sum(f'{burst_type}_burst_group_keypress_count').alias(f'{burst_type}_burst_keypress_count_sum'),\n",
    "                pl.mean(f'{burst_type}_burst_group_keypress_count').alias(f'{burst_type}_burst_keypress_count_mean'),\n",
    "                pl.std(f'{burst_type}_burst_group_keypress_count').alias(f'{burst_type}_burst_keypress_count_std'),\n",
    "                pl.max(f'{burst_type}_burst_group_keypress_count').alias(f'{burst_type}_burst_keypress_count_max'),\n",
    "                #每个id出现连续的burst_group的action_time的总时间求和,均值,方差,最大值\n",
    "                pl.sum(f'{burst_type}_burst_group_timespent').alias(f'{burst_type}_burst_timespent_sum'),\n",
    "                pl.mean(f'{burst_type}_burst_group_timespent').alias(f'{burst_type}_burst_timespent_mean'),\n",
    "                pl.std(f'{burst_type}_burst_group_timespent').alias(f'{burst_type}_burst_timespent_std'),\n",
    "                pl.max(f'{burst_type}_burst_group_timespent').alias(f'{burst_type}_burst_timespent_max'),\n",
    "                #每个id出现连续的burst_group的action_time的平均时间求均值\n",
    "                pl.mean(f'{burst_type}_burst_keypress_timespent_mean').alias(f'{burst_type}_burst_keypress_timespent_mean'),\n",
    "                #每个id出现连续的burst_group的action_time的方差求平均\n",
    "                pl.mean(f'{burst_type}_burst_keypress_timespent_std').alias(f'{burst_type}_burst_keypress_timespent_std'),\n",
    "                #每个id的burst开始的时间,每个id的burst结束的时间.\n",
    "                pl.min(f'{burst_type}_burst_keypress_timestamp_first').alias(f'{burst_type}_burst_keypress_timestamp_first'),\n",
    "                pl.max(f'{burst_type}_burst_keypress_timestamp_last').alias(f'{burst_type}_burst_keypress_timestamp_last')\n",
    "    )\n",
    "    \n",
    "    #将polars对象转成pandas对象.\n",
    "    temp = temp.to_pandas()\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f78a880",
   "metadata": {
    "papermill": {
     "duration": 0.005984,
     "end_time": "2024-01-11T08:48:46.481465",
     "exception": false,
     "start_time": "2024-01-11T08:48:46.475481",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### This is a total function for feature extraction of logs and essays content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7fb8daf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-11T08:48:46.496038Z",
     "iopub.status.busy": "2024-01-11T08:48:46.495561Z",
     "iopub.status.idle": "2024-01-11T08:48:46.505710Z",
     "shell.execute_reply": "2024-01-11T08:48:46.504676Z"
    },
    "papermill": {
     "duration": 0.019678,
     "end_time": "2024-01-11T08:48:46.507774",
     "exception": false,
     "start_time": "2024-01-11T08:48:46.488096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Preprocessor(logs):\n",
    "    #将数据从pandas对象转成polars对象\n",
    "    pl_logs = pl.from_pandas(logs)\n",
    "\n",
    "    print(\"< Creating keys_pressed_per_second features >\")                \n",
    "    feat_df = get_keys_pressed_per_second(logs)#统计每秒有几个Input或者Remove/cut操作\n",
    "             \n",
    "    feat_df = feat_df.merge(dev_feats(df=pl_logs), how=\"left\", on=\"id\") \n",
    "    \n",
    "    print(\"< Creating PR-Burst features >\")  \n",
    "    #这里是P-burst和R-burst\n",
    "    feat_df = feat_df.merge(burst_features(df=pl_logs, burst_type=\"p\"), how=\"left\", on=\"id\")    \n",
    "    feat_df = feat_df.merge(burst_features(df=pl_logs, burst_type=\"r\"), how=\"left\", on=\"id\")    \n",
    "    \n",
    "    #获取每个id的论文\n",
    "    essays = logs.groupby(\"id\").apply(get_Essays)\n",
    "    essays = pd.DataFrame(essays.tolist(), columns=[\"id\", \"essay\"])\n",
    "    essay_feats = TextProcessor(essays)\n",
    "    feat_df=feat_df.merge(essay_feats,how=\"left\", on=\"id\")\n",
    "    \n",
    "    #每秒花费多少时间在P-burst和R-burst上\n",
    "    feat_df[\"p_bursts_time_ratio\"] = feat_df[\"p_burst_timespent_sum\"] / (feat_df[\"up_time_max\"] / 1000)\n",
    "    feat_df[\"r_bursts_time_ratio\"] = feat_df[\"r_burst_timespent_sum\"] / (feat_df[\"up_time_max\"] / 1000)\n",
    "    #action_time在写作总时间的占比情况\n",
    "    feat_df[\"action_time_ratio\"] = feat_df[\"action_time_sum\"] / feat_df[\"up_time_max\"]\n",
    "    #所有数据的停顿时间在总时间的占比\n",
    "    feat_df[\"pause_time_ratio\"] = feat_df[\"iw_total_pause_time\"] / (feat_df[\"up_time_max\"] / 1000)\n",
    "    #平均每秒发生几次停顿2~3秒\n",
    "    feat_df[\"pausecount_time_ratio\"] = feat_df[\"iw_pauses_2_sec\"] / (feat_df[\"up_time_max\"] / 1000)\n",
    "    #平均每秒写多少词数\n",
    "    feat_df['word_time_ratio'] = feat_df['word_count_max'] / (feat_df[\"up_time_max\"] / 1000)\n",
    "    #平均每个event写多少词数\n",
    "    feat_df['word_event_ratio'] = feat_df['word_count_max'] / feat_df[\"event_id_max\"] \n",
    "    #平均每秒多少个event\n",
    "    feat_df['event_time_ratio'] = feat_df['event_id_max']  / (feat_df[\"up_time_max\"] / 1000)\n",
    "    #文本长度/秒数,即:每秒写多少文本.\n",
    "    feat_df[\"text_length_time_ratio\"] = feat_df[\"text_length\"] / (feat_df[\"up_time_max\"] / 1000)\n",
    "    \n",
    "    return feat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79545b5",
   "metadata": {
    "papermill": {
     "duration": 0.005817,
     "end_time": "2024-01-11T08:48:46.519750",
     "exception": false,
     "start_time": "2024-01-11T08:48:46.513933",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76e2facf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-11T08:48:46.533725Z",
     "iopub.status.busy": "2024-01-11T08:48:46.533310Z",
     "iopub.status.idle": "2024-01-11T08:49:59.741955Z",
     "shell.execute_reply": "2024-01-11T08:49:59.740770Z"
    },
    "papermill": {
     "duration": 73.218709,
     "end_time": "2024-01-11T08:49:59.744547",
     "exception": false,
     "start_time": "2024-01-11T08:48:46.525838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_logs):8405898\n",
      "len(test_logs):6\n",
      "feature engineer\n",
      "< Creating keys_pressed_per_second features >\n",
      "< Count by values features >\n",
      "< Numerical columns features >\n",
      "< Categorical columns features >\n",
      "< Creating pause features >\n",
      "< Creating PR-Burst features >\n",
      "creating complete features\n",
      "< Creating keys_pressed_per_second features >\n",
      "< Count by values features >\n",
      "< Numerical columns features >\n",
      "< Categorical columns features >\n",
      "< Creating pause features >\n",
      "< Creating PR-Burst features >\n",
      "creating complete features\n",
      "drop unique_cols:['cursor_position_min', 'event_id_min']\n",
      "total_feats_counts:205\n"
     ]
    }
   ],
   "source": [
    "train_logs=pd.read_csv(\"/kaggle/input/linking-writing-processes-to-writing-quality/train_logs.csv\")\n",
    "print(f\"len(train_logs):{len(train_logs)}\")\n",
    "train_logs=train_logs.sort_values(by=['id', 'down_time'])\n",
    "# 重置索引\n",
    "train_logs = train_logs.reset_index(drop=True)\n",
    "# 根据'id'列进行分组，并为每个分组添加一个递增的序列\n",
    "train_logs['event_id'] = train_logs.groupby('id').cumcount() + 1\n",
    "\n",
    "train_scores=pd.read_csv(\"/kaggle/input/linking-writing-processes-to-writing-quality/train_scores.csv\")\n",
    "\n",
    "test_logs=pd.read_csv(\"/kaggle/input/linking-writing-processes-to-writing-quality/test_logs.csv\")\n",
    "print(f\"len(test_logs):{len(test_logs)}\")\n",
    "test_logs=test_logs.sort_values(by=['id', 'down_time'])\n",
    "# 重置索引\n",
    "test_logs = test_logs.reset_index(drop=True)\n",
    "# 根据'id'列进行分组，并为每个分组添加一个递增的序列\n",
    "test_logs['event_id'] = test_logs.groupby('id').cumcount() + 1\n",
    "\n",
    "print(\"feature engineer\")\n",
    "train_feats = Preprocessor(train_logs)\n",
    "train_feats = train_feats.merge(train_scores, how=\"left\", on=\"id\")\n",
    "test_feats = Preprocessor(test_logs)\n",
    "\n",
    "#找到只有唯一值的列,删掉\n",
    "keys=train_feats.keys().values\n",
    "unique_cols=[key for key in keys if train_feats[key].nunique()<2]\n",
    "print(f\"drop unique_cols:{unique_cols}\")\n",
    "train_feats = train_feats.drop(columns=unique_cols)\n",
    "test_feats = test_feats.drop(columns=unique_cols)\n",
    "#将里面正无穷和负无穷的值替换成缺失值\n",
    "train_feats.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "test_feats.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "train_feats.drop(['id'],axis=1,inplace=True)\n",
    "print(f\"total_feats_counts:{len(test_feats.keys().values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a8eb47",
   "metadata": {
    "papermill": {
     "duration": 0.007028,
     "end_time": "2024-01-11T08:49:59.758584",
     "exception": false,
     "start_time": "2024-01-11T08:49:59.751556",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "817a2211",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-11T08:49:59.774106Z",
     "iopub.status.busy": "2024-01-11T08:49:59.773704Z",
     "iopub.status.idle": "2024-01-11T08:56:24.977822Z",
     "shell.execute_reply": "2024-01-11T08:56:24.976569Z"
    },
    "papermill": {
     "duration": 385.214609,
     "end_time": "2024-01-11T08:56:24.979987",
     "exception": false,
     "start_time": "2024-01-11T08:49:59.765378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.6118700133535795 on fold 0\n",
      "RMSE: 0.5861380795510701 on fold 1\n",
      "RMSE: 0.5844454066432955 on fold 2\n",
      "RMSE: 0.6428243709476585 on fold 3\n",
      "RMSE: 0.5558994115824849 on fold 4\n",
      "RMSE: 0.6405881079204031 on fold 5\n",
      "RMSE: 0.602368118195772 on fold 6\n",
      "RMSE: 0.6018270789677977 on fold 7\n",
      "RMSE: 0.6190125123254278 on fold 8\n",
      "RMSE: 0.6222219843955334 on fold 9\n",
      "RMSE: 0.6096263459792947 on fold 0\n",
      "RMSE: 0.575155316866594 on fold 1\n",
      "RMSE: 0.5691166264262864 on fold 2\n",
      "RMSE: 0.6351918862502496 on fold 3\n",
      "RMSE: 0.5403459127268923 on fold 4\n",
      "RMSE: 0.6265794584132214 on fold 5\n",
      "RMSE: 0.6073534868811833 on fold 6\n",
      "RMSE: 0.5748708241160853 on fold 7\n",
      "RMSE: 0.6016549785519412 on fold 8\n",
      "RMSE: 0.6272928640137898 on fold 9\n",
      "RMSE: 0.6110827118754691 on fold 0\n",
      "RMSE: 0.5782560285668693 on fold 1\n",
      "RMSE: 0.5792202949010211 on fold 2\n",
      "RMSE: 0.6339162809680355 on fold 3\n",
      "RMSE: 0.5529262360763745 on fold 4\n",
      "RMSE: 0.6337667438729901 on fold 5\n",
      "RMSE: 0.6081065565269683 on fold 6\n",
      "RMSE: 0.5772173306951736 on fold 7\n",
      "RMSE: 0.598410140714516 on fold 8\n",
      "RMSE: 0.6247883019541133 on fold 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>score_lgb</th>\n",
       "      <th>score_cat</th>\n",
       "      <th>score_xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.465509</td>\n",
       "      <td>1.337883</td>\n",
       "      <td>1.252754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2222bbbb</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.451861</td>\n",
       "      <td>1.305795</td>\n",
       "      <td>1.251406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4444cccc</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.450663</td>\n",
       "      <td>1.316733</td>\n",
       "      <td>1.255507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  score  score_lgb  score_cat  score_xgb\n",
       "0  0000aaaa    3.5   1.465509   1.337883   1.252754\n",
       "1  2222bbbb    3.5   1.451861   1.305795   1.251406\n",
       "2  4444cccc    3.5   1.450663   1.316733   1.255507"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#这里创建了lgbm,cat,xgb模型\n",
    "def make_model():\n",
    "    \n",
    "    #大佬找好的参数,这里不做改动\n",
    "    cat_params = {'learning_rate': 0.024906985231770738, 'depth': 5, \n",
    "                  'l2_leaf_reg': 3.7139894959529283, 'subsample': 0.18527466886647015, \n",
    "                  'colsample_bylevel': 0.6552973951000719, 'min_data_in_leaf': 93,\n",
    "                \"silent\": True,\"iterations\": 1000, \"random_state\": seed,\"use_best_model\":False\n",
    "                 }\n",
    "    \n",
    "    lgb_params={'reg_alpha': 1.0894488472899402, 'reg_lambda': 6.290929934336985,\n",
    "                'colsample_bytree': 0.6218522907548012, 'subsample': 0.9579924238280629, \n",
    "                'learning_rate': 0.0027076430412427566, 'max_depth': 8, 'num_leaves': 947, \n",
    "                 'min_child_samples': 57,'n_estimators': 2500,'metric': 'rmse',\n",
    "                'random_state': seed,'verbosity': -1,'force_col_wise': True\n",
    "                }\n",
    "    \n",
    "    xgb_params={'max_depth': 2, 'learning_rate': 0.009998236038809146,\n",
    "                'n_estimators': 1000, 'min_child_weight': 17,\n",
    "                'gamma': 0.1288249858838246, 'subsample': 0.5078057280148618,\n",
    "                'colsample_bytree': 0.7355762136239921, 'reg_alpha': 0.670956206987811,\n",
    "                'reg_lambda': 0.06818351284100388, 'random_state': seed\n",
    "               }\n",
    "    \n",
    "    model1 = LGBMRegressor(**lgb_params)\n",
    "    \n",
    "    model2 = CatBoostRegressor(**cat_params)\n",
    "    \n",
    "    model3 = XGBRegressor(**xgb_params)\n",
    "    \n",
    "    models = []\n",
    "    models.append((model1, 'lgb'))\n",
    "    models.append((model2, 'cat'))\n",
    "    models.append((model3, 'xgb'))\n",
    "    \n",
    "    return models\n",
    "#评估指标是RMSE\n",
    "def RMSE(y_true,y_pred):\n",
    "    return np.sqrt(np.mean((y_true-y_pred)**2))\n",
    "\n",
    "X=train_feats.drop(['score'],axis=1)\n",
    "y=train_feats['score']\n",
    "\n",
    "models_and_errors_dict = {}\n",
    "\n",
    "y_hats = dict()\n",
    "\n",
    "#设置submission_df,id和score\n",
    "submission_df = pd.DataFrame(test_feats['id'])\n",
    "submission_df['score'] = 3.5#如果报错,将预测结果设置为3.5\n",
    "\n",
    "#取出test_feats中所有列\n",
    "X_unseen = test_feats.drop(['id'],axis=1).copy()\n",
    "\n",
    "num_folds=10\n",
    "\n",
    "for model, model_type in make_model():\n",
    "    \n",
    "    oof_pred=np.zeros((len(y)))\n",
    "    \n",
    "    y_hats[model_type] = []#某个model的预测结果\n",
    "        \n",
    "    #10折交叉验证\n",
    "    skf = StratifiedKFold(n_splits=num_folds,random_state=seed, shuffle=True)\n",
    "\n",
    "    for fold, (train_index, valid_index) in (enumerate(skf.split(X, y.astype(str)))):\n",
    "\n",
    "        # Split data into train and test sets\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[valid_index]\n",
    "\n",
    "        X_train_copy, X_test_copy = X_train.copy(), X_test.copy()\n",
    "\n",
    "        model.fit(X_train_copy, y_train)\n",
    "\n",
    "        #做预测\n",
    "        y_hat = model.predict(X_test_copy)\n",
    "        \n",
    "        oof_pred[valid_index]=y_hat\n",
    "        #计算RMSE\n",
    "        rmse = RMSE(y_test, y_hat)\n",
    "        print(f'RMSE: {rmse} on fold {fold}')\n",
    "        \n",
    "        \n",
    "        #复制是因为有的要归一化\n",
    "        X_unseen_copy = X_unseen.copy()\n",
    "        X_unseen_copy=X_unseen_copy\n",
    "        \n",
    "        y_hats[model_type].append(model.predict(X_unseen_copy))\n",
    "        #在字典里没有就创建一个\n",
    "        if model_type not in models_and_errors_dict:\n",
    "            models_and_errors_dict[model_type] = []\n",
    "        models_and_errors_dict[model_type].append((model, rmse, None, None,oof_pred))\n",
    "for key in y_hats.keys():\n",
    "    #如果有值的话,求平均,赋值给submission_df\n",
    "    if y_hats[key]:\n",
    "        y_hat_avg = np.mean(y_hats[key], axis=0)\n",
    "        submission_df['score_' + key] = y_hat_avg\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19283181",
   "metadata": {
    "papermill": {
     "duration": 0.00857,
     "end_time": "2024-01-11T08:56:24.997727",
     "exception": false,
     "start_time": "2024-01-11T08:56:24.989157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "632e37ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-11T08:56:25.017683Z",
     "iopub.status.busy": "2024-01-11T08:56:25.017297Z",
     "iopub.status.idle": "2024-01-11T08:56:40.368979Z",
     "shell.execute_reply": "2024-01-11T08:56:40.367439Z"
    },
    "papermill": {
     "duration": 15.364746,
     "end_time": "2024-01-11T08:56:40.371473",
     "exception": false,
     "start_time": "2024-01-11T08:56:25.006727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_RMSE:0.5970264086685236,blending_weights:{'lgb': 0.048, 'cat': 0.725, 'xgb': 0.227}\n",
      "blending\n",
      "blended_score:0    1.324685\n",
      "1    1.300460\n",
      "2    1.309263\n",
      "Name: score_lgb, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#融合模型的权重\n",
    "blending_weights = {\n",
    "    'lgb': 0.4,\n",
    "    'cat': 0.4,\n",
    "    'xgb': 0.2,\n",
    "}\n",
    "lgb_oof_pred=models_and_errors_dict['lgb'][num_folds-1][4]\n",
    "cat_oof_pred=models_and_errors_dict['cat'][num_folds-1][4]\n",
    "xgb_oof_pred=models_and_errors_dict['xgb'][num_folds-1][4]\n",
    "margin=1000\n",
    "target=y.values\n",
    "current_RMSE=RMSE(target,(lgb_oof_pred+cat_oof_pred+xgb_oof_pred)/3)\n",
    "best_i=0\n",
    "best_j=0\n",
    "for i in range(0,margin):\n",
    "    for j in range(0,margin-i):\n",
    "        #k=1000-i-j\n",
    "        blend_oof_pred=(i*lgb_oof_pred+j*cat_oof_pred+(margin-i-j)*xgb_oof_pred)/margin\n",
    "        if RMSE(target,blend_oof_pred)<current_RMSE:\n",
    "            current_RMSE=RMSE(target,blend_oof_pred)\n",
    "            best_i=i\n",
    "            best_j=j\n",
    "#找到最好的参数之后\n",
    "blending_weights['lgb']=best_i/margin\n",
    "blending_weights['cat']=best_j/margin\n",
    "blending_weights['xgb']=(margin-best_i-best_j)/margin\n",
    "print(f\"current_RMSE:{current_RMSE},blending_weights:{blending_weights}\")\n",
    "print(\"blending\")\n",
    "blended_score=np.zeros((len(test_feats)))\n",
    "for k, v in blending_weights.items():\n",
    "    blended_score += submission_df['score_' + k] * v\n",
    "print(f\"blended_score:{blended_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddc498c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-11T08:34:29.126037Z",
     "iopub.status.idle": "2024-01-11T08:34:29.126551Z",
     "shell.execute_reply": "2024-01-11T08:34:29.126339Z",
     "shell.execute_reply.started": "2024-01-11T08:34:29.126316Z"
    },
    "papermill": {
     "duration": 0.008941,
     "end_time": "2024-01-11T08:56:40.390151",
     "exception": false,
     "start_time": "2024-01-11T08:56:40.381210",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d70521b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-11T08:56:40.410878Z",
     "iopub.status.busy": "2024-01-11T08:56:40.410475Z",
     "iopub.status.idle": "2024-01-11T08:56:40.438316Z",
     "shell.execute_reply": "2024-01-11T08:56:40.437437Z"
    },
    "papermill": {
     "duration": 0.041354,
     "end_time": "2024-01-11T08:56:40.440663",
     "exception": false,
     "start_time": "2024-01-11T08:56:40.399309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>1.324685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2222bbbb</td>\n",
       "      <td>1.300460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4444cccc</td>\n",
       "      <td>1.309263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     score\n",
       "0  0000aaaa  1.324685\n",
       "1  2222bbbb  1.300460\n",
       "2  4444cccc  1.309263"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission=pd.read_csv(\"/kaggle/input/linking-writing-processes-to-writing-quality/sample_submission.csv\")\n",
    "submission['score']=blended_score\n",
    "submission.to_csv(\"submission.csv\",index=None)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 6678907,
     "sourceId": 59291,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 482.11473,
   "end_time": "2024-01-11T08:56:41.475739",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-11T08:48:39.361009",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
