{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52620c3c",
   "metadata": {
    "papermill": {
     "duration": 0.003538,
     "end_time": "2024-07-31T13:51:30.170739",
     "exception": false,
     "start_time": "2024-07-31T13:51:30.167201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Created by <a href=\"https://github.com/yunsuxiaozi\">yunsuxiaozi</a>  2024/7/31\n",
    "\n",
    "####  在<a href=\"https://www.kaggle.com/code/yunsuxiaozi/chatglm6b-huanhuan-finetune-training\">chatglm6b-huanhuan-finetune(training)</a>这个notebook里,我们使用了甄嬛传的数据集完成了chatglm-6b大模型的微调,训练出了专属于我们的个性化AI--chat_huanhuan。如果有人想要体验chat_huanhuan,可以使用这个notebook来进行体验,下面为示例代码。\n",
    "\n",
    "#### 注:由于这里的代码都在模型微调的代码里出现过,如果有人想看代码的具体解释可以看模型微调的notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "212245e1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-31T13:51:30.178138Z",
     "iopub.status.busy": "2024-07-31T13:51:30.177835Z",
     "iopub.status.idle": "2024-07-31T13:51:32.730571Z",
     "shell.execute_reply": "2024-07-31T13:51:32.729659Z"
    },
    "papermill": {
     "duration": 2.559049,
     "end_time": "2024-07-31T13:51:32.732886",
     "exception": false,
     "start_time": "2024-07-31T13:51:30.173837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ChatGLM-6B'...\r\n",
      "remote: Enumerating objects: 1252, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (17/17), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (11/11), done.\u001b[K\r\n",
      "remote: Total 1252 (delta 8), reused 11 (delta 6), pack-reused 1235\u001b[K\r\n",
      "Receiving objects: 100% (1252/1252), 9.15 MiB | 17.78 MiB/s, done.\r\n",
      "Resolving deltas: 100% (737/737), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/THUDM/ChatGLM-6B.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33bcd743",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T13:51:32.742794Z",
     "iopub.status.busy": "2024-07-31T13:51:32.742045Z",
     "iopub.status.idle": "2024-07-31T13:52:02.312940Z",
     "shell.execute_reply": "2024-07-31T13:52:02.312030Z"
    },
    "papermill": {
     "duration": 29.578182,
     "end_time": "2024-07-31T13:52:02.315209",
     "exception": false,
     "start_time": "2024-07-31T13:51:32.737027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from -r ChatGLM-6B/requirements.txt (line 1)) (3.20.3)\r\n",
      "Collecting transformers==4.27.1 (from -r ChatGLM-6B/requirements.txt (line 2))\r\n",
      "  Downloading transformers-4.27.1-py3-none-any.whl.metadata (106 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting cpm_kernels (from -r ChatGLM-6B/requirements.txt (line 3))\r\n",
      "  Downloading cpm_kernels-1.0.11-py3-none-any.whl.metadata (1.2 kB)\r\n",
      "Requirement already satisfied: torch>=1.10 in /opt/conda/lib/python3.10/site-packages (from -r ChatGLM-6B/requirements.txt (line 4)) (2.1.2)\r\n",
      "Collecting gradio (from -r ChatGLM-6B/requirements.txt (line 5))\r\n",
      "  Downloading gradio-4.39.0-py3-none-any.whl.metadata (15 kB)\r\n",
      "Collecting mdtex2html (from -r ChatGLM-6B/requirements.txt (line 6))\r\n",
      "  Downloading mdtex2html-1.3.0-py3-none-any.whl.metadata (4.1 kB)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from -r ChatGLM-6B/requirements.txt (line 7)) (0.2.0)\r\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from -r ChatGLM-6B/requirements.txt (line 8)) (0.32.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-6B/requirements.txt (line 2)) (3.13.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-6B/requirements.txt (line 2)) (0.23.4)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-6B/requirements.txt (line 2)) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-6B/requirements.txt (line 2)) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-6B/requirements.txt (line 2)) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-6B/requirements.txt (line 2)) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-6B/requirements.txt (line 2)) (2.32.3)\r\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.27.1->-r ChatGLM-6B/requirements.txt (line 2))\r\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-6B/requirements.txt (line 2)) (4.66.4)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->-r ChatGLM-6B/requirements.txt (line 4)) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->-r ChatGLM-6B/requirements.txt (line 4)) (1.13.0)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->-r ChatGLM-6B/requirements.txt (line 4)) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->-r ChatGLM-6B/requirements.txt (line 4)) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->-r ChatGLM-6B/requirements.txt (line 4)) (2024.5.0)\r\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (22.1.0)\r\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (4.2.0)\r\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (0.108.0)\r\n",
      "Collecting ffmpy (from gradio->-r ChatGLM-6B/requirements.txt (line 5))\r\n",
      "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Collecting gradio-client==1.1.1 (from gradio->-r ChatGLM-6B/requirements.txt (line 5))\r\n",
      "  Downloading gradio_client-1.1.1-py3-none-any.whl.metadata (7.1 kB)\r\n",
      "Requirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (0.27.0)\r\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (6.1.1)\r\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (2.1.3)\r\n",
      "Requirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (3.7.5)\r\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (3.9.10)\r\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (2.2.2)\r\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (9.5.0)\r\n",
      "Requirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (2.5.3)\r\n",
      "Requirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (0.25.1)\r\n",
      "Collecting python-multipart>=0.0.9 (from gradio->-r ChatGLM-6B/requirements.txt (line 5))\r\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting ruff>=0.2.2 (from gradio->-r ChatGLM-6B/requirements.txt (line 5))\r\n",
      "  Downloading ruff-0.5.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\r\n",
      "Collecting semantic-version~=2.0 (from gradio->-r ChatGLM-6B/requirements.txt (line 5))\r\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\r\n",
      "Collecting tomlkit==0.12.0 (from gradio->-r ChatGLM-6B/requirements.txt (line 5))\r\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\r\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (0.12.3)\r\n",
      "Collecting urllib3~=2.0 (from gradio->-r ChatGLM-6B/requirements.txt (line 5))\r\n",
      "  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\r\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (0.25.0)\r\n",
      "Collecting websockets<12.0,>=10.0 (from gradio-client==1.1.1->gradio->-r ChatGLM-6B/requirements.txt (line 5))\r\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: markdown in /opt/conda/lib/python3.10/site-packages (from mdtex2html->-r ChatGLM-6B/requirements.txt (line 6)) (3.5.2)\r\n",
      "Collecting latex2mathml (from mdtex2html->-r ChatGLM-6B/requirements.txt (line 6))\r\n",
      "  Downloading latex2mathml-3.77.0-py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->-r ChatGLM-6B/requirements.txt (line 8)) (5.9.3)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r ChatGLM-6B/requirements.txt (line 8)) (0.4.3)\r\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (3.6)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (1.3.0)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (1.2.0)\r\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (2024.7.4)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (1.0.5)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (0.14.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (4.47.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (1.4.5)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (3.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (2023.4)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (2.14.6)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (8.1.7)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (13.7.0)\r\n",
      "Requirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (0.32.0.post1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.27.1->-r ChatGLM-6B/requirements.txt (line 2)) (3.3.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10->-r ChatGLM-6B/requirements.txt (line 4)) (1.3.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (1.16.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (2.17.2)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (0.1.2)\r\n",
      "Downloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading gradio-4.39.0-py3-none-any.whl (12.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading gradio_client-1.1.1-py3-none-any.whl (318 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.2/318.2 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\r\n",
      "Downloading mdtex2html-1.3.0-py3-none-any.whl (13 kB)\r\n",
      "Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\r\n",
      "Downloading ruff-0.5.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\r\n",
      "Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading urllib3-2.2.2-py3-none-any.whl (121 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\r\n",
      "Downloading latex2mathml-3.77.0-py3-none-any.whl (73 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: tokenizers, cpm_kernels, websockets, urllib3, tomlkit, semantic-version, ruff, python-multipart, latex2mathml, ffmpy, mdtex2html, transformers, gradio-client, gradio\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.19.1\r\n",
      "    Uninstalling tokenizers-0.19.1:\r\n",
      "      Successfully uninstalled tokenizers-0.19.1\r\n",
      "  Attempting uninstall: websockets\r\n",
      "    Found existing installation: websockets 12.0\r\n",
      "    Uninstalling websockets-12.0:\r\n",
      "      Successfully uninstalled websockets-12.0\r\n",
      "  Attempting uninstall: urllib3\r\n",
      "    Found existing installation: urllib3 1.26.18\r\n",
      "    Uninstalling urllib3-1.26.18:\r\n",
      "      Successfully uninstalled urllib3-1.26.18\r\n",
      "  Attempting uninstall: tomlkit\r\n",
      "    Found existing installation: tomlkit 0.12.5\r\n",
      "    Uninstalling tomlkit-0.12.5:\r\n",
      "      Successfully uninstalled tomlkit-0.12.5\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.42.3\r\n",
      "    Uninstalling transformers-4.42.3:\r\n",
      "      Successfully uninstalled transformers-4.42.3\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n",
      "distributed 2024.5.1 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\r\n",
      "kaggle-environments 1.14.15 requires transformers>=4.33.1, but you have transformers 4.27.1 which is incompatible.\r\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "kfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.2 which is incompatible.\r\n",
      "rapids-dask-dependency 24.6.0a0 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\r\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed cpm_kernels-1.0.11 ffmpy-0.4.0 gradio-4.39.0 gradio-client-1.1.1 latex2mathml-3.77.0 mdtex2html-1.3.0 python-multipart-0.0.9 ruff-0.5.5 semantic-version-2.10.0 tokenizers-0.13.3 tomlkit-0.12.0 transformers-4.27.1 urllib3-2.1.0 websockets-11.0.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ChatGLM-6B/requirements.txt  #安装chatglm需要依赖的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a051a8f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T13:52:02.334988Z",
     "iopub.status.busy": "2024-07-31T13:52:02.334686Z",
     "iopub.status.idle": "2024-07-31T13:52:15.233281Z",
     "shell.execute_reply": "2024-07-31T13:52:15.232081Z"
    },
    "papermill": {
     "duration": 12.911303,
     "end_time": "2024-07-31T13:52:15.235864",
     "exception": false,
     "start_time": "2024-07-31T13:52:02.324561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q rouge_chinese nltk jieba datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69078d8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T13:52:15.266596Z",
     "iopub.status.busy": "2024-07-31T13:52:15.266091Z",
     "iopub.status.idle": "2024-07-31T13:55:28.942716Z",
     "shell.execute_reply": "2024-07-31T13:55:28.941294Z"
    },
    "papermill": {
     "duration": 193.697638,
     "end_time": "2024-07-31T13:55:28.945730",
     "exception": false,
     "start_time": "2024-07-31T13:52:15.248092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'chatglm-6b-int4'...\r\n",
      "remote: Enumerating objects: 137, done.\u001b[K\r\n",
      "remote: Total 137 (delta 0), reused 0 (delta 0), pack-reused 137 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (137/137), 62.10 KiB | 15.53 MiB/s, done.\r\n",
      "Resolving deltas: 100% (79/79), done.\r\n",
      "Filtering content: 100% (2/2), 3.62 GiB | 19.37 MiB/s, done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/THUDM/chatglm-6b-int4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ea30249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T13:55:29.036405Z",
     "iopub.status.busy": "2024-07-31T13:55:29.035622Z",
     "iopub.status.idle": "2024-07-31T13:55:45.765993Z",
     "shell.execute_reply": "2024-07-31T13:55:45.765205Z"
    },
    "papermill": {
     "duration": 16.778985,
     "end_time": "2024-07-31T13:55:45.768167",
     "exception": false,
     "start_time": "2024-07-31T13:55:28.989182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.c -shared -o /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.so\n",
      "Load kernel : /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.so\n",
      "Setting CPU quantization kernel threads to 2\n",
      "Parallel kernel is not recommended when parallel num < 4.\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n"
     ]
    }
   ],
   "source": [
    "# AutoTokenizer自动加载与模型对应的分词器,AutoModel自动加载预训练模型\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_path = \"chatglm-6b-int4\"#模型的参数\n",
    "#根据模型的路径加载预训练分词器,允许远程加载代码(trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "#根据模型的路径加载预训练模型,允许远程加载代码(trust_remote_code=True),half是半精度浮点数,cuda是移动到GPU上\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "042a9bb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T13:55:45.789803Z",
     "iopub.status.busy": "2024-07-31T13:55:45.789330Z",
     "iopub.status.idle": "2024-07-31T13:55:53.967142Z",
     "shell.execute_reply": "2024-07-31T13:55:53.966290Z"
    },
    "papermill": {
     "duration": 8.191009,
     "end_time": "2024-07-31T13:55:53.969453",
     "exception": false,
     "start_time": "2024-07-31T13:55:45.778444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.c -shared -o /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.so\n",
      "Load kernel : /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.so\n",
      "Setting CPU quantization kernel threads to 2\n",
      "Parallel kernel is not recommended when parallel num < 4.\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at chatglm-6b-int4 and are newly initialized: ['transformer.prefix_encoder.embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch#pytorch这个深度学习框架\n",
    "from transformers import AutoConfig#自动下载和配置预训练模型的配置\n",
    "\n",
    "#根据模型路径加载config,允许远程加载代码(trust_remote_code=True),大模型输入序列的最大长度\n",
    "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True, pre_seq_len=256)\n",
    "#根据模型的路径和参数加载模型,允许远程加载代码(trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_path, config=config, trust_remote_code=True)\n",
    "\n",
    "#从二进制(binary)文件中加载模型的状态字典,这个参数字典一般是在某个检查点(checkpoint)保存下来的。\n",
    "prefix_state_dict = torch.load(\"/kaggle/input/chatglm6b-huanhuan-finetune-training/output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-500/pytorch_model.bin\")\n",
    "#进行参数的更新\n",
    "new_prefix_state_dict = {}\n",
    "for k, v in prefix_state_dict.items():\n",
    "    new_prefix_state_dict[k[len(\"transformer.prefix_encoder.\"):]] = v\n",
    "model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)\n",
    "\n",
    "#half是半精度浮点数,cuda是移动到GPU上\n",
    "model = model.half().cuda()\n",
    "#将模型prefix_encoder部分的参数换成全精度浮点数float32\n",
    "model.transformer.prefix_encoder.float()\n",
    "#将大模型换成评估模式\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d4a2fac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T13:55:53.991904Z",
     "iopub.status.busy": "2024-07-31T13:55:53.991624Z",
     "iopub.status.idle": "2024-07-31T13:56:08.678944Z",
     "shell.execute_reply": "2024-07-31T13:56:08.677900Z"
    },
    "papermill": {
     "duration": 14.700595,
     "end_time": "2024-07-31T13:56:08.681095",
     "exception": false,
     "start_time": "2024-07-31T13:55:53.980500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 13:56:00.003249: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-31 13:56:00.003347: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-31 13:56:00.131327: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:朕的后宫佳丽三千,朕最喜欢的就是你。\n",
      "response:皇上最喜我，我自然高兴。\n"
     ]
    }
   ],
   "source": [
    "question='朕的后宫佳丽三千,朕最喜欢的就是你。'\n",
    "response, history = model.chat(tokenizer, question, history=[])\n",
    "print(f\"question:{question}\\nresponse:{response}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5467682,
     "sourceId": 9065856,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 190568050,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 284.280021,
   "end_time": "2024-07-31T13:56:11.696753",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-31T13:51:27.416732",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
