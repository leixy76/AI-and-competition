{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f2f056",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.006717,
     "end_time": "2024-07-31T08:46:36.048161",
     "exception": false,
     "start_time": "2024-07-31T08:46:36.041444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Created by <a href=\"https://github.com/yunsuxiaozi\">yunsuxiaozi</a>  2024/7/31\n",
    "\n",
    "#### 这里将使用甄嬛传的数据集来完成对大模型chatglm-6b的微调,实现一个个性化的AI。数据集来源于github的一个开源项目:<a href=\"https://github.com/KMnO4-zx/xlab-huanhuan\">xlab-huanhuan</a>,目前已将数据集上传到Kaggle上:<a href=\"https://www.kaggle.com/datasets/yunsuxiaozi/chat-huanhuan\">chat-huanhuan</a>,各位可以用于学习大模型微调的技术。这里关于chatglm-6b的微调参考了这个notebook:<a href=\"https://www.kaggle.com/code/tiansztianszs/chatglm-fine-tuning/notebook\">chatglm finetune</a>。这里使用的是Kaggle上的GPU T4\\*2。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbaf71b",
   "metadata": {
    "papermill": {
     "duration": 0.006668,
     "end_time": "2024-07-31T08:46:36.061132",
     "exception": false,
     "start_time": "2024-07-31T08:46:36.054464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.熟悉数据集\n",
    "\n",
    "#### 这里先来看看我们使用的数据集是什么样的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c26e2b42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T08:46:36.076590Z",
     "iopub.status.busy": "2024-07-31T08:46:36.075637Z",
     "iopub.status.idle": "2024-07-31T08:46:36.120822Z",
     "shell.execute_reply": "2024-07-31T08:46:36.119849Z"
    },
    "papermill": {
     "duration": 0.055039,
     "end_time": "2024-07-31T08:46:36.123093",
     "exception": false,
     "start_time": "2024-07-31T08:46:36.068054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(data):3729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'instruction': '小姐，别的秀女都在求中选，唯有咱们小姐想被撂牌子，菩萨一定记得真真儿的——',\n",
       " 'input': '',\n",
       " 'output': '嘘——都说许愿说破是不灵的。'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json#用于处理JavaScript Object Notation数据格式\n",
    "#用utf-8编码读取json文件\n",
    "with open(\"/kaggle/input/chat-huanhuan/huanhuan.json\",mode='r',encoding='utf-8') as f:\n",
    "    data=json.load(f)\n",
    "print(f\"len(data):{len(data)}\")\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0b1641",
   "metadata": {
    "papermill": {
     "duration": 0.006335,
     "end_time": "2024-07-31T08:46:36.135633",
     "exception": false,
     "start_time": "2024-07-31T08:46:36.129298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 我们可以看到,数据有三千多条,每个数据都是一个字典,包含'instruction','input'和'output',这里我们要微调一个对话的AI,'instruction'就是你和AI说的话,'output'就是AI给你的回答。\n",
    "\n",
    "#### 如果要微调大模型去打比赛,比如大模型小说创作比赛,instruction就是故事的梗概,output就是故事的具体内容;如果是大模型推理比赛,instruction就是推理的问题,output就是推理的回答。只要有足够的高质量的数据,我们就可以把大模型微调成我们想要的样子。\n",
    "\n",
    "#### 这里使用的数据集已经是json文件,并且也是非常干净(也就是不需要我们再做什么处理)的数据,我们后续可以直接将json文件的路径作为一个参数完成大模型的微调。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e8275a",
   "metadata": {
    "papermill": {
     "duration": 0.006239,
     "end_time": "2024-07-31T08:46:36.148110",
     "exception": false,
     "start_time": "2024-07-31T08:46:36.141871",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.chatglm-6b的微调环境准备\n",
    "\n",
    "#### 先用克隆命令将chatglm-6b从github上克隆到本地环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7945343b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T08:46:36.162881Z",
     "iopub.status.busy": "2024-07-31T08:46:36.162453Z",
     "iopub.status.idle": "2024-07-31T08:46:38.353499Z",
     "shell.execute_reply": "2024-07-31T08:46:38.352031Z"
    },
    "papermill": {
     "duration": 2.20236,
     "end_time": "2024-07-31T08:46:38.356805",
     "exception": false,
     "start_time": "2024-07-31T08:46:36.154445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ChatGLM-6B'...\r\n",
      "remote: Enumerating objects: 1252, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (17/17), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (11/11), done.\u001b[K\r\n",
      "remote: Total 1252 (delta 8), reused 11 (delta 6), pack-reused 1235\u001b[K\r\n",
      "Receiving objects: 100% (1252/1252), 9.15 MiB | 24.59 MiB/s, done.\r\n",
      "Resolving deltas: 100% (737/737), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/THUDM/ChatGLM-6B.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94912e37",
   "metadata": {
    "papermill": {
     "duration": 0.010355,
     "end_time": "2024-07-31T08:46:38.375770",
     "exception": false,
     "start_time": "2024-07-31T08:46:38.365415",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 安装chatglm-6b需要依赖的一些库。requirements.txt文件里写了chatglm-6b里依赖的库以及对应的版本号,比如transformers==4.27.1,我们需要安装这些库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54c3f311",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T08:46:38.394547Z",
     "iopub.status.busy": "2024-07-31T08:46:38.393803Z",
     "iopub.status.idle": "2024-07-31T08:46:38.401754Z",
     "shell.execute_reply": "2024-07-31T08:46:38.400773Z"
    },
    "papermill": {
     "duration": 0.02,
     "end_time": "2024-07-31T08:46:38.404615",
     "exception": false,
     "start_time": "2024-07-31T08:46:38.384615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protobuf\n",
      "\n",
      "transformers==4.27.1\n",
      "\n",
      "cpm_kernels\n",
      "\n",
      "torch>=1.10\n",
      "\n",
      "gradio\n",
      "\n",
      "mdtex2html\n",
      "\n",
      "sentencepiece\n",
      "\n",
      "accelerate\n"
     ]
    }
   ],
   "source": [
    "#这里就是查看一下requirements有哪些库,可以简单理解为print(),删除掉这段代码不会影响程序的正常运行。\n",
    "with open(\"/kaggle/working/ChatGLM-6B/requirements.txt\",mode='r',encoding='utf-8') as f:\n",
    "    data=f.readline()\n",
    "    while data:\n",
    "        print(data)\n",
    "        data=f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93dbc4b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T08:46:38.421769Z",
     "iopub.status.busy": "2024-07-31T08:46:38.420971Z",
     "iopub.status.idle": "2024-07-31T08:47:11.879608Z",
     "shell.execute_reply": "2024-07-31T08:47:11.878573Z"
    },
    "papermill": {
     "duration": 33.46992,
     "end_time": "2024-07-31T08:47:11.882292",
     "exception": false,
     "start_time": "2024-07-31T08:46:38.412372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from -r ChatGLM-6B/requirements.txt (line 1)) (3.20.3)\r\n",
      "Collecting transformers==4.27.1 (from -r ChatGLM-6B/requirements.txt (line 2))\r\n",
      "  Downloading transformers-4.27.1-py3-none-any.whl.metadata (106 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting cpm_kernels (from -r ChatGLM-6B/requirements.txt (line 3))\r\n",
      "  Downloading cpm_kernels-1.0.11-py3-none-any.whl.metadata (1.2 kB)\r\n",
      "Requirement already satisfied: torch>=1.10 in /opt/conda/lib/python3.10/site-packages (from -r ChatGLM-6B/requirements.txt (line 4)) (2.1.2)\r\n",
      "Collecting gradio (from -r ChatGLM-6B/requirements.txt (line 5))\r\n",
      "  Downloading gradio-4.39.0-py3-none-any.whl.metadata (15 kB)\r\n",
      "Collecting mdtex2html (from -r ChatGLM-6B/requirements.txt (line 6))\r\n",
      "  Downloading mdtex2html-1.3.0-py3-none-any.whl.metadata (4.1 kB)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from -r ChatGLM-6B/requirements.txt (line 7)) (0.2.0)\r\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from -r ChatGLM-6B/requirements.txt (line 8)) (0.32.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-6B/requirements.txt (line 2)) (3.13.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-6B/requirements.txt (line 2)) (0.23.4)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-6B/requirements.txt (line 2)) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-6B/requirements.txt (line 2)) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-6B/requirements.txt (line 2)) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-6B/requirements.txt (line 2)) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-6B/requirements.txt (line 2)) (2.32.3)\r\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.27.1->-r ChatGLM-6B/requirements.txt (line 2))\r\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-6B/requirements.txt (line 2)) (4.66.4)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->-r ChatGLM-6B/requirements.txt (line 4)) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->-r ChatGLM-6B/requirements.txt (line 4)) (1.13.0)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->-r ChatGLM-6B/requirements.txt (line 4)) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->-r ChatGLM-6B/requirements.txt (line 4)) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->-r ChatGLM-6B/requirements.txt (line 4)) (2024.5.0)\r\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (22.1.0)\r\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (4.2.0)\r\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (0.108.0)\r\n",
      "Collecting ffmpy (from gradio->-r ChatGLM-6B/requirements.txt (line 5))\r\n",
      "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Collecting gradio-client==1.1.1 (from gradio->-r ChatGLM-6B/requirements.txt (line 5))\r\n",
      "  Downloading gradio_client-1.1.1-py3-none-any.whl.metadata (7.1 kB)\r\n",
      "Requirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (0.27.0)\r\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (6.1.1)\r\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (2.1.3)\r\n",
      "Requirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (3.7.5)\r\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (3.9.10)\r\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (2.2.2)\r\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (9.5.0)\r\n",
      "Requirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (2.5.3)\r\n",
      "Requirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (0.25.1)\r\n",
      "Collecting python-multipart>=0.0.9 (from gradio->-r ChatGLM-6B/requirements.txt (line 5))\r\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting ruff>=0.2.2 (from gradio->-r ChatGLM-6B/requirements.txt (line 5))\r\n",
      "  Downloading ruff-0.5.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\r\n",
      "Collecting semantic-version~=2.0 (from gradio->-r ChatGLM-6B/requirements.txt (line 5))\r\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\r\n",
      "Collecting tomlkit==0.12.0 (from gradio->-r ChatGLM-6B/requirements.txt (line 5))\r\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\r\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (0.12.3)\r\n",
      "Collecting urllib3~=2.0 (from gradio->-r ChatGLM-6B/requirements.txt (line 5))\r\n",
      "  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\r\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio->-r ChatGLM-6B/requirements.txt (line 5)) (0.25.0)\r\n",
      "Collecting websockets<12.0,>=10.0 (from gradio-client==1.1.1->gradio->-r ChatGLM-6B/requirements.txt (line 5))\r\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: markdown in /opt/conda/lib/python3.10/site-packages (from mdtex2html->-r ChatGLM-6B/requirements.txt (line 6)) (3.5.2)\r\n",
      "Collecting latex2mathml (from mdtex2html->-r ChatGLM-6B/requirements.txt (line 6))\r\n",
      "  Downloading latex2mathml-3.77.0-py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->-r ChatGLM-6B/requirements.txt (line 8)) (5.9.3)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r ChatGLM-6B/requirements.txt (line 8)) (0.4.3)\r\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (3.6)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (1.3.0)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (1.2.0)\r\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (2024.7.4)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (1.0.5)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (0.14.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (4.47.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (1.4.5)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (3.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (2023.4)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (2.14.6)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (8.1.7)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (13.7.0)\r\n",
      "Requirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (0.32.0.post1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.27.1->-r ChatGLM-6B/requirements.txt (line 2)) (3.3.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10->-r ChatGLM-6B/requirements.txt (line 4)) (1.3.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (1.16.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (2.17.2)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio->-r ChatGLM-6B/requirements.txt (line 5)) (0.1.2)\r\n",
      "Downloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading gradio-4.39.0-py3-none-any.whl (12.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading gradio_client-1.1.1-py3-none-any.whl (318 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.2/318.2 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\r\n",
      "Downloading mdtex2html-1.3.0-py3-none-any.whl (13 kB)\r\n",
      "Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\r\n",
      "Downloading ruff-0.5.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\r\n",
      "Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading urllib3-2.2.2-py3-none-any.whl (121 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\r\n",
      "Downloading latex2mathml-3.77.0-py3-none-any.whl (73 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: tokenizers, cpm_kernels, websockets, urllib3, tomlkit, semantic-version, ruff, python-multipart, latex2mathml, ffmpy, mdtex2html, transformers, gradio-client, gradio\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.19.1\r\n",
      "    Uninstalling tokenizers-0.19.1:\r\n",
      "      Successfully uninstalled tokenizers-0.19.1\r\n",
      "  Attempting uninstall: websockets\r\n",
      "    Found existing installation: websockets 12.0\r\n",
      "    Uninstalling websockets-12.0:\r\n",
      "      Successfully uninstalled websockets-12.0\r\n",
      "  Attempting uninstall: urllib3\r\n",
      "    Found existing installation: urllib3 1.26.18\r\n",
      "    Uninstalling urllib3-1.26.18:\r\n",
      "      Successfully uninstalled urllib3-1.26.18\r\n",
      "  Attempting uninstall: tomlkit\r\n",
      "    Found existing installation: tomlkit 0.12.5\r\n",
      "    Uninstalling tomlkit-0.12.5:\r\n",
      "      Successfully uninstalled tomlkit-0.12.5\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.42.3\r\n",
      "    Uninstalling transformers-4.42.3:\r\n",
      "      Successfully uninstalled transformers-4.42.3\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n",
      "distributed 2024.5.1 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\r\n",
      "kaggle-environments 1.14.15 requires transformers>=4.33.1, but you have transformers 4.27.1 which is incompatible.\r\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "kfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.2 which is incompatible.\r\n",
      "rapids-dask-dependency 24.6.0a0 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\r\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed cpm_kernels-1.0.11 ffmpy-0.4.0 gradio-4.39.0 gradio-client-1.1.1 latex2mathml-3.77.0 mdtex2html-1.3.0 python-multipart-0.0.9 ruff-0.5.5 semantic-version-2.10.0 tokenizers-0.13.3 tomlkit-0.12.0 transformers-4.27.1 urllib3-2.1.0 websockets-11.0.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ChatGLM-6B/requirements.txt  #安装chatglm需要依赖的库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9215133a",
   "metadata": {
    "papermill": {
     "duration": 0.014273,
     "end_time": "2024-07-31T08:47:11.912129",
     "exception": false,
     "start_time": "2024-07-31T08:47:11.897856",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 安装chatglm依赖的其他库\n",
    "\n",
    "- rouge_chinese:这个库提供了用于评估生成文本和参考文本重叠度的ROUGE（Recall-Oriented Understudy for Gisting Evaluation）分数的实现。\n",
    "\n",
    "- nltk:自然语言处理工具包。\n",
    "\n",
    "- jieba:中文文本分词库。\n",
    "\n",
    "- datasets:用于处理和加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "819cb0be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T08:47:11.943247Z",
     "iopub.status.busy": "2024-07-31T08:47:11.942306Z",
     "iopub.status.idle": "2024-07-31T08:47:25.648374Z",
     "shell.execute_reply": "2024-07-31T08:47:25.647026Z"
    },
    "papermill": {
     "duration": 13.724319,
     "end_time": "2024-07-31T08:47:25.650989",
     "exception": false,
     "start_time": "2024-07-31T08:47:11.926670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q rouge_chinese nltk jieba datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf506fd",
   "metadata": {
    "papermill": {
     "duration": 0.014649,
     "end_time": "2024-07-31T08:47:25.680955",
     "exception": false,
     "start_time": "2024-07-31T08:47:25.666306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 克隆经过int4量化处理的chatglm-6b模型的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf0839e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T08:47:25.712292Z",
     "iopub.status.busy": "2024-07-31T08:47:25.711913Z",
     "iopub.status.idle": "2024-07-31T08:48:00.911683Z",
     "shell.execute_reply": "2024-07-31T08:48:00.910073Z"
    },
    "papermill": {
     "duration": 35.218647,
     "end_time": "2024-07-31T08:48:00.914493",
     "exception": false,
     "start_time": "2024-07-31T08:47:25.695846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'chatglm-6b-int4'...\r\n",
      "remote: Enumerating objects: 137, done.\u001b[K\r\n",
      "remote: Total 137 (delta 0), reused 0 (delta 0), pack-reused 137 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (137/137), 58.06 KiB | 14.52 MiB/s, done.\r\n",
      "Resolving deltas: 100% (79/79), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/THUDM/chatglm-6b-int4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f14810",
   "metadata": {
    "papermill": {
     "duration": 0.048228,
     "end_time": "2024-07-31T08:48:00.988919",
     "exception": false,
     "start_time": "2024-07-31T08:48:00.940691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.chatglm 模型微调\n",
    "\n",
    "#### 加载预训练模型和分词器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e995ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T08:48:01.087619Z",
     "iopub.status.busy": "2024-07-31T08:48:01.086489Z",
     "iopub.status.idle": "2024-07-31T08:48:16.603106Z",
     "shell.execute_reply": "2024-07-31T08:48:16.601997Z"
    },
    "papermill": {
     "duration": 15.569309,
     "end_time": "2024-07-31T08:48:16.605768",
     "exception": false,
     "start_time": "2024-07-31T08:48:01.036459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.c -shared -o /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.so\n",
      "Load kernel : /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.so\n",
      "Setting CPU quantization kernel threads to 2\n",
      "Parallel kernel is not recommended when parallel num < 4.\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n"
     ]
    }
   ],
   "source": [
    "# AutoTokenizer自动加载与模型对应的分词器,AutoModel自动加载预训练模型\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_path = \"chatglm-6b-int4\"#模型的参数\n",
    "#根据模型的路径加载预训练分词器,允许远程加载代码(trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "#根据模型的路径加载预训练模型,允许远程加载代码(trust_remote_code=True),half是半精度浮点数,cuda是移动到GPU上\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fd55cc",
   "metadata": {
    "papermill": {
     "duration": 0.015862,
     "end_time": "2024-07-31T08:48:16.637881",
     "exception": false,
     "start_time": "2024-07-31T08:48:16.622019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 为了比较大语言模型微调前后的效果,这里先试试微调前的response。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "820bc3aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T08:48:16.671793Z",
     "iopub.status.busy": "2024-07-31T08:48:16.671226Z",
     "iopub.status.idle": "2024-07-31T08:48:44.350223Z",
     "shell.execute_reply": "2024-07-31T08:48:44.349178Z"
    },
    "papermill": {
     "duration": 27.698818,
     "end_time": "2024-07-31T08:48:44.352835",
     "exception": false,
     "start_time": "2024-07-31T08:48:16.654017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 08:48:33.472631: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-31 08:48:33.472758: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-31 08:48:33.644111: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:朕的后宫里佳丽三千,你可知朕最宠幸的是谁?\n",
      "response:作为一个人工智能助手，我的职责是辅助人类完成任务并提供信息。然而，作为一个虚拟的存在，我没有实际的身体，因此我无法在现实世界中扮演一个真实的人类角色，包括在后宫中宠幸佳丽。此外，作为一个人工智能助手，我也无法了解或记录历史上的任何事件或人物。\n"
     ]
    }
   ],
   "source": [
    "#将大模型换成评估模式\n",
    "question='朕的后宫里佳丽三千,你可知朕最宠幸的是谁?'\n",
    "model = model.eval()\n",
    "response, history = model.chat(tokenizer, question, history=[])\n",
    "print(f\"question:{question}\\nresponse:{response}\")\n",
    "model=model.train()#将模型转回训练模式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cebfd90",
   "metadata": {
    "papermill": {
     "duration": 0.01568,
     "end_time": "2024-07-31T08:48:44.385282",
     "exception": false,
     "start_time": "2024-07-31T08:48:44.369602",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 关闭wandb,不要深度学习的实验跟踪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4dfcb36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T08:48:44.419697Z",
     "iopub.status.busy": "2024-07-31T08:48:44.419002Z",
     "iopub.status.idle": "2024-07-31T08:48:44.424017Z",
     "shell.execute_reply": "2024-07-31T08:48:44.423051Z"
    },
    "papermill": {
     "duration": 0.024427,
     "end_time": "2024-07-31T08:48:44.426166",
     "exception": false,
     "start_time": "2024-07-31T08:48:44.401739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os#与操作系统进行交互的库\n",
    "#Wandb是Weights & Biases的缩写，它是一个用于机器学习实验跟踪和可视化的工具和平台。\n",
    "#关闭wandb。\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a539008",
   "metadata": {
    "papermill": {
     "duration": 0.016063,
     "end_time": "2024-07-31T08:48:44.458699",
     "exception": false,
     "start_time": "2024-07-31T08:48:44.442636",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 大语言模型微调的脚本,下面是参数的详细解释:\n",
    "\n",
    "- PRE_SEQ_LEN:训练过程中大模型输入序列的最大长度。这是传入大模型的一个参数,这个参数是影响大模型内部的设置的,而后面出现的max_source_length是控制文本的最大长度,是影响文本序列的。\n",
    "\n",
    "- LR:学习率\n",
    "\n",
    "- CUDA_VISIBLE_DEVICES=0:使用第一个GPU设备加速。\n",
    "\n",
    "- python3 ChatGLM-6B/ptuning/main.py:用python3运行微调的脚本。\n",
    "\n",
    "- do_train:执行训练的过程。\n",
    "\n",
    "- train_file、validation_file,训练数据和验证数据的路径,这里选择了同一个数据集。\n",
    "\n",
    "- prompt_column:提示词列,用户问AI的question文本的名称,这里是instruction。\n",
    "\n",
    "- response_column:AI回复用户问题的文本的名称,这里是'output'。\n",
    "\n",
    "- overwrite_cache:允许脚本覆盖现有的缓存文件。\n",
    "\n",
    "- model_name_or_path:指定了模型的名称或路径,这里使用的是量化为 4 位的 ChatGLM-6B 模型。\n",
    "\n",
    "- output_dir:输出文本的路径。\n",
    "\n",
    "- overwrite_output_dir:如果output_dir这里的文件夹已经存在,运行的时候会覆盖掉上一次的文件夹。\n",
    "\n",
    "- max_source_length:用户问AI问题的最大文本长度。\n",
    "\n",
    "- max_target_length:AI回答问题的最大文本长度。\n",
    "\n",
    "- per_device_train_batch_size:训练的时候每次训练2个数据。\n",
    "\n",
    "- per_device_eval_batch_size:验证的时候使用1个数据。\n",
    "\n",
    "- gradient_accumulation_steps:梯度积累几个batch之后更新一次参数。\n",
    "\n",
    "- predict_with_generate:模型预测的时候使用generate生成文本,有助于生成更加自然和连贯的文本。\n",
    "\n",
    "- max_steps:训练多少个steps。\n",
    "\n",
    "- logging_steps:多少个steps输出一次日志。\n",
    "\n",
    "- save_steps:多少个steps保存一个模型。\n",
    "\n",
    "- learning_rate:学习率\n",
    "\n",
    "- pre_seq_len:训练过程中大模型输入序列的最大长度\n",
    " \n",
    "- quantization_bit:指定了模型的量化位数为4位量化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81c3596d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T08:48:44.492940Z",
     "iopub.status.busy": "2024-07-31T08:48:44.492569Z",
     "iopub.status.idle": "2024-07-31T10:05:51.176732Z",
     "shell.execute_reply": "2024-07-31T10:05:51.175103Z"
    },
    "papermill": {
     "duration": 4626.704763,
     "end_time": "2024-07-31T10:05:51.179805",
     "exception": false,
     "start_time": "2024-07-31T08:48:44.475042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-31 08:48:52.444986: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-07-31 08:48:52.445039: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-07-31 08:48:52.446291: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\r\n",
      "/opt/conda/lib/python3.10/site-packages/datasets/load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\r\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\r\n",
      "  warnings.warn(\r\n",
      "Generating train split: 3729 examples [00:00, 39266.81 examples/s]\r\n",
      "Generating validation split: 3729 examples [00:00, 152684.67 examples/s]\r\n",
      "[INFO|configuration_utils.py:666] 2024-07-31 08:48:55,376 >> loading configuration file chatglm-6b-int4/config.json\r\n",
      "[WARNING|configuration_auto.py:905] 2024-07-31 08:48:55,376 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\r\n",
      "[INFO|configuration_utils.py:666] 2024-07-31 08:48:55,487 >> loading configuration file chatglm-6b-int4/config.json\r\n",
      "[INFO|configuration_utils.py:720] 2024-07-31 08:48:55,488 >> Model config ChatGLMConfig {\r\n",
      "  \"_name_or_path\": \"chatglm-6b-int4\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"ChatGLMModel\"\r\n",
      "  ],\r\n",
      "  \"auto_map\": {\r\n",
      "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\r\n",
      "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\r\n",
      "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\r\n",
      "  },\r\n",
      "  \"bos_token_id\": 130004,\r\n",
      "  \"eos_token_id\": 130005,\r\n",
      "  \"gmask_token_id\": 130001,\r\n",
      "  \"hidden_size\": 4096,\r\n",
      "  \"inner_hidden_size\": 16384,\r\n",
      "  \"layernorm_epsilon\": 1e-05,\r\n",
      "  \"mask_token_id\": 130000,\r\n",
      "  \"max_sequence_length\": 2048,\r\n",
      "  \"model_type\": \"chatglm\",\r\n",
      "  \"num_attention_heads\": 32,\r\n",
      "  \"num_layers\": 28,\r\n",
      "  \"pad_token_id\": 3,\r\n",
      "  \"position_encoding_2d\": true,\r\n",
      "  \"pre_seq_len\": null,\r\n",
      "  \"prefix_projection\": false,\r\n",
      "  \"quantization_bit\": 4,\r\n",
      "  \"quantization_embeddings\": false,\r\n",
      "  \"torch_dtype\": \"float16\",\r\n",
      "  \"transformers_version\": \"4.27.1\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"vocab_size\": 130528\r\n",
      "}\r\n",
      "\r\n",
      "[WARNING|tokenization_auto.py:652] 2024-07-31 08:48:55,488 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\r\n",
      "[INFO|tokenization_utils_base.py:1800] 2024-07-31 08:48:55,608 >> loading file ice_text.model\r\n",
      "[INFO|tokenization_utils_base.py:1800] 2024-07-31 08:48:55,608 >> loading file added_tokens.json\r\n",
      "[INFO|tokenization_utils_base.py:1800] 2024-07-31 08:48:55,608 >> loading file special_tokens_map.json\r\n",
      "[INFO|tokenization_utils_base.py:1800] 2024-07-31 08:48:55,608 >> loading file tokenizer_config.json\r\n",
      "[WARNING|auto_factory.py:456] 2024-07-31 08:48:55,887 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\r\n",
      "[INFO|modeling_utils.py:2400] 2024-07-31 08:48:56,036 >> loading weights file chatglm-6b-int4/pytorch_model.bin\r\n",
      "[INFO|configuration_utils.py:575] 2024-07-31 08:48:58,384 >> Generate config GenerationConfig {\r\n",
      "  \"_from_model_config\": true,\r\n",
      "  \"bos_token_id\": 130004,\r\n",
      "  \"eos_token_id\": 130005,\r\n",
      "  \"pad_token_id\": 3,\r\n",
      "  \"transformers_version\": \"4.27.1\"\r\n",
      "}\r\n",
      "\r\n",
      "No compiled kernel found.\r\n",
      "Compiling kernels : /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.c\r\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.c -shared -o /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.so\r\n",
      "Load kernel : /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.so\r\n",
      "Setting CPU quantization kernel threads to 2\r\n",
      "Parallel kernel is not recommended when parallel num < 4.\r\n",
      "Using quantization cache\r\n",
      "Applying quantization to glm layers\r\n",
      "[INFO|modeling_utils.py:3032] 2024-07-31 08:49:01,323 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\r\n",
      "\r\n",
      "[WARNING|modeling_utils.py:3034] 2024-07-31 08:49:01,323 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at chatglm-6b-int4 and are newly initialized: ['transformer.prefix_encoder.embedding.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "[INFO|modeling_utils.py:2690] 2024-07-31 08:49:01,398 >> Generation config file not found, using a generation config created from the model config.\r\n",
      "Quantized to 4 bit\r\n",
      "No compiled kernel found.\r\n",
      "Compiling kernels : /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.c\r\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.c -shared -o /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.so\r\n",
      "Load kernel : /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.so\r\n",
      "Setting CPU quantization kernel threads to 2\r\n",
      "Parallel kernel is not recommended when parallel num < 4.\r\n",
      "Running tokenizer on train dataset: 100%|█| 3729/3729 [00:07<00:00, 475.95 examp\r\n",
      "input_ids [5, 68301, 6, 67703, 65026, 64173, 65404, 64679, 63833, 64438, 6, 70833, 67533, 68301, 64003, 63863, 119641, 76285, 6, 67512, 64928, 66399, 110370, 80001, 125806, 130001, 130004, 5, 99246, 125806, 71066, 95268, 63867, 64703, 63829, 110047, 63825, 63823, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\r\n",
      "inputs 小姐,别的秀女都在求中选,唯有咱们小姐想被撂牌子,菩萨一定记得真真儿的—— 嘘——都说许愿说破是不灵的。\r\n",
      "label_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 99246, 125806, 71066, 95268, 63867, 64703, 63829, 110047, 63825, 63823, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\r\n",
      "labels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 嘘——都说许愿说破是不灵的。<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\r\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\r\n",
      "  warnings.warn(\r\n",
      "  0%|                                                   | 0/500 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 4.42, 'learning_rate': 0.045000000000000005, 'epoch': 0.11}\r\n",
      " 10%|████                                    | 50/500 [07:26<1:09:00,  9.20s/it]Saving PrefixEncoder\r\n",
      "[INFO|configuration_utils.py:457] 2024-07-31 08:56:39,242 >> Configuration saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-50/config.json\r\n",
      "[INFO|configuration_utils.py:362] 2024-07-31 08:56:39,245 >> Configuration saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-50/generation_config.json\r\n",
      "[INFO|modeling_utils.py:1762] 2024-07-31 08:56:39,719 >> Model weights saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-50/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:2163] 2024-07-31 08:56:39,721 >> tokenizer config file saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-50/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2170] 2024-07-31 08:56:39,721 >> Special tokens file saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-50/special_tokens_map.json\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 3.9912, 'learning_rate': 0.04000000000000001, 'epoch': 0.21}\r\n",
      " 20%|███████▊                               | 100/500 [15:07<1:01:19,  9.20s/it]Saving PrefixEncoder\r\n",
      "[INFO|configuration_utils.py:457] 2024-07-31 09:04:19,721 >> Configuration saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-100/config.json\r\n",
      "[INFO|configuration_utils.py:362] 2024-07-31 09:04:19,723 >> Configuration saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-100/generation_config.json\r\n",
      "[INFO|modeling_utils.py:1762] 2024-07-31 09:04:20,203 >> Model weights saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-100/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:2163] 2024-07-31 09:04:20,205 >> tokenizer config file saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-100/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2170] 2024-07-31 09:04:20,205 >> Special tokens file saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-100/special_tokens_map.json\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 3.9479, 'learning_rate': 0.034999999999999996, 'epoch': 0.32}\r\n",
      " 30%|████████████▎                            | 150/500 [22:48<53:34,  9.18s/it]Saving PrefixEncoder\r\n",
      "[INFO|configuration_utils.py:457] 2024-07-31 09:12:00,676 >> Configuration saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-150/config.json\r\n",
      "[INFO|configuration_utils.py:362] 2024-07-31 09:12:00,678 >> Configuration saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-150/generation_config.json\r\n",
      "[INFO|modeling_utils.py:1762] 2024-07-31 09:12:01,147 >> Model weights saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-150/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:2163] 2024-07-31 09:12:01,148 >> tokenizer config file saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-150/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2170] 2024-07-31 09:12:01,149 >> Special tokens file saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-150/special_tokens_map.json\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 3.8422, 'learning_rate': 0.03, 'epoch': 0.43}\r\n",
      " 40%|████████████████▍                        | 200/500 [30:29<45:56,  9.19s/it]Saving PrefixEncoder\r\n",
      "[INFO|configuration_utils.py:457] 2024-07-31 09:19:41,832 >> Configuration saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-200/config.json\r\n",
      "[INFO|configuration_utils.py:362] 2024-07-31 09:19:41,835 >> Configuration saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-200/generation_config.json\r\n",
      "[INFO|modeling_utils.py:1762] 2024-07-31 09:19:42,307 >> Model weights saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-200/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:2163] 2024-07-31 09:19:42,309 >> tokenizer config file saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-200/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2170] 2024-07-31 09:19:42,309 >> Special tokens file saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-200/special_tokens_map.json\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 3.8839, 'learning_rate': 0.025, 'epoch': 0.54}\r\n",
      " 50%|████████████████████▌                    | 250/500 [38:10<38:19,  9.20s/it]Saving PrefixEncoder\r\n",
      "[INFO|configuration_utils.py:457] 2024-07-31 09:27:22,840 >> Configuration saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-250/config.json\r\n",
      "[INFO|configuration_utils.py:362] 2024-07-31 09:27:22,843 >> Configuration saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-250/generation_config.json\r\n",
      "[INFO|modeling_utils.py:1762] 2024-07-31 09:27:23,320 >> Model weights saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-250/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:2163] 2024-07-31 09:27:23,321 >> tokenizer config file saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-250/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2170] 2024-07-31 09:27:23,321 >> Special tokens file saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-250/special_tokens_map.json\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 3.8659, 'learning_rate': 0.020000000000000004, 'epoch': 0.64}\r\n",
      " 60%|████████████████████████▌                | 300/500 [45:50<30:33,  9.17s/it]Saving PrefixEncoder\r\n",
      "[INFO|configuration_utils.py:457] 2024-07-31 09:35:03,097 >> Configuration saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-300/config.json\r\n",
      "[INFO|configuration_utils.py:362] 2024-07-31 09:35:03,100 >> Configuration saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-300/generation_config.json\r\n",
      "[INFO|modeling_utils.py:1762] 2024-07-31 09:35:03,569 >> Model weights saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-300/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:2163] 2024-07-31 09:35:03,571 >> tokenizer config file saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-300/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2170] 2024-07-31 09:35:03,571 >> Special tokens file saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-300/special_tokens_map.json\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 3.9224, 'learning_rate': 0.015, 'epoch': 0.75}\r\n",
      " 70%|████████████████████████████▋            | 350/500 [53:31<22:58,  9.19s/it]Saving PrefixEncoder\r\n",
      "[INFO|configuration_utils.py:457] 2024-07-31 09:42:44,305 >> Configuration saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-350/config.json\r\n",
      "[INFO|configuration_utils.py:362] 2024-07-31 09:42:44,308 >> Configuration saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-350/generation_config.json\r\n",
      "[INFO|modeling_utils.py:1762] 2024-07-31 09:42:44,793 >> Model weights saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-350/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:2163] 2024-07-31 09:42:44,794 >> tokenizer config file saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-350/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2170] 2024-07-31 09:42:44,795 >> Special tokens file saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-350/special_tokens_map.json\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 3.7978, 'learning_rate': 0.010000000000000002, 'epoch': 0.86}\r\n",
      " 80%|███████████████████████████████▏       | 400/500 [1:01:13<15:20,  9.21s/it]Saving PrefixEncoder\r\n",
      "[INFO|configuration_utils.py:457] 2024-07-31 09:50:25,753 >> Configuration saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-400/config.json\r\n",
      "[INFO|configuration_utils.py:362] 2024-07-31 09:50:25,756 >> Configuration saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-400/generation_config.json\r\n",
      "[INFO|modeling_utils.py:1762] 2024-07-31 09:50:26,249 >> Model weights saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-400/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:2163] 2024-07-31 09:50:26,251 >> tokenizer config file saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-400/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2170] 2024-07-31 09:50:26,251 >> Special tokens file saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-400/special_tokens_map.json\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 3.721, 'learning_rate': 0.005000000000000001, 'epoch': 0.97}\r\n",
      " 90%|███████████████████████████████████    | 450/500 [1:08:54<07:39,  9.18s/it]Saving PrefixEncoder\r\n",
      "[INFO|configuration_utils.py:457] 2024-07-31 09:58:06,909 >> Configuration saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-450/config.json\r\n",
      "[INFO|configuration_utils.py:362] 2024-07-31 09:58:06,911 >> Configuration saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-450/generation_config.json\r\n",
      "[INFO|modeling_utils.py:1762] 2024-07-31 09:58:07,396 >> Model weights saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-450/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:2163] 2024-07-31 09:58:07,398 >> tokenizer config file saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-450/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2170] 2024-07-31 09:58:07,398 >> Special tokens file saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-450/special_tokens_map.json\r\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 3.4927, 'learning_rate': 0.0, 'epoch': 1.07}\r\n",
      "100%|███████████████████████████████████████| 500/500 [1:16:34<00:00,  9.19s/it]Saving PrefixEncoder\r\n",
      "[INFO|configuration_utils.py:457] 2024-07-31 10:05:47,034 >> Configuration saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-500/config.json\r\n",
      "[INFO|configuration_utils.py:362] 2024-07-31 10:05:47,036 >> Configuration saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-500/generation_config.json\r\n",
      "[INFO|modeling_utils.py:1762] 2024-07-31 10:05:47,505 >> Model weights saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-500/pytorch_model.bin\r\n",
      "[INFO|tokenization_utils_base.py:2163] 2024-07-31 10:05:47,507 >> tokenizer config file saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-500/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2170] 2024-07-31 10:05:47,507 >> Special tokens file saved in output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-500/special_tokens_map.json\r\n",
      "{'train_runtime': 4595.9195, 'train_samples_per_second': 0.87, 'train_steps_per_second': 0.109, 'train_loss': 3.888508544921875, 'epoch': 1.07}\r\n",
      "100%|███████████████████████████████████████| 500/500 [1:16:35<00:00,  9.19s/it]\r\n",
      "***** train metrics *****\r\n",
      "  epoch                    =       1.07\r\n",
      "  train_loss               =     3.8885\r\n",
      "  train_runtime            = 1:16:35.91\r\n",
      "  train_samples            =       3729\r\n",
      "  train_samples_per_second =       0.87\r\n",
      "  train_steps_per_second   =      0.109\r\n"
     ]
    }
   ],
   "source": [
    "!PRE_SEQ_LEN=256 && LR=5e-2 && CUDA_VISIBLE_DEVICES=0 python3 ChatGLM-6B/ptuning/main.py \\\n",
    "    --do_train \\\n",
    "    --train_file /kaggle/input/chat-huanhuan/huanhuan.json \\\n",
    "    --validation_file /kaggle/input/chat-huanhuan/huanhuan.json \\\n",
    "    --prompt_column instruction \\\n",
    "    --response_column output \\\n",
    "    --overwrite_cache \\\n",
    "    --model_name_or_path chatglm-6b-int4 \\\n",
    "    --output_dir output/infer-chatglm-6b-int4-pt-$PRE_SEQ_LEN-$LR \\\n",
    "    --overwrite_output_dir \\\n",
    "    --max_source_length 256 \\\n",
    "    --max_target_length 128 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --predict_with_generate \\\n",
    "    --max_steps 500 \\\n",
    "    --logging_steps 50 \\\n",
    "    --save_steps 50 \\\n",
    "    --learning_rate $LR \\\n",
    "    --pre_seq_len $PRE_SEQ_LEN \\\n",
    "    --quantization_bit 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf3f4b",
   "metadata": {
    "papermill": {
     "duration": 0.06083,
     "end_time": "2024-07-31T10:05:51.302354",
     "exception": false,
     "start_time": "2024-07-31T10:05:51.241524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.微调后的模型测试\n",
    "\n",
    "#### 由于大模型微调50次就已经具有甄嬛的特点了,所以这里使用了checkpoint50来回答question,各位也可以使用训练500次的checkpoint来回答问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08376116",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T10:05:51.429823Z",
     "iopub.status.busy": "2024-07-31T10:05:51.429373Z",
     "iopub.status.idle": "2024-07-31T10:05:59.120092Z",
     "shell.execute_reply": "2024-07-31T10:05:59.119263Z"
    },
    "papermill": {
     "duration": 7.757995,
     "end_time": "2024-07-31T10:05:59.122540",
     "exception": false,
     "start_time": "2024-07-31T10:05:51.364545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No compiled kernel found.\n",
      "Compiling kernels : /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.c\n",
      "Compiling gcc -O3 -fPIC -pthread -fopenmp -std=c99 /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.c -shared -o /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.so\n",
      "Load kernel : /root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization_kernels_parallel.so\n",
      "Setting CPU quantization kernel threads to 2\n",
      "Parallel kernel is not recommended when parallel num < 4.\n",
      "Using quantization cache\n",
      "Applying quantization to glm layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at chatglm-6b-int4 and are newly initialized: ['transformer.prefix_encoder.embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch#pytorch这个深度学习框架\n",
    "from transformers import AutoConfig#自动下载和配置预训练模型的配置\n",
    "\n",
    "#根据模型路径加载config,允许远程加载代码(trust_remote_code=True),大模型输入序列的最大长度\n",
    "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True, pre_seq_len=256)\n",
    "#根据模型的路径和参数加载模型,允许远程加载代码(trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_path, config=config, trust_remote_code=True)\n",
    "\n",
    "#从二进制(binary)文件中加载模型的状态字典,这个参数字典一般是在某个检查点(checkpoint)保存下来的。\n",
    "prefix_state_dict = torch.load(\"output/infer-chatglm-6b-int4-pt-256-5e-2/checkpoint-50/pytorch_model.bin\")\n",
    "#进行参数的更新\n",
    "new_prefix_state_dict = {}\n",
    "for k, v in prefix_state_dict.items():\n",
    "    new_prefix_state_dict[k[len(\"transformer.prefix_encoder.\"):]] = v\n",
    "model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)\n",
    "\n",
    "#half是半精度浮点数,cuda是移动到GPU上\n",
    "model = model.half().cuda()\n",
    "#将模型prefix_encoder部分的参数换成全精度浮点数float32\n",
    "model.transformer.prefix_encoder.float()\n",
    "#将大模型换成评估模式\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c3066",
   "metadata": {
    "papermill": {
     "duration": 0.059718,
     "end_time": "2024-07-31T10:05:59.305508",
     "exception": false,
     "start_time": "2024-07-31T10:05:59.245790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 最后来看看大语言模型微调后的回答。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94e804b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T10:05:59.433386Z",
     "iopub.status.busy": "2024-07-31T10:05:59.433015Z",
     "iopub.status.idle": "2024-07-31T10:06:03.603902Z",
     "shell.execute_reply": "2024-07-31T10:06:03.602696Z"
    },
    "papermill": {
     "duration": 4.238114,
     "end_time": "2024-07-31T10:06:03.606103",
     "exception": false,
     "start_time": "2024-07-31T10:05:59.367989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:朕的后宫里佳丽三千,你可知朕最宠幸的是谁?\n",
      "response:皇上最宠幸的是谁？是皇后娘娘啊！她可是皇上的恩宠啊！\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, question, history=[])\n",
    "print(f\"question:{question}\\nresponse:{response}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5467682,
     "sourceId": 9065856,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4773.644247,
   "end_time": "2024-07-31T10:06:06.404129",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-31T08:46:32.759882",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
