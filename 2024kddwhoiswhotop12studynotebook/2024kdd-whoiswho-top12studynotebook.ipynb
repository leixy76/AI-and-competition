{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aac28521",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.008312,
     "end_time": "2024-06-18T06:33:29.372456",
     "exception": false,
     "start_time": "2024-06-18T06:33:29.364144",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Created by <a href=\"https://github.com/yunsuxiaozi\">yunsuxiaozi</a> 2024/6/18\n",
    "\n",
    "#### 这里学习的是2024年kddcup的Whoiswho赛道的top12solution.github链接如下:<a href=\"https://github.com/Leo1998-Lu/KDD2024-WhoIsWho\">Leo1998-Lu</a>.这里重点学习的是作者的特征工程,所以只学习了作者的train.py文件.我删除了作者的无用代码,并修正了原作者一部分错误代码,给代码添加了一定的注释方便阅读,并写下了我所学到的新东西以及我的心得体会。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c47fae",
   "metadata": {
    "papermill": {
     "duration": 0.006894,
     "end_time": "2024-06-18T06:33:29.386752",
     "exception": false,
     "start_time": "2024-06-18T06:33:29.379858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.导入必要的库,并且固定随机种子,这里没什么好说的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee725954",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T06:33:29.403246Z",
     "iopub.status.busy": "2024-06-18T06:33:29.402803Z",
     "iopub.status.idle": "2024-06-18T06:33:48.093128Z",
     "shell.execute_reply": "2024-06-18T06:33:48.091938Z"
    },
    "papermill": {
     "duration": 18.70177,
     "end_time": "2024-06-18T06:33:48.095844",
     "exception": false,
     "start_time": "2024-06-18T06:33:29.394074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd#导入csv文件的库\n",
    "import numpy as np#进行矩阵运算的库\n",
    "import json#用于读取和写入json数据格式\n",
    "import re#用于正则表达式提取\n",
    "#model lgb分类模型,日志评估,早停防止过拟合\n",
    "from  lightgbm import LGBMClassifier,log_evaluation,early_stopping\n",
    "#metric\n",
    "from sklearn.metrics import roc_auc_score#导入roc_auc曲线\n",
    "#KFold是直接分成k折,StratifiedKFold还要考虑每种类别的占比\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from tqdm import tqdm#加载进度条的库\n",
    "from collections import Counter#用于对一组元素计数\n",
    "from sklearn.preprocessing import LabelEncoder#将分类数据转换成数值格式\n",
    "#两个词转换成向量的常用模型,CountVectorizer就是简单的词频统计,TfidfVectorizer则是tfidf模型\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "#gensim提供了许多文本向量化的工具和算法\n",
    "from gensim.models import Word2Vec#用来生成词嵌入的算法\n",
    "#转小写,去除标点符号,分词.例如:'He is a very smart boy.'->['he', 'is', 'very', 'smart', 'boy']\n",
    "from gensim.utils import simple_preprocess\n",
    "import gc#垃圾回收模块\n",
    "import warnings#避免一些可以忽略的报错\n",
    "warnings.filterwarnings('ignore')#filterwarnings()方法是用于设置警告过滤器的方法，它可以控制警告信息的输出方式和级别。\n",
    "\n",
    "import random#提供了一些用于生成随机数的函数\n",
    "#设置随机种子,保证模型可以复现\n",
    "def seed_everything(seed):\n",
    "    np.random.seed(seed)#numpy的随机种子\n",
    "    random.seed(seed)#python内置的随机种子\n",
    "seed_everything(seed=2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd2c9dc",
   "metadata": {
    "papermill": {
     "duration": 0.007207,
     "end_time": "2024-06-18T06:33:48.110681",
     "exception": false,
     "start_time": "2024-06-18T06:33:48.103474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.读取数据,这里也没什么好说的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e7c4608",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T06:33:48.127741Z",
     "iopub.status.busy": "2024-06-18T06:33:48.126775Z",
     "iopub.status.idle": "2024-06-18T06:34:21.023723Z",
     "shell.execute_reply": "2024-06-18T06:34:21.022568Z"
    },
    "papermill": {
     "duration": 32.908483,
     "end_time": "2024-06-18T06:34:21.026445",
     "exception": false,
     "start_time": "2024-06-18T06:33:48.117962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/2024kddcupwhoiswho/2024kddcupwhoiswho/train_author.json\",encoding='utf-8') as f:\n",
    "    train_author=json.load(f)\n",
    "with open(\"/kaggle/input/2024kddcupwhoiswho/2024kddcupwhoiswho/ind_test_author_filter_public.json\",encoding='utf-8') as f:\n",
    "    test_author=json.load(f)\n",
    "with open(\"/kaggle/input/2024kddcupwhoiswho/2024kddcupwhoiswho/pid_to_info_all.json\",encoding='utf-8') as f:\n",
    "    pid_to_info=json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57399cd",
   "metadata": {
    "papermill": {
     "duration": 0.00725,
     "end_time": "2024-06-18T06:34:21.041525",
     "exception": false,
     "start_time": "2024-06-18T06:34:21.034275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.构造训练数据.\n",
    "\n",
    "#### 这里的初始特征除了label有13个，下面是详细含义。\n",
    "\n",
    "- author_id:就是一个编号,不具有具体的含义.\n",
    "\n",
    "- name:作者名字.\n",
    "\n",
    "- author:原始数据中的列表,里面有作者名和组织的字典。(例如[{'name':XXX,'org':XXX}])\n",
    "\n",
    "- venue:就是把原始数据中地点稍微做了一些文本的处理(clean_text)\n",
    "\n",
    "- keywords:原来是一个字符串的列表,例如['a','b'],结果变成字符串\"['a','b']\"后做了文本的处理。(这里比较巧妙的是如果keywords是空列表就用title来代替。)\n",
    "\n",
    "- text: title+空格+keywords+空格+abstract\n",
    "\n",
    "- x1~x6:title的长度、abstract的长度、keywords的个数、作者的个数、2024-year,地点的长度.这里不知道为什么要2024-year,传入year和2024-year对树模型应该是没有影响的。\n",
    "\n",
    "- top_author:这里选择前5个作者用逗号拼接在了一起。\n",
    "\n",
    "#### 我学到了什么:\n",
    "\n",
    "##### 1.首先是对抗学习,这个任务中正负样本比例是不平衡的,负样本约为正样本的十分之一。他这里对负样本做了2个label,一次构造成正样本,一次构造成负样本,叫做对抗学习。据我目前的理解,样本同时为正样本和负样本,这样模型对于这些样本肯定学不好,如果预测为正样本,那么负样本的损失就会大,预测为负样本,那么正样本的损失就会大,模型就会更加努力的学习这些样本,从而就能学的比较好。也有可能是最后预测概率为0.5的时候损失会最小,正负样本比例不平衡的时候对负样本的预测概率为0.5也已经很不错了。\n",
    "\n",
    "##### 2.keywords为空列表的时候用title来替代,这点我没有想到.title,abstract,keywords都含有文章的主要信息,abstract太长,长度和keywords相差太大,所以当keywords缺失的时候就可以用title来填充。\n",
    "\n",
    "##### 3.top5author.有些论文可能会有几千个作者,如果把全部作者都进行统计很容易会有噪音.对论文越重要的作者往往排的位置会越靠前,所以他这里选择了top5作者来进行统计。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c55471e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T06:34:21.058874Z",
     "iopub.status.busy": "2024-06-18T06:34:21.058466Z",
     "iopub.status.idle": "2024-06-18T06:34:24.759731Z",
     "shell.execute_reply": "2024-06-18T06:34:24.758353Z"
    },
    "papermill": {
     "duration": 3.712805,
     "end_time": "2024-06-18T06:34:24.762045",
     "exception": false,
     "start_time": "2024-06-18T06:34:21.049240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape:(165594, 14),labels.shape:(165594,)\n",
      "np.mean(labels):0.895618198727007\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>venue</th>\n",
       "      <th>keywords</th>\n",
       "      <th>text</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>top_author</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iki037dt</td>\n",
       "      <td>atsushi ochiai</td>\n",
       "      <td>[{'name': 'Hideaki Bando', 'org': ''}, {'name'...</td>\n",
       "      <td>annals of oncology</td>\n",
       "      <td>the power of the high-sensitive kras test to p...</td>\n",
       "      <td>THE POWER OF THE HIGH-SENSITIVE KRAS TEST TO P...</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>Hideaki Bando,Takayuki Yoshino,Kazuya Tuchihar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iki037dt</td>\n",
       "      <td>atsushi ochiai</td>\n",
       "      <td>[{'name': 'jintetsu soh', 'org': ''}, {'name':...</td>\n",
       "      <td>the journal of urology</td>\n",
       "      <td>1812 nicardipine versus saline injection as tr...</td>\n",
       "      <td>1812 NICARDIPINE VERSUS SALINE INJECTION AS TR...</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>jintetsu soh,yoshio naya,atsushi ochiai,yasuyu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iki037dt</td>\n",
       "      <td>atsushi ochiai</td>\n",
       "      <td>[{'name': 'M Inaba', 'org': 'Department of Uro...</td>\n",
       "      <td>ultrasound in medicine &amp; biology</td>\n",
       "      <td>vesicoureteral reflux, bladder hypertrophy, ch...</td>\n",
       "      <td>Possible use of ultrasound estimated bladder w...</td>\n",
       "      <td>100</td>\n",
       "      <td>986</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>32</td>\n",
       "      <td>M Inaba,O Ukimura,A Kawauchi,T Iwata,M Kanazawa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iki037dt</td>\n",
       "      <td>atsushi ochiai</td>\n",
       "      <td>[{'name': 'hisao ito', 'org': ''}, {'name': 'm...</td>\n",
       "      <td>journal of steroid biochemistry</td>\n",
       "      <td>interaction between epidermal growth factor an...</td>\n",
       "      <td>Interaction between epidermal growth factor an...</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>37</td>\n",
       "      <td>31</td>\n",
       "      <td>hisao ito,masanobu yamamoto,naruhito oda,fumio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iki037dt</td>\n",
       "      <td>atsushi ochiai</td>\n",
       "      <td>[{'name': 'yasutoshi kuboki', 'org': 'Departme...</td>\n",
       "      <td>annals of oncology</td>\n",
       "      <td>gastric cancer, amplification, mutation, next-...</td>\n",
       "      <td>Comprehensive Analyses Using Next-Generation S...</td>\n",
       "      <td>133</td>\n",
       "      <td>1629</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>yasutoshi kuboki,seigo yamashita,tohru niwa,to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author_id            name  \\\n",
       "0  Iki037dt  atsushi ochiai   \n",
       "1  Iki037dt  atsushi ochiai   \n",
       "2  Iki037dt  atsushi ochiai   \n",
       "3  Iki037dt  atsushi ochiai   \n",
       "4  Iki037dt  atsushi ochiai   \n",
       "\n",
       "                                              author  \\\n",
       "0  [{'name': 'Hideaki Bando', 'org': ''}, {'name'...   \n",
       "1  [{'name': 'jintetsu soh', 'org': ''}, {'name':...   \n",
       "2  [{'name': 'M Inaba', 'org': 'Department of Uro...   \n",
       "3  [{'name': 'hisao ito', 'org': ''}, {'name': 'm...   \n",
       "4  [{'name': 'yasutoshi kuboki', 'org': 'Departme...   \n",
       "\n",
       "                              venue  \\\n",
       "0                annals of oncology   \n",
       "1            the journal of urology   \n",
       "2  ultrasound in medicine & biology   \n",
       "3   journal of steroid biochemistry   \n",
       "4                annals of oncology   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  the power of the high-sensitive kras test to p...   \n",
       "1  1812 nicardipine versus saline injection as tr...   \n",
       "2  vesicoureteral reflux, bladder hypertrophy, ch...   \n",
       "3  interaction between epidermal growth factor an...   \n",
       "4  gastric cancer, amplification, mutation, next-...   \n",
       "\n",
       "                                                text   x1    x2  x3  x4  x5  \\\n",
       "0  THE POWER OF THE HIGH-SENSITIVE KRAS TEST TO P...  120     0   0  14  14   \n",
       "1  1812 NICARDIPINE VERSUS SALINE INJECTION AS TR...  123     0   0   8  13   \n",
       "2  Possible use of ultrasound estimated bladder w...  100   986   4   9  23   \n",
       "3  Interaction between epidermal growth factor an...  103     0   0  10  37   \n",
       "4  Comprehensive Analyses Using Next-Generation S...  133  1629   5  10   9   \n",
       "\n",
       "   x6                                         top_author  label  \n",
       "0  18  Hideaki Bando,Takayuki Yoshino,Kazuya Tuchihar...      1  \n",
       "1  22  jintetsu soh,yoshio naya,atsushi ochiai,yasuyu...      1  \n",
       "2  32    M Inaba,O Ukimura,A Kawauchi,T Iwata,M Kanazawa      1  \n",
       "3  31  hisao ito,masanobu yamamoto,naruhito oda,fumio...      1  \n",
       "4  18  yasutoshi kuboki,seigo yamashita,tohru niwa,to...      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对文本进行清理的函数\n",
    "def clean_text(text):\n",
    "    #删除标点符号,至于为什么删除这些也许是试出来效果好吧,要问就问原作者\n",
    "    punctuation=['[',']','{','}',\"'\",':']\n",
    "    for p in punctuation:\n",
    "        text = text.replace(p,'')\n",
    "    #字符串转小写\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "df=[]\n",
    "labels=[]\n",
    "columns=['author_id',#就是一个编号\n",
    "              'name',#作者的名字\n",
    "              'author',#原始数据中['name','org']\n",
    "              'venue',#原始数据地点(clean)\n",
    "              'keywords',#原始数据中列表变成字符串,keywords没有用title替代.\n",
    "              'text',#title+keywords+abstract\n",
    "              'x1',#len_title\n",
    "              'x2',#len_abstract\n",
    "              'x3',#len_keywords\n",
    "              'x4',#len_authors\n",
    "              'x5',#2024-year\n",
    "              'x6',#len(venue)\n",
    "              'top_author',#前5个作者拼接\n",
    "]\n",
    "for id,person_info in train_author.items():\n",
    "    names = person_info['name'].lower()#作者的名字\n",
    "    for text_id in person_info['normal_data']:\n",
    "        feat=pid_to_info[text_id]\n",
    "        #一篇论文选取前5个作者\n",
    "        author_names = [x['name'] for x in feat['authors'][:5]]\n",
    "        #作者用逗号拼接在一起\n",
    "        ans = ','.join(author_names)\n",
    "        #text是title+keywords+abstract,中间用空格隔开。\n",
    "        text = str(feat['title'])+' '+ clean_text(str(feat['keywords']))+' '+str(feat['abstract'])\n",
    "        #feat['keywords']=[],则str(feat['keywords'])='[]',len()=2\n",
    "        if len(str(feat['keywords']))<3:#如果没有关键词就用title来替换关键词\n",
    "            keywords = feat['title'].lower()\n",
    "        else:#为了把[]去掉就需要clean_text函数\n",
    "            keywords = clean_text(str( feat['keywords'])) \n",
    "        try:\n",
    "            df.append(\n",
    "                [id,names,feat['authors'],clean_text(str(feat['venue'])),keywords,text,len(feat['title']),len(feat['abstract']),len(feat['keywords']),len(feat['authors'])\n",
    "                 ,2024-int(feat['year']), len(feat['venue']),ans]\n",
    "                 )\n",
    "        except:\n",
    "            df.append(\n",
    "                [id,names,feat['authors'],clean_text(str(feat['venue'])),keywords,text,len(feat['title']),len(feat['abstract']),len(feat['keywords']),len(feat['authors'])\n",
    "                 ,20,10,ans]\n",
    "                 )\n",
    "        labels.append(1)\n",
    "    for text_id in person_info['outliers']:\n",
    "        feat=pid_to_info[text_id]\n",
    "        author_names = [x['name'] for x in feat['authors'][:5]]\n",
    "        ans = ','.join(author_names)\n",
    "        text = feat['title']+ clean_text(str(feat['keywords'])) +feat['abstract']\n",
    "   \n",
    "        if len(str(feat['keywords']))<3:\n",
    "            keywords = feat['title'].lower()\n",
    "        else:\n",
    "            keywords = clean_text(str( feat['keywords']))\n",
    "        #对抗性学习 一个样本既标记成1又标记为0 \n",
    "        for i in range(2):\n",
    "            try:\n",
    "                df.append(\n",
    "                    [id,names,feat['authors'],clean_text(str(feat['venue'])),keywords,text,\n",
    "                     len(feat['title']),len(feat['abstract']),len(feat['keywords']),len(feat['authors'])\n",
    "                     ,2024-int(feat['year']), len(feat['venue']),ans]\n",
    "                     )\n",
    "            except:\n",
    "                df.append(\n",
    "                    [id,names,feat['authors'],clean_text(str(feat['venue'])),keywords,text,\n",
    "                     len(feat['title']),len(feat['abstract']),len(feat['keywords']),len(feat['authors'])\n",
    "                     ,20,10,ans]\n",
    "                     )\n",
    "            labels.append(i)\n",
    "labels=np.array(labels)\n",
    "df=pd.DataFrame(df)\n",
    "df['label']=labels\n",
    "print(f\"df.shape:{df.shape},labels.shape:{labels.shape}\")\n",
    "print(f\"np.mean(labels):{np.mean(labels)}\")\n",
    "df.columns = columns+['label']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3bba27",
   "metadata": {
    "papermill": {
     "duration": 0.007628,
     "end_time": "2024-06-18T06:34:24.778229",
     "exception": false,
     "start_time": "2024-06-18T06:34:24.770601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 测试数据的特征构造和训练数据一致,这里就不做重复说明了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61dd83af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T06:34:24.796019Z",
     "iopub.status.busy": "2024-06-18T06:34:24.795594Z",
     "iopub.status.idle": "2024-06-18T06:34:28.331035Z",
     "shell.execute_reply": "2024-06-18T06:34:28.329720Z"
    },
    "papermill": {
     "duration": 3.547624,
     "end_time": "2024-06-18T06:34:28.333955",
     "exception": false,
     "start_time": "2024-06-18T06:34:24.786331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_feats.shape:(116262, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>venue</th>\n",
       "      <th>keywords</th>\n",
       "      <th>text</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>top_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fkb16wn7</td>\n",
       "      <td>jitendra malik</td>\n",
       "      <td>[{'name': 'Saurabh Gupta', 'org': 'university ...</td>\n",
       "      <td>eccv (7)</td>\n",
       "      <td>learning rich features from rgb-d images for o...</td>\n",
       "      <td>Learning Rich Features from RGB-D Images for O...</td>\n",
       "      <td>79</td>\n",
       "      <td>1295</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>Saurabh Gupta,Ross B. Girshick,Pablo Arbelaez,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fkb16wn7</td>\n",
       "      <td>jitendra malik</td>\n",
       "      <td>[{'name': 'João Carreira', 'org': 'University ...</td>\n",
       "      <td>computer vision and pattern recognition</td>\n",
       "      <td>human pose estimation with iterative error fee...</td>\n",
       "      <td>Human Pose Estimation with Iterative Error Fee...</td>\n",
       "      <td>51</td>\n",
       "      <td>1017</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>João Carreira,pulkit agrawal,katerina fragkiad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fkb16wn7</td>\n",
       "      <td>jitendra malik</td>\n",
       "      <td>[{'name': 'Ashish Kumar', 'org': 'Univ Calif B...</td>\n",
       "      <td>advances in neural information processing syst...</td>\n",
       "      <td>visual memory</td>\n",
       "      <td>Visual Memory for Robust Path Following. visua...</td>\n",
       "      <td>40</td>\n",
       "      <td>710</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>Ashish Kumar,Saurabh Gupta,David F. Fouhey,Ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fkb16wn7</td>\n",
       "      <td>jitendra malik</td>\n",
       "      <td>[{'name': 'Li Yi', 'org': ''}, {'name': 'Lin S...</td>\n",
       "      <td>arxiv computer vision and pattern recognition</td>\n",
       "      <td>large-scale 3d shape reconstruction and segmen...</td>\n",
       "      <td>Large-Scale 3D Shape Reconstruction and Segmen...</td>\n",
       "      <td>74</td>\n",
       "      <td>687</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>46</td>\n",
       "      <td>Li Yi,Lin Shao,Manolis Savva,Haibin Huang,Yang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fkb16wn7</td>\n",
       "      <td>jitendra malik</td>\n",
       "      <td>[{'name': 'Ke Li', 'org': 'University of Calif...</td>\n",
       "      <td>icml</td>\n",
       "      <td>fast k-nearest neighbour search via prioritize...</td>\n",
       "      <td>Fast k-Nearest Neighbour Search via Prioritize...</td>\n",
       "      <td>52</td>\n",
       "      <td>1061</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Ke Li,Jitendra Malik</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author_id            name  \\\n",
       "0  Fkb16wn7  jitendra malik   \n",
       "1  Fkb16wn7  jitendra malik   \n",
       "2  Fkb16wn7  jitendra malik   \n",
       "3  Fkb16wn7  jitendra malik   \n",
       "4  Fkb16wn7  jitendra malik   \n",
       "\n",
       "                                              author  \\\n",
       "0  [{'name': 'Saurabh Gupta', 'org': 'university ...   \n",
       "1  [{'name': 'João Carreira', 'org': 'University ...   \n",
       "2  [{'name': 'Ashish Kumar', 'org': 'Univ Calif B...   \n",
       "3  [{'name': 'Li Yi', 'org': ''}, {'name': 'Lin S...   \n",
       "4  [{'name': 'Ke Li', 'org': 'University of Calif...   \n",
       "\n",
       "                                               venue  \\\n",
       "0                                           eccv (7)   \n",
       "1            computer vision and pattern recognition   \n",
       "2  advances in neural information processing syst...   \n",
       "3      arxiv computer vision and pattern recognition   \n",
       "4                                               icml   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  learning rich features from rgb-d images for o...   \n",
       "1  human pose estimation with iterative error fee...   \n",
       "2                                      visual memory   \n",
       "3  large-scale 3d shape reconstruction and segmen...   \n",
       "4  fast k-nearest neighbour search via prioritize...   \n",
       "\n",
       "                                                text  x1    x2  x3  x4  x5  \\\n",
       "0  Learning Rich Features from RGB-D Images for O...  79  1295   0   4  10   \n",
       "1  Human Pose Estimation with Iterative Error Fee...  51  1017   0   4   8   \n",
       "2  Visual Memory for Robust Path Following. visua...  40   710   1   5   6   \n",
       "3  Large-Scale 3D Shape Reconstruction and Segmen...  74   687   0  50   7   \n",
       "4  Fast k-Nearest Neighbour Search via Prioritize...  52  1061   0   2   7   \n",
       "\n",
       "   x6                                         top_author  \n",
       "0   8  Saurabh Gupta,Ross B. Girshick,Pablo Arbelaez,...  \n",
       "1  39  João Carreira,pulkit agrawal,katerina fragkiad...  \n",
       "2  64  Ashish Kumar,Saurabh Gupta,David F. Fouhey,Ser...  \n",
       "3  46  Li Yi,Lin Shao,Manolis Savva,Haibin Huang,Yang...  \n",
       "4   4                               Ke Li,Jitendra Malik  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feats=[]\n",
    "for id,person_info in test_author.items():\n",
    "    names = person_info['name'].lower()\n",
    "    for text_id in person_info['papers']:\n",
    "        feat=pid_to_info[text_id]\n",
    "        author_names = [x['name'] for x in feat['authors'][:5]]\n",
    "        ans = ','.join(author_names)\n",
    "        text = str(feat['title'])+' '+ clean_text(str(feat['keywords']))+' '+str(feat['abstract'])\n",
    "\n",
    "        if len(str(feat['keywords']))<3:\n",
    "            keywords = feat['title'].lower()\n",
    "        else:\n",
    "            keywords = clean_text(str(feat['keywords']))\n",
    "        try:\n",
    "            test_feats.append(\n",
    "                [id,names, feat['authors'],clean_text(str(feat['venue'])),keywords,text, len(feat['title']),len(feat['abstract']),len(feat['keywords']),len(feat['authors'])\n",
    "                 ,2024-int(feat['year']), len(feat['venue']),ans]\n",
    "                 )\n",
    "        except:\n",
    "            test_feats.append(\n",
    "                [id,names, feat['authors'],clean_text(str(feat['venue'])),keywords,text, len(feat['title']),len(feat['abstract']),len(feat['keywords']),len(feat['authors'])\n",
    "                 ,20,10,ans]\n",
    "                 )\n",
    "test_feats=pd.DataFrame(test_feats)\n",
    "print(f\"valid_feats.shape:{test_feats.shape}\")\n",
    "test_feats.columns = columns\n",
    "test_feats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41740d1b",
   "metadata": {
    "papermill": {
     "duration": 0.008946,
     "end_time": "2024-06-18T06:34:28.352182",
     "exception": false,
     "start_time": "2024-06-18T06:34:28.343236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.gg函数\n",
    "\n",
    "### 首先看这个作者是不是top5作者,这非常重要,如果一篇论文有1000个作者,这个作者又是最后一个,那可能就是挂个名字,没有做什么贡献。然后统计了字符串的长度特征,其中author是列表,keywords已经在前面的操作中转成列表了,text是文本,top_author也是文本.将文本转成小写这个操作很常规.最后Labelencoder对作者id和地点转成一个数值."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c129a46a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T06:34:28.372516Z",
     "iopub.status.busy": "2024-06-18T06:34:28.372040Z",
     "iopub.status.idle": "2024-06-18T06:34:37.081495Z",
     "shell.execute_reply": "2024-06-18T06:34:37.080324Z"
    },
    "papermill": {
     "duration": 8.722805,
     "end_time": "2024-06-18T06:34:37.084103",
     "exception": false,
     "start_time": "2024-06-18T06:34:28.361298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>venue</th>\n",
       "      <th>keywords</th>\n",
       "      <th>text</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>top_author</th>\n",
       "      <th>author_ex</th>\n",
       "      <th>author_len</th>\n",
       "      <th>keywords_len</th>\n",
       "      <th>text_len</th>\n",
       "      <th>top_author_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fkb16wn7</td>\n",
       "      <td>127</td>\n",
       "      <td>[{'name': 'Saurabh Gupta', 'org': 'university ...</td>\n",
       "      <td>6079</td>\n",
       "      <td>learning rich features from rgb-d images for o...</td>\n",
       "      <td>learning rich features from rgb-d images for o...</td>\n",
       "      <td>79</td>\n",
       "      <td>1295</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>saurabh gupta,ross b. girshick,pablo arbelaez,...</td>\n",
       "      <td>0</td>\n",
       "      <td>252</td>\n",
       "      <td>79</td>\n",
       "      <td>1376</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fkb16wn7</td>\n",
       "      <td>127</td>\n",
       "      <td>[{'name': 'João Carreira', 'org': 'University ...</td>\n",
       "      <td>5249</td>\n",
       "      <td>human pose estimation with iterative error fee...</td>\n",
       "      <td>human pose estimation with iterative error fee...</td>\n",
       "      <td>51</td>\n",
       "      <td>1017</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>joão carreira,pulkit agrawal,katerina fragkiad...</td>\n",
       "      <td>1</td>\n",
       "      <td>287</td>\n",
       "      <td>51</td>\n",
       "      <td>1070</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fkb16wn7</td>\n",
       "      <td>127</td>\n",
       "      <td>[{'name': 'Ashish Kumar', 'org': 'Univ Calif B...</td>\n",
       "      <td>2261</td>\n",
       "      <td>visual memory</td>\n",
       "      <td>visual memory for robust path following. visua...</td>\n",
       "      <td>40</td>\n",
       "      <td>710</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>ashish kumar,saurabh gupta,david f. fouhey,ser...</td>\n",
       "      <td>0</td>\n",
       "      <td>407</td>\n",
       "      <td>13</td>\n",
       "      <td>765</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fkb16wn7</td>\n",
       "      <td>127</td>\n",
       "      <td>[{'name': 'Li Yi', 'org': ''}, {'name': 'Lin S...</td>\n",
       "      <td>3017</td>\n",
       "      <td>large-scale 3d shape reconstruction and segmen...</td>\n",
       "      <td>large-scale 3d shape reconstruction and segmen...</td>\n",
       "      <td>74</td>\n",
       "      <td>687</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>46</td>\n",
       "      <td>li yi,lin shao,manolis savva,haibin huang,yang...</td>\n",
       "      <td>0</td>\n",
       "      <td>1847</td>\n",
       "      <td>74</td>\n",
       "      <td>763</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fkb16wn7</td>\n",
       "      <td>127</td>\n",
       "      <td>[{'name': 'Ke Li', 'org': 'University of Calif...</td>\n",
       "      <td>8078</td>\n",
       "      <td>fast k-nearest neighbour search via prioritize...</td>\n",
       "      <td>fast k-nearest neighbour search via prioritize...</td>\n",
       "      <td>52</td>\n",
       "      <td>1061</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>ke li,jitendra malik</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>52</td>\n",
       "      <td>1115</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author_id  name                                             author venue  \\\n",
       "0  Fkb16wn7   127  [{'name': 'Saurabh Gupta', 'org': 'university ...  6079   \n",
       "1  Fkb16wn7   127  [{'name': 'João Carreira', 'org': 'University ...  5249   \n",
       "2  Fkb16wn7   127  [{'name': 'Ashish Kumar', 'org': 'Univ Calif B...  2261   \n",
       "3  Fkb16wn7   127  [{'name': 'Li Yi', 'org': ''}, {'name': 'Lin S...  3017   \n",
       "4  Fkb16wn7   127  [{'name': 'Ke Li', 'org': 'University of Calif...  8078   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  learning rich features from rgb-d images for o...   \n",
       "1  human pose estimation with iterative error fee...   \n",
       "2                                      visual memory   \n",
       "3  large-scale 3d shape reconstruction and segmen...   \n",
       "4  fast k-nearest neighbour search via prioritize...   \n",
       "\n",
       "                                                text  x1    x2  x3  x4  x5  \\\n",
       "0  learning rich features from rgb-d images for o...  79  1295   0   4  10   \n",
       "1  human pose estimation with iterative error fee...  51  1017   0   4   8   \n",
       "2  visual memory for robust path following. visua...  40   710   1   5   6   \n",
       "3  large-scale 3d shape reconstruction and segmen...  74   687   0  50   7   \n",
       "4  fast k-nearest neighbour search via prioritize...  52  1061   0   2   7   \n",
       "\n",
       "   x6                                         top_author  author_ex  \\\n",
       "0   8  saurabh gupta,ross b. girshick,pablo arbelaez,...          0   \n",
       "1  39  joão carreira,pulkit agrawal,katerina fragkiad...          1   \n",
       "2  64  ashish kumar,saurabh gupta,david f. fouhey,ser...          0   \n",
       "3  46  li yi,lin shao,manolis savva,haibin huang,yang...          0   \n",
       "4   4                               ke li,jitendra malik          0   \n",
       "\n",
       "   author_len  keywords_len  text_len  top_author_len  \n",
       "0         252            79      1376              60  \n",
       "1         287            51      1070              64  \n",
       "2         407            13       765              71  \n",
       "3        1847            74       763              51  \n",
       "4         127            52      1115              20  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gg(df):\n",
    "    \n",
    "    #一个作者是不是top5主要作者\n",
    "    author_ex = [] \n",
    "    for x,y in zip(df['name'],df['top_author']):\n",
    "        author_ex.append(int(x in y))\n",
    "    df['author_ex'] = author_ex\n",
    "    \n",
    "    for x in ['author','keywords','text','top_author']:\n",
    "        df[f'{x}_len'] = df[x].apply(lambda x: len(str(x)))\n",
    "        \n",
    "    for col in ['text','top_author','name']:#将文本转成小写很常规\n",
    "        df[col] = df[col].apply(lambda x: x.lower())\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    df['name'] = le.fit_transform(df['author_id'])\n",
    "    df['venue'] = le.fit_transform(df['venue'])\n",
    "    df['venue'] = df['venue'].apply(lambda x: str(x))\n",
    "\n",
    "    return df\n",
    "train = gg(df)\n",
    "valid = gg(test_feats)\n",
    "del df,test_feats\n",
    "gc.collect()#手动触发垃圾回收,强制回收由垃圾回收器标记为未使用的内存\n",
    "valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38106a2c",
   "metadata": {
    "papermill": {
     "duration": 0.00882,
     "end_time": "2024-06-18T06:34:37.102001",
     "exception": false,
     "start_time": "2024-06-18T06:34:37.093181",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5.词向量特征\n",
    "\n",
    "#### 这里用tf-idf模型对text,keywords,venue构造特征,用词频统计模型对text,keywords,venue,topauthor构造特征,用word2vec对text构造特征.\n",
    "\n",
    "#### 我其实在比赛中也想过构造词向量特征,不过只统计过title和abstract的top100,同时我是面向训练数据建模的。它这里是面向全部数据建模,虽然不知道允不允许,但是训练集和测试集一致的话效果肯定会更好。\n",
    "\n",
    "#### text里有title+keywords+abstract,信息是最多的,所以用3个模型对其建模,topauthor信息是最少的,所以就用一个模型给它建模."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cf4fad8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T06:34:37.122043Z",
     "iopub.status.busy": "2024-06-18T06:34:37.121651Z",
     "iopub.status.idle": "2024-06-18T07:20:13.892585Z",
     "shell.execute_reply": "2024-06-18T07:20:13.891260Z"
    },
    "papermill": {
     "duration": 2736.810549,
     "end_time": "2024-06-18T07:20:13.921659",
     "exception": false,
     "start_time": "2024-06-18T06:34:37.111110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>name</th>\n",
       "      <th>author</th>\n",
       "      <th>venue</th>\n",
       "      <th>keywords</th>\n",
       "      <th>text</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_246</th>\n",
       "      <th>w2v_247</th>\n",
       "      <th>w2v_248</th>\n",
       "      <th>w2v_249</th>\n",
       "      <th>w2v_250</th>\n",
       "      <th>w2v_251</th>\n",
       "      <th>w2v_252</th>\n",
       "      <th>w2v_253</th>\n",
       "      <th>w2v_254</th>\n",
       "      <th>w2v_255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iki037dt</td>\n",
       "      <td>224</td>\n",
       "      <td>[{'name': 'Hideaki Bando', 'org': ''}, {'name'...</td>\n",
       "      <td>2607</td>\n",
       "      <td>the power of the high-sensitive kras test to p...</td>\n",
       "      <td>the power of the high-sensitive kras test to p...</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254230</td>\n",
       "      <td>-0.591256</td>\n",
       "      <td>-0.047873</td>\n",
       "      <td>-0.518315</td>\n",
       "      <td>-0.170804</td>\n",
       "      <td>-0.094971</td>\n",
       "      <td>0.009196</td>\n",
       "      <td>-0.261595</td>\n",
       "      <td>0.165669</td>\n",
       "      <td>0.259170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iki037dt</td>\n",
       "      <td>224</td>\n",
       "      <td>[{'name': 'jintetsu soh', 'org': ''}, {'name':...</td>\n",
       "      <td>18917</td>\n",
       "      <td>1812 nicardipine versus saline injection as tr...</td>\n",
       "      <td>1812 nicardipine versus saline injection as tr...</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.683561</td>\n",
       "      <td>-0.365802</td>\n",
       "      <td>0.765757</td>\n",
       "      <td>-0.325274</td>\n",
       "      <td>-0.062557</td>\n",
       "      <td>0.341192</td>\n",
       "      <td>0.125490</td>\n",
       "      <td>-0.561785</td>\n",
       "      <td>0.108586</td>\n",
       "      <td>0.246013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iki037dt</td>\n",
       "      <td>224</td>\n",
       "      <td>[{'name': 'M Inaba', 'org': 'Department of Uro...</td>\n",
       "      <td>19400</td>\n",
       "      <td>vesicoureteral reflux, bladder hypertrophy, ch...</td>\n",
       "      <td>possible use of ultrasound estimated bladder w...</td>\n",
       "      <td>100</td>\n",
       "      <td>986</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014613</td>\n",
       "      <td>-0.199763</td>\n",
       "      <td>-0.033882</td>\n",
       "      <td>-0.048321</td>\n",
       "      <td>-0.051605</td>\n",
       "      <td>0.066802</td>\n",
       "      <td>0.160459</td>\n",
       "      <td>-0.193787</td>\n",
       "      <td>0.172351</td>\n",
       "      <td>0.265669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iki037dt</td>\n",
       "      <td>224</td>\n",
       "      <td>[{'name': 'hisao ito', 'org': ''}, {'name': 'm...</td>\n",
       "      <td>13047</td>\n",
       "      <td>interaction between epidermal growth factor an...</td>\n",
       "      <td>interaction between epidermal growth factor an...</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713096</td>\n",
       "      <td>0.010418</td>\n",
       "      <td>0.031974</td>\n",
       "      <td>-0.343218</td>\n",
       "      <td>0.052702</td>\n",
       "      <td>-0.485268</td>\n",
       "      <td>-0.381349</td>\n",
       "      <td>-0.482236</td>\n",
       "      <td>0.081133</td>\n",
       "      <td>0.303045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iki037dt</td>\n",
       "      <td>224</td>\n",
       "      <td>[{'name': 'yasutoshi kuboki', 'org': 'Departme...</td>\n",
       "      <td>2607</td>\n",
       "      <td>gastric cancer, amplification, mutation, next-...</td>\n",
       "      <td>comprehensive analyses using next-generation s...</td>\n",
       "      <td>133</td>\n",
       "      <td>1629</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232979</td>\n",
       "      <td>-0.391160</td>\n",
       "      <td>0.260753</td>\n",
       "      <td>-0.290798</td>\n",
       "      <td>0.068610</td>\n",
       "      <td>0.043475</td>\n",
       "      <td>-0.130058</td>\n",
       "      <td>-0.040746</td>\n",
       "      <td>-0.159678</td>\n",
       "      <td>0.224960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 829 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  author_id  name                                             author  venue  \\\n",
       "0  Iki037dt   224  [{'name': 'Hideaki Bando', 'org': ''}, {'name'...   2607   \n",
       "1  Iki037dt   224  [{'name': 'jintetsu soh', 'org': ''}, {'name':...  18917   \n",
       "2  Iki037dt   224  [{'name': 'M Inaba', 'org': 'Department of Uro...  19400   \n",
       "3  Iki037dt   224  [{'name': 'hisao ito', 'org': ''}, {'name': 'm...  13047   \n",
       "4  Iki037dt   224  [{'name': 'yasutoshi kuboki', 'org': 'Departme...   2607   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  the power of the high-sensitive kras test to p...   \n",
       "1  1812 nicardipine versus saline injection as tr...   \n",
       "2  vesicoureteral reflux, bladder hypertrophy, ch...   \n",
       "3  interaction between epidermal growth factor an...   \n",
       "4  gastric cancer, amplification, mutation, next-...   \n",
       "\n",
       "                                                text   x1    x2  x3  x4  ...  \\\n",
       "0  the power of the high-sensitive kras test to p...  120     0   0  14  ...   \n",
       "1  1812 nicardipine versus saline injection as tr...  123     0   0   8  ...   \n",
       "2  possible use of ultrasound estimated bladder w...  100   986   4   9  ...   \n",
       "3  interaction between epidermal growth factor an...  103     0   0  10  ...   \n",
       "4  comprehensive analyses using next-generation s...  133  1629   5  10  ...   \n",
       "\n",
       "    w2v_246   w2v_247   w2v_248   w2v_249   w2v_250   w2v_251   w2v_252  \\\n",
       "0  0.254230 -0.591256 -0.047873 -0.518315 -0.170804 -0.094971  0.009196   \n",
       "1  0.683561 -0.365802  0.765757 -0.325274 -0.062557  0.341192  0.125490   \n",
       "2  0.014613 -0.199763 -0.033882 -0.048321 -0.051605  0.066802  0.160459   \n",
       "3  0.713096  0.010418  0.031974 -0.343218  0.052702 -0.485268 -0.381349   \n",
       "4  0.232979 -0.391160  0.260753 -0.290798  0.068610  0.043475 -0.130058   \n",
       "\n",
       "    w2v_253   w2v_254   w2v_255  \n",
       "0 -0.261595  0.165669  0.259170  \n",
       "1 -0.561785  0.108586  0.246013  \n",
       "2 -0.193787  0.172351  0.265669  \n",
       "3 -0.482236  0.081133  0.303045  \n",
       "4 -0.040746 -0.159678  0.224960  \n",
       "\n",
       "[5 rows x 829 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将训练数据和测试数据拼接\n",
    "df = pd.concat([train,valid],axis=0).reset_index(drop=True)\n",
    "del train,valid\n",
    "gc.collect()#手动触发垃圾回收,强制回收由垃圾回收器标记为未使用的内存\n",
    "df['id'] = df.index\n",
    "#用tfidf将text转成词向量的形式,具体参数没研究过.\n",
    "vectorizer = TfidfVectorizer(\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            token_pattern=None,\n",
    "            strip_accents='unicode',\n",
    "            analyzer = 'word',\n",
    "            ngram_range=(1,4),\n",
    "            min_df=0.05,\n",
    "            max_df=0.95,\n",
    "            sublinear_tf=True,max_features=100\n",
    ")\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "X = X.toarray()\n",
    "tfidf_df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "\n",
    "#用tfidf模型将keywords转成词向量的形式,具体参数没研究过.\n",
    "vectorizer2 = TfidfVectorizer(\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            token_pattern=None,\n",
    "            strip_accents='unicode',\n",
    "            analyzer = 'word',\n",
    "            ngram_range=(1,3),\n",
    "            min_df=0.05,\n",
    "            max_df=0.95,\n",
    "            sublinear_tf=True,max_features=100,\n",
    ")\n",
    "X2 = vectorizer2.fit_transform(df['keywords'])\n",
    "X2 = X2.toarray()\n",
    "tfidf_df2 = pd.DataFrame(X2, columns=[f'keywords_{i}' for i in range(X2.shape[1])])\n",
    "\n",
    "#用tfidf模型将地点转换成词向量表示,具体参数没有研究过.\n",
    "vectorizer3 = TfidfVectorizer(\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            token_pattern=None,\n",
    "            strip_accents='unicode',\n",
    "            analyzer = 'word',\n",
    "            ngram_range=(1,2),\n",
    "            min_df=0.05,\n",
    "            max_df=0.95,\n",
    "            sublinear_tf=True\n",
    ")\n",
    "X3 = vectorizer3.fit_transform(df['venue'])\n",
    "X3 = X3.toarray()\n",
    "tfidf_df3 = pd.DataFrame(X3, columns=[f'venue_{i}' for i in range(X3.shape[1])])\n",
    "\n",
    "#用普通的词频统计模型将text转成词向量,具体参数没有研究过.\n",
    "vectorizer_cnt = CountVectorizer(\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            token_pattern=None,\n",
    "            strip_accents='unicode',\n",
    "            analyzer = 'word',\n",
    "            ngram_range=(1,3),\n",
    "            min_df=0.10,\n",
    "            max_df=0.85,max_features=100\n",
    ")\n",
    "train_tfid = vectorizer_cnt.fit_transform([i for i in df['text']])\n",
    "train_tfid  = train_tfid.toarray()\n",
    "cnt_df = pd.DataFrame(train_tfid, columns=[f'tfid_cnt_{i}' for i in range(train_tfid.shape[1])])\n",
    "\n",
    "#用普通的词频统计模型将keywords转成词向量,具体参数没有研究过.\n",
    "vectorizer_cnt2 = CountVectorizer(\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            token_pattern=None,\n",
    "            strip_accents='unicode',\n",
    "            analyzer = 'word',\n",
    "            ngram_range=(2,3),\n",
    "            min_df=0.10,\n",
    "            max_df=0.85,max_features=100\n",
    ")\n",
    "train_tfid2 = vectorizer_cnt2.fit_transform([i for i in df['keywords']])\n",
    "train_tfid2 = train_tfid2.toarray()\n",
    "cnt_df2 = pd.DataFrame(train_tfid2, columns=[f'tfid_cnt2_{i}' for i in range(train_tfid2.shape[1])])\n",
    "\n",
    "#用普通的词频统计模型将venue转成词向量,具体参数没有研究过.\n",
    "vectorizer_cnt3 = CountVectorizer(\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            token_pattern=None,\n",
    "            strip_accents='unicode',\n",
    "            analyzer = 'word',\n",
    "            ngram_range=(1,2),\n",
    "            min_df=0.10,\n",
    "            max_df=0.85,\n",
    ")\n",
    "train_tfid3 = vectorizer_cnt3.fit_transform([i for i in df['venue']])\n",
    "train_tfid3 = train_tfid3.toarray()\n",
    "cnt_df3 = pd.DataFrame(train_tfid3, columns=[f'tfid_cnt3_{i}' for i in range(train_tfid3.shape[1])])\n",
    "\n",
    "#用普通的词频统计模型将topauthor转成词向量,具体参数没有研究过.\n",
    "vectorizer_cnt4 = CountVectorizer(\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            token_pattern=None,\n",
    "            strip_accents='unicode',\n",
    "            analyzer = 'word',\n",
    "            ngram_range=(1,2),\n",
    "            min_df=0.10,\n",
    "            max_df=0.85,\n",
    ")\n",
    "train_tfid4 = vectorizer_cnt4.fit_transform([i for i in df['top_author']])\n",
    "train_tfid4 = train_tfid4.toarray()\n",
    "cnt_df4 = pd.DataFrame(train_tfid4, columns=[f'tfid_cnt4_{i}' for i in range(train_tfid4.shape[1])])\n",
    "\n",
    "\n",
    "#用word2vec将text转换成词向量\n",
    "processed_texts = [simple_preprocess(text) for text in df['text']]#小写、标点、分词\n",
    "size = 256#词向量的维度为256维\n",
    "#训练word2vec模型,传入文本数据processed_texts,词向量维度为256维,上下文窗口为5,即考虑前后2个词,考虑最少出现2次的词,并行训练模型\n",
    "model = Word2Vec(processed_texts, vector_size=size, window=5, min_count=2, workers=16)\n",
    "#均值词向量\n",
    "X = []\n",
    "for text in processed_texts:\n",
    "    vector = np.zeros(size)#初始化向量\n",
    "    count = 0\n",
    "    for word in text:\n",
    "        if word in model.wv:#如果一个词在词汇表中\n",
    "            vector += model.wv[word]#加上对应的向量\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        vector /= count\n",
    "    X.append(vector)\n",
    "w2v = pd.DataFrame(X, columns=[f'w2v_{i}' for i in range(size)])\n",
    "\n",
    "del vectorizer,vectorizer2,vectorizer3,vectorizer_cnt,vectorizer_cnt2,vectorizer_cnt3,vectorizer_cnt4,model\n",
    "gc.collect()#手动触发垃圾回收,强制回收由垃圾回收器标记为未使用的内存\n",
    "\n",
    "df = pd.concat([df.reset_index(drop=True),tfidf_df,tfidf_df2,tfidf_df3,cnt_df,cnt_df2,cnt_df3,cnt_df4,w2v],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9539b1bf",
   "metadata": {
    "papermill": {
     "duration": 0.011256,
     "end_time": "2024-06-18T07:20:13.942937",
     "exception": false,
     "start_time": "2024-06-18T07:20:13.931681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 6.top100重合度特征\n",
    "\n",
    "#### 这里先将文本去标点、小写,去掉停用词,统计每个author的top100词,然后统计text中的每个词是top100的占比。比如有个文本分词后是['a','b','c'], 'a'和'b'是top100词库里的,那么占比就是66%.\n",
    "\n",
    "#### 为什么要去标点、小写、去停用词呢?停用词、标点这种每篇文章都有,无法表现出每个作者各自的特点,如果统计的top100中都是标点、停用词,那可能构造的重合度特征就没什么用了,可能大家都差不多,你一句话里有一个句号,我一句话里也有个句号。\n",
    "\n",
    "#### 注:我之前自己在比赛中是取出现最频繁的100个词,对title和abstract做词频统计,其中有很多是标点和停用词,所以效果也没有那么好。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cf9a226",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T07:20:13.965999Z",
     "iopub.status.busy": "2024-06-18T07:20:13.965594Z",
     "iopub.status.idle": "2024-06-18T07:24:16.242520Z",
     "shell.execute_reply": "2024-06-18T07:24:16.241305Z"
    },
    "papermill": {
     "duration": 242.292719,
     "end_time": "2024-06-18T07:24:16.245510",
     "exception": false,
     "start_time": "2024-06-18T07:20:13.952791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 779/779 [00:06<00:00, 119.20it/s]\n",
      "100%|██████████| 515/515 [00:04<00:00, 111.85it/s]\n",
      "100%|██████████| 779/779 [00:01<00:00, 566.18it/s]\n",
      "100%|██████████| 515/515 [00:00<00:00, 532.58it/s]\n"
     ]
    }
   ],
   "source": [
    "#label不是nan的就是训练集,是nan的就是测试集\n",
    "train,valid = df[~df['label'].isna()].reset_index(drop=True),df[df['label'].isna()].reset_index(drop=True)\n",
    "del df\n",
    "gc.collect()#手动触发垃圾回收,强制回收由垃圾回收器标记为未使用的内存\n",
    "#删除标点和stopwords是为了让后面的top100词带有作者的特点。\n",
    "def preprocess_text(text):\n",
    "    #空字符替换成空字符,并删除所有的标点符号\n",
    "    text = text.translate(str.maketrans(\"\", \"\", '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'))\n",
    "    #转换为小写\n",
    "    text = text.lower()\n",
    "    #据我分析,他这里可能是把停用词删除掉.\n",
    "    stopwords=['the','of','a','an','how','what','which','why','where','who','we','are','is','by','base','on','in']\n",
    "    for s in stopwords:\n",
    "        text = text.replace(s,'')\n",
    "    return text\n",
    "\n",
    "#这里就是统计text里的word是top100word在text里的占比\n",
    "def calculate_overlap_score(text, top100_words):\n",
    "    words=text.split()#文本按空格拆分成单词\n",
    "    #统计words里每个word在top100_words的个数\n",
    "    overlap_count = sum(int(word in top100_words) for word in words)\n",
    "    overlap_score = overlap_count / len(words) if len(words) > 0 else 0\n",
    "    return overlap_score\n",
    "\n",
    "# 按作者分组\n",
    "def get_overlap_score(df,col):\n",
    "    #主要就是文本去除标点符号和停用词\n",
    "    df[col] = df[col].astype(str).apply(preprocess_text)\n",
    "    grouped = df.groupby(\"name\")#按照作者的姓名进行分组\n",
    "    \n",
    "    #统计每个作者的所有文本中top100words(由于label为0的样本也有标签1,故也可以认为是label=1的样本中top100)\n",
    "    top100_words = {}\n",
    "    for author, group in tqdm(grouped):\n",
    "        #应该是把这个作者的所有文本拼接在一起\n",
    "        author_label_1_text = \" \".join(group[col])\n",
    "        #分词操作\n",
    "        words = author_label_1_text.split()\n",
    "        #返回一个字典,表示每个词出现的词数{'a':2,'b':1}\n",
    "        word_counts = Counter(words)\n",
    "        #获取top100的词\n",
    "        top100 = [word for word, _ in word_counts.most_common(100)]\n",
    "        #存入字典,author的top100个词就是top100.\n",
    "        top100_words[author] = top100\n",
    "\n",
    "    cnt = 0\n",
    "    for x,y in zip(df['name'],df[col]):#x是作者姓名,y是作者文本\n",
    "        cnt  += 1\n",
    "        #文本中有多少词是来自这个作者top100的词\n",
    "        overlap_score = calculate_overlap_score(y, top100_words[x])\n",
    "        #这篇论文的分数是多少\n",
    "        df.loc[cnt-1,f'{col}_overlap_score'] = overlap_score\n",
    "    return df\n",
    "#增加文本top100重合度的特征.\n",
    "for col in ['text','keywords']:\n",
    "    train = get_overlap_score(train,col)\n",
    "    valid  = get_overlap_score(valid,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f3250",
   "metadata": {
    "papermill": {
     "duration": 0.020196,
     "end_time": "2024-06-18T07:24:16.286503",
     "exception": false,
     "start_time": "2024-06-18T07:24:16.266307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 7.关键特征.\n",
    "\n",
    "#### 这里特征太多,不一定说的清楚,还是直接看代码吧。\n",
    "\n",
    "#### 首先是text按照句号、逗号、空格分割统计词数,然后对name,year(也就是x5),‘venue’进行groupby,例如这个人发表的年份有几个(这里是nunique),发表的地点有几个。在这个年份发表的作者有几个,在这个年份有几个地点(或者应该说学术会议)。然后是论文长度和词数的特征对于每个作者是不一样的,每个年份也是不一样的,所以又进行了groupby的统计特征。然后拿一个人的一篇论文和这个人的其他论文进行比较,和同一年发表的其他论文进行比较,这里的比较有作差和tfidf两种特征。上一步统计出来的词向量特征也是和这个人groupby特征进行了一个比较,这里是作差比较。\n",
    "\n",
    "\n",
    "#### 本次比赛就是要找异常,而异常就是和大多数不一样的数据,所以和groupby的特征作差能够把异常暴露出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abd19545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T07:24:16.329654Z",
     "iopub.status.busy": "2024-06-18T07:24:16.328744Z",
     "iopub.status.idle": "2024-06-18T07:25:06.625597Z",
     "shell.execute_reply": "2024-06-18T07:25:06.624448Z"
    },
    "papermill": {
     "duration": 50.321315,
     "end_time": "2024-06-18T07:25:06.628009",
     "exception": false,
     "start_time": "2024-06-18T07:24:16.306694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 23.86it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 23.17it/s]\n",
      "100%|██████████| 221/221 [00:05<00:00, 38.75it/s]\n",
      "100%|██████████| 332/332 [00:09<00:00, 35.82it/s]\n",
      "100%|██████████| 256/256 [00:06<00:00, 38.09it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 31.53it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 31.04it/s]\n",
      "100%|██████████| 221/221 [00:03<00:00, 59.46it/s]\n",
      "100%|██████████| 332/332 [00:06<00:00, 52.49it/s]\n",
      "100%|██████████| 256/256 [00:05<00:00, 48.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>name</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>top_author</th>\n",
       "      <th>label</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_252_min_w2v_dif</th>\n",
       "      <th>w2v_253_mean_w2v_dif</th>\n",
       "      <th>w2v_253_max_w2v_dif</th>\n",
       "      <th>w2v_253_min_w2v_dif</th>\n",
       "      <th>w2v_254_mean_w2v_dif</th>\n",
       "      <th>w2v_254_max_w2v_dif</th>\n",
       "      <th>w2v_254_min_w2v_dif</th>\n",
       "      <th>w2v_255_mean_w2v_dif</th>\n",
       "      <th>w2v_255_max_w2v_dif</th>\n",
       "      <th>w2v_255_min_w2v_dif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iki037dt</td>\n",
       "      <td>224</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>hideaki bando,takayuki yoshino,kazuya tuchihar...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.243632</td>\n",
       "      <td>0.212314</td>\n",
       "      <td>1.149465</td>\n",
       "      <td>-0.777461</td>\n",
       "      <td>-0.190124</td>\n",
       "      <td>1.220609</td>\n",
       "      <td>-1.051494</td>\n",
       "      <td>-0.209003</td>\n",
       "      <td>0.730881</td>\n",
       "      <td>-1.148365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iki037dt</td>\n",
       "      <td>224</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>jintetsu soh,yoshio naya,atsushi ochiai,yasuyu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.359926</td>\n",
       "      <td>0.512504</td>\n",
       "      <td>1.449654</td>\n",
       "      <td>-0.477271</td>\n",
       "      <td>-0.133041</td>\n",
       "      <td>1.277692</td>\n",
       "      <td>-0.994411</td>\n",
       "      <td>-0.195847</td>\n",
       "      <td>0.744038</td>\n",
       "      <td>-1.135209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iki037dt</td>\n",
       "      <td>224</td>\n",
       "      <td>100</td>\n",
       "      <td>986</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>32</td>\n",
       "      <td>m inaba,o ukimura,a kawauchi,t iwata,m kanazawa</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.394895</td>\n",
       "      <td>0.144506</td>\n",
       "      <td>1.081657</td>\n",
       "      <td>-0.845269</td>\n",
       "      <td>-0.196806</td>\n",
       "      <td>1.213927</td>\n",
       "      <td>-1.058176</td>\n",
       "      <td>-0.215502</td>\n",
       "      <td>0.724383</td>\n",
       "      <td>-1.154864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iki037dt</td>\n",
       "      <td>224</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>37</td>\n",
       "      <td>31</td>\n",
       "      <td>hisao ito,masanobu yamamoto,naruhito oda,fumio...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.853086</td>\n",
       "      <td>0.432955</td>\n",
       "      <td>1.370105</td>\n",
       "      <td>-0.556821</td>\n",
       "      <td>-0.105588</td>\n",
       "      <td>1.305145</td>\n",
       "      <td>-0.966958</td>\n",
       "      <td>-0.252879</td>\n",
       "      <td>0.687006</td>\n",
       "      <td>-1.192241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iki037dt</td>\n",
       "      <td>224</td>\n",
       "      <td>133</td>\n",
       "      <td>1629</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>yasutoshi kuboki,seigo yamashita,tohru niwa,to...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.104377</td>\n",
       "      <td>-0.008535</td>\n",
       "      <td>0.928615</td>\n",
       "      <td>-0.998311</td>\n",
       "      <td>0.135223</td>\n",
       "      <td>1.545956</td>\n",
       "      <td>-0.726147</td>\n",
       "      <td>-0.174793</td>\n",
       "      <td>0.765092</td>\n",
       "      <td>-1.114155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3409 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  author_id  name   x1    x2  x3  x4  x5  x6  \\\n",
       "0  Iki037dt   224  120     0   0  14  14  18   \n",
       "1  Iki037dt   224  123     0   0   8  13  22   \n",
       "2  Iki037dt   224  100   986   4   9  23  32   \n",
       "3  Iki037dt   224  103     0   0  10  37  31   \n",
       "4  Iki037dt   224  133  1629   5  10   9  18   \n",
       "\n",
       "                                          top_author  label  ...  \\\n",
       "0  hideaki bando,takayuki yoshino,kazuya tuchihar...    1.0  ...   \n",
       "1  jintetsu soh,yoshio naya,atsushi ochiai,yasuyu...    1.0  ...   \n",
       "2    m inaba,o ukimura,a kawauchi,t iwata,m kanazawa    1.0  ...   \n",
       "3  hisao ito,masanobu yamamoto,naruhito oda,fumio...    1.0  ...   \n",
       "4  yasutoshi kuboki,seigo yamashita,tohru niwa,to...    1.0  ...   \n",
       "\n",
       "   w2v_252_min_w2v_dif  w2v_253_mean_w2v_dif  w2v_253_max_w2v_dif  \\\n",
       "0            -1.243632              0.212314             1.149465   \n",
       "1            -1.359926              0.512504             1.449654   \n",
       "2            -1.394895              0.144506             1.081657   \n",
       "3            -0.853086              0.432955             1.370105   \n",
       "4            -1.104377             -0.008535             0.928615   \n",
       "\n",
       "   w2v_253_min_w2v_dif  w2v_254_mean_w2v_dif  w2v_254_max_w2v_dif  \\\n",
       "0            -0.777461             -0.190124             1.220609   \n",
       "1            -0.477271             -0.133041             1.277692   \n",
       "2            -0.845269             -0.196806             1.213927   \n",
       "3            -0.556821             -0.105588             1.305145   \n",
       "4            -0.998311              0.135223             1.545956   \n",
       "\n",
       "   w2v_254_min_w2v_dif  w2v_255_mean_w2v_dif  w2v_255_max_w2v_dif  \\\n",
       "0            -1.051494             -0.209003             0.730881   \n",
       "1            -0.994411             -0.195847             0.744038   \n",
       "2            -1.058176             -0.215502             0.724383   \n",
       "3            -0.966958             -0.252879             0.687006   \n",
       "4            -0.726147             -0.174793             0.765092   \n",
       "\n",
       "   w2v_255_min_w2v_dif  \n",
       "0            -1.148365  \n",
       "1            -1.135209  \n",
       "2            -1.154864  \n",
       "3            -1.192241  \n",
       "4            -1.114155  \n",
       "\n",
       "[5 rows x 3409 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stage1_fe(df):\n",
    "\n",
    "    #统计按照逗号、句号、空格分割字符串的个数\n",
    "    df[\"text_content_cnt\"] = df['text'].apply(lambda x: len(x.split('.')))\n",
    "    df[\"text_sentence_cnt\"] = df['text'].apply(lambda x: len(x.split(',')))\n",
    "    df[\"text_word_cnt\"] = df['text'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "    #x5=2024-year,venue是label_encode的原始地点,id就是数据的index吧,name是作者本人名字\n",
    "    for col in ['x5','venue','id']:\n",
    "        #作者的发表年份有几个唯一值,作者发表了几篇论文,作者发表的地点有几个唯一值,作者发表了几篇论文\n",
    "        #作者idx有几个唯一值,作者idx的count\n",
    "        df[f'name_{col}_nunique'] = df.groupby(['name'])[col].transform('nunique')\n",
    "        df[f'name_{col}_count'] = df.groupby(['name'])[col].transform('count')\n",
    "    for col in ['name','venue','id']:\n",
    "        #每个年份有几个作者发表,发表了多少篇论文,每个年份有几个地点的论文,发表多少篇论文\n",
    "        #每个年份有多少个idx,idx的count\n",
    "        df[f'year_{col}_nunique'] = df.groupby(['x5'])[col].transform('nunique')\n",
    "        df[f'year_{col}_count'] = df.groupby(['x5'])[col].transform('count')\n",
    "    #作者写了多少篇论文\n",
    "    name_dict = df['name'].value_counts().to_dict()\n",
    "    df['name_count'] = df['name'].map(name_dict)\n",
    "    #每个地点有多少篇论文\n",
    "    venue_dict = df['venue'].value_counts().to_dict()\n",
    "    df['venue_count'] = df['venue'].map(venue_dict)\n",
    "    \n",
    "    #论文的各种长度和词数对于每个人的特征统计\n",
    "    for col in tqdm(['x1', 'x2', 'x3', 'x4', 'x5','x6',\n",
    "                     \"text_content_cnt\",\"text_sentence_cnt\",\"text_word_cnt\"]):\n",
    "        for meth in ['mean','max','min','std','median','skew']:\n",
    "            df[f'{col}_{meth}_name'] = df.groupby(['name'])[col].transform(meth)\n",
    "    #论文的各种长度和词数在每个年份的特征    \n",
    "    for col in tqdm(['x1', 'x2', 'x3', 'x4', 'x6',\n",
    "                     \"text_content_cnt\",\"text_sentence_cnt\",\"text_word_cnt\"]):\n",
    "        for meth in ['mean','max','min','std','median','skew']:\n",
    "            df[f'year_{col}_{meth}'] = df.groupby(['x5'])[col].transform(meth)\n",
    "\n",
    "    #词数、句子数\n",
    "    for col in [\"text_content_cnt\",\"text_sentence_cnt\",\"text_word_cnt\"]:\n",
    "        for meth in ['mean','max','min']:\n",
    "            #和这个人特征的不同\n",
    "            df[f'{col}_{meth}_name_dif'] = df[f'{col}_{meth}_name'] - df[col]\n",
    "            df[f'{col}_{meth}_name_ratio'] = df[col]/(1+df[f'{col}_{meth}_name'])\n",
    "            #和这个年份特征的不同\n",
    "            df[f'year_{col}_{meth}_dif'] = df[f'year_{col}_{meth}'] - df[col]\n",
    "            df[f'year_{col}_{meth}_ratio'] = df[col]/(1+df[f'year_{col}_{meth}']) \n",
    "    \n",
    "    #tf-idf,word2vec,count等词频统计的模型和这个人的不同之处\n",
    "    for col in tqdm(list(tfidf_df.columns)+list(tfidf_df2.columns)+list(tfidf_df3.columns)):\n",
    "         for meth in ['mean','max','min']:\n",
    "            df[f'{col}_{meth}_tfidf_dif'] = df.groupby(['name'])[col].transform(meth)-df[col]\n",
    "    for col in tqdm(list(cnt_df.columns)+list(cnt_df2.columns)+list(cnt_df3.columns)+list(cnt_df4.columns)):\n",
    "         for meth in ['mean','max','min']:\n",
    "            df[f'{col}_{meth}_cnt_dif'] = df.groupby(['name'])[col].transform(meth)-df[col]\n",
    "    for col in tqdm(list(w2v.columns)):\n",
    "         for meth in ['mean','max','min']:\n",
    "            df[f'{col}_{meth}_w2v_dif'] = df.groupby(['name'])[col].transform(meth)-df[col]\n",
    "   \n",
    "    return df\n",
    "train = stage1_fe(train)\n",
    "valid = stage1_fe(valid)\n",
    "del tfidf_df,tfidf_df2,tfidf_df3,cnt_df,cnt_df2,cnt_df3,w2v\n",
    "gc.collect()#手动触发垃圾回收,强制回收由垃圾回收器标记为未使用的内存\n",
    "#author好像从来都没有用过:原始数据中['name','org'],文本特征也基本提取完成了\n",
    "train = train.drop(['author','text','keywords','venue'],axis=1)\n",
    "valid = valid.drop(['author','text','keywords','venue'],axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e8a4c0",
   "metadata": {
    "papermill": {
     "duration": 0.054602,
     "end_time": "2024-06-18T07:25:06.899873",
     "exception": false,
     "start_time": "2024-06-18T07:25:06.845271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8.余弦相似度\n",
    "\n",
    "#### 就是把词向量特征进行余弦相似度的构造,他这里很聪明,是和上一步均值作差的特征进行余弦相似度计算。如果和论文特征groupby的均值越靠近,那么就是和论文整体是靠近的,那么就应该是normal的数据,而反映到数据上和groupby均值靠近,和均值相减就越接近0,余弦相似度也就是接近0的。另一方面他这样做时间复杂度是O(n)。\n",
    "\n",
    "#### 我之前尝试余弦相似度的时候是两两数据的词向量计算余弦相似度,这样时间复杂度高,是O(n^2),而且论文的特征和整体相似才是真的相似,和另外一篇论文是否相似其实无关紧要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b6dbf05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T07:25:07.011346Z",
     "iopub.status.busy": "2024-06-18T07:25:07.010462Z",
     "iopub.status.idle": "2024-06-18T07:25:24.967803Z",
     "shell.execute_reply": "2024-06-18T07:25:24.966335Z"
    },
    "papermill": {
     "duration": 18.016548,
     "end_time": "2024-06-18T07:25:24.970644",
     "exception": false,
     "start_time": "2024-06-18T07:25:06.954096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165594/165594 [00:03<00:00, 42651.77it/s]\n",
      "100%|██████████| 116262/116262 [00:02<00:00, 41163.63it/s]\n",
      "100%|██████████| 165594/165594 [00:04<00:00, 37622.05it/s]\n",
      "100%|██████████| 116262/116262 [00:03<00:00, 35276.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#这里就是两组向量余弦相似度的比较(和均值去比较余弦相似度)\n",
    "def stage2_fe(df,col1,col2,name):\n",
    "    arr_1 = np.array(df[col1])\n",
    "    arr_2 = np.array(df[col2])\n",
    "    cosine_sims = []\n",
    "    for i in tqdm(range(len(arr_1))):\n",
    "        vec_1,vec_2 = arr_1[i],arr_2[i]\n",
    "        cosine_sim = np.dot(vec_1, vec_2) / (np.linalg.norm(vec_1) * np.linalg.norm(vec_2))\n",
    "        cosine_sims.append(cosine_sim)\n",
    "    return cosine_sims\n",
    "\n",
    "w2v_mean_columns = [col for col in train.columns if \"mean_w2v_dif\" in col]\n",
    "w2v_columns = [f'w2v_{i}' for i in range(size)]\n",
    "\n",
    "tfidf2_mean_columns = [col for col in train.columns if \"mean_cnt_dif\" in col]\n",
    "cnt_columns = [col for col in train.columns if \"tfid_cnt\" in col]\n",
    "all_cnt_columns = cnt_columns[:len(tfidf2_mean_columns)]\n",
    "\n",
    "w2v_cos_train = stage2_fe(train,w2v_columns,w2v_mean_columns,'w2v')\n",
    "w2v_cos_valid = stage2_fe(valid,w2v_columns,w2v_mean_columns,'w2v')  \n",
    "\n",
    "cnt_cos_train = stage2_fe(train,all_cnt_columns,tfidf2_mean_columns,'cnt')\n",
    "cnt_cos_valid = stage2_fe(valid,all_cnt_columns,tfidf2_mean_columns,'cnt')   \n",
    "\n",
    "train['w2v_cosine_sims'] = w2v_cos_train\n",
    "valid['w2v_cosine_sims'] = w2v_cos_valid\n",
    "train['cnt_cosine_sims'] = cnt_cos_train\n",
    "valid['cnt_cosine_sims'] = cnt_cos_valid\n",
    "del w2v_cos_train,w2v_cos_valid,cnt_cos_train,cnt_cos_valid\n",
    "gc.collect()#手动触发垃圾回收,强制回收由垃圾回收器标记为未使用的内存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b8b8c5",
   "metadata": {
    "papermill": {
     "duration": 0.069803,
     "end_time": "2024-06-18T07:25:25.109368",
     "exception": false,
     "start_time": "2024-06-18T07:25:25.039565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 9.reduce_mem_usage\n",
    "\n",
    "#### 这是我自己加的,因为要在Kaggle上运行,不加的话会超内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bde08cd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T07:25:25.249990Z",
     "iopub.status.busy": "2024-06-18T07:25:25.249169Z",
     "iopub.status.idle": "2024-06-18T07:25:38.792887Z",
     "shell.execute_reply": "2024-06-18T07:25:38.791730Z"
    },
    "papermill": {
     "duration": 13.617497,
     "end_time": "2024-06-18T07:25:38.795556",
     "exception": false,
     "start_time": "2024-06-18T07:25:25.178059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 4309.40 MB\n",
      "Memory usage after optimization is: 966.96 MB\n",
      "Decreased by 77.6%\n",
      "Memory usage of dataframe is 3025.59 MB\n",
      "Memory usage after optimization is: 678.23 MB\n",
      "Decreased by 77.6%\n"
     ]
    }
   ],
   "source": [
    "#遍历表格df的所有列修改数据类型减少内存使用\n",
    "def reduce_mem_usage(df, float16_as32=True):\n",
    "    #memory_usage()是df每列的内存使用量,sum是对它们求和, B->KB->MB\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:#遍历每列的列名\n",
    "        col_type = df[col].dtype#列名的type\n",
    "        if col_type != object and str(col_type)!='category':#不是object也就是说这里处理的是数值类型的变量\n",
    "            c_min,c_max = df[col].min(),df[col].max() #求出这列的最大值和最小值\n",
    "            if str(col_type)[:3] == 'int':#如果是int类型的变量,不管是int8,int16,int32还是int64\n",
    "                #如果这列的取值范围是在int8的取值范围内,那就对类型进行转换 (-128 到 127)\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                #如果这列的取值范围是在int16的取值范围内,那就对类型进行转换(-32,768 到 32,767)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                #如果这列的取值范围是在int32的取值范围内,那就对类型进行转换(-2,147,483,648到2,147,483,647)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                #如果这列的取值范围是在int64的取值范围内,那就对类型进行转换(-9,223,372,036,854,775,808到9,223,372,036,854,775,807)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:#如果是浮点数类型.\n",
    "                #如果数值在float16的取值范围内,如果觉得需要更高精度可以考虑float32\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    if float16_as32:#如果数据需要更高的精度可以选择float32\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float16)  \n",
    "                #如果数值在float32的取值范围内，对它进行类型转换\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                #如果数值在float64的取值范围内，对它进行类型转换\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    #计算一下结束后的内存\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    #相比一开始的内存减少了百分之多少\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "train=reduce_mem_usage(train, float16_as32=False)\n",
    "valid=reduce_mem_usage(valid, float16_as32=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1808ae67",
   "metadata": {
    "papermill": {
     "duration": 0.071991,
     "end_time": "2024-06-18T07:25:38.936342",
     "exception": false,
     "start_time": "2024-06-18T07:25:38.864351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 10.Model training\n",
    "\n",
    "\n",
    "#### 模型训练代码我也做过修改,我想看看用官方评估指标达到的成绩,不过最后由于做过数据增强,线下CV也不准。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7246d307",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T07:25:39.079461Z",
     "iopub.status.busy": "2024-06-18T07:25:39.078994Z",
     "iopub.status.idle": "2024-06-18T09:39:59.268199Z",
     "shell.execute_reply": "2024-06-18T09:39:59.266721Z"
    },
    "papermill": {
     "duration": 8060.347985,
     "end_time": "2024-06-18T09:39:59.355026",
     "exception": false,
     "start_time": "2024-06-18T07:25:39.007041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name lgb,fold:0\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.777487\n",
      "[200]\tvalid_0's auc: 0.791134\n",
      "[300]\tvalid_0's auc: 0.799193\n",
      "[400]\tvalid_0's auc: 0.804648\n",
      "[500]\tvalid_0's auc: 0.809445\n",
      "[600]\tvalid_0's auc: 0.812966\n",
      "[700]\tvalid_0's auc: 0.815444\n",
      "[800]\tvalid_0's auc: 0.816672\n",
      "[900]\tvalid_0's auc: 0.817493\n",
      "[1000]\tvalid_0's auc: 0.818861\n",
      "[1100]\tvalid_0's auc: 0.820228\n",
      "[1200]\tvalid_0's auc: 0.821152\n",
      "[1300]\tvalid_0's auc: 0.821613\n",
      "[1400]\tvalid_0's auc: 0.822557\n",
      "[1500]\tvalid_0's auc: 0.823092\n",
      "[1600]\tvalid_0's auc: 0.823445\n",
      "[1700]\tvalid_0's auc: 0.823524\n",
      "[1800]\tvalid_0's auc: 0.824086\n",
      "[1900]\tvalid_0's auc: 0.824145\n",
      "[2000]\tvalid_0's auc: 0.824148\n",
      "Early stopping, best iteration is:\n",
      "[1931]\tvalid_0's auc: 0.824365\n",
      "name lgb,fold:1\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.727946\n",
      "[200]\tvalid_0's auc: 0.740052\n",
      "[300]\tvalid_0's auc: 0.749021\n",
      "[400]\tvalid_0's auc: 0.752548\n",
      "[500]\tvalid_0's auc: 0.754369\n",
      "[600]\tvalid_0's auc: 0.759406\n",
      "[700]\tvalid_0's auc: 0.761574\n",
      "[800]\tvalid_0's auc: 0.763616\n",
      "[900]\tvalid_0's auc: 0.765584\n",
      "[1000]\tvalid_0's auc: 0.766686\n",
      "[1100]\tvalid_0's auc: 0.768483\n",
      "[1200]\tvalid_0's auc: 0.769919\n",
      "[1300]\tvalid_0's auc: 0.771364\n",
      "[1400]\tvalid_0's auc: 0.770745\n",
      "Early stopping, best iteration is:\n",
      "[1307]\tvalid_0's auc: 0.771485\n",
      "name lgb,fold:2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.762996\n",
      "[200]\tvalid_0's auc: 0.773848\n",
      "[300]\tvalid_0's auc: 0.780888\n",
      "[400]\tvalid_0's auc: 0.785938\n",
      "[500]\tvalid_0's auc: 0.790759\n",
      "[600]\tvalid_0's auc: 0.790897\n",
      "[700]\tvalid_0's auc: 0.794079\n",
      "[800]\tvalid_0's auc: 0.795521\n",
      "[900]\tvalid_0's auc: 0.796209\n",
      "[1000]\tvalid_0's auc: 0.797526\n",
      "[1100]\tvalid_0's auc: 0.799506\n",
      "[1200]\tvalid_0's auc: 0.801367\n",
      "[1300]\tvalid_0's auc: 0.801854\n",
      "[1400]\tvalid_0's auc: 0.802326\n",
      "[1500]\tvalid_0's auc: 0.802966\n",
      "[1600]\tvalid_0's auc: 0.80348\n",
      "[1700]\tvalid_0's auc: 0.803395\n",
      "Early stopping, best iteration is:\n",
      "[1628]\tvalid_0's auc: 0.803718\n",
      "name lgb,fold:3\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.710762\n",
      "[200]\tvalid_0's auc: 0.727303\n",
      "[300]\tvalid_0's auc: 0.739014\n",
      "[400]\tvalid_0's auc: 0.748159\n",
      "[500]\tvalid_0's auc: 0.750297\n",
      "[600]\tvalid_0's auc: 0.753948\n",
      "[700]\tvalid_0's auc: 0.755795\n",
      "Early stopping, best iteration is:\n",
      "[649]\tvalid_0's auc: 0.756545\n",
      "name lgb,fold:4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's auc: 0.800834\n",
      "[200]\tvalid_0's auc: 0.813154\n",
      "[300]\tvalid_0's auc: 0.821658\n",
      "[400]\tvalid_0's auc: 0.826556\n",
      "[500]\tvalid_0's auc: 0.8296\n",
      "[600]\tvalid_0's auc: 0.83181\n",
      "[700]\tvalid_0's auc: 0.832666\n",
      "[800]\tvalid_0's auc: 0.834484\n",
      "[900]\tvalid_0's auc: 0.83536\n",
      "[1000]\tvalid_0's auc: 0.836046\n",
      "[1100]\tvalid_0's auc: 0.837024\n",
      "[1200]\tvalid_0's auc: 0.837337\n",
      "[1300]\tvalid_0's auc: 0.838776\n",
      "[1400]\tvalid_0's auc: 0.839011\n",
      "[1500]\tvalid_0's auc: 0.840287\n",
      "Early stopping, best iteration is:\n",
      "[1499]\tvalid_0's auc: 0.840302\n",
      "weight_roc_auc:0.6879037104264646\n"
     ]
    }
   ],
   "source": [
    "def weight_rocauc(authorid,y_true,y_pro):\n",
    "    unique_authorid=[]\n",
    "    for a in authorid:\n",
    "        if a not in unique_authorid:\n",
    "            unique_authorid.append(a) \n",
    "    total_weight_auc=0\n",
    "    for i in range(len(unique_authorid)):\n",
    "        cur_idx=np.where(authorid==unique_authorid[i])[0]\n",
    "        if len(cur_idx)>0:#authorid可能是\n",
    "            auc=roc_auc_score(y_true[cur_idx],y_pro[cur_idx])\n",
    "            weight_auc=np.sum(y_true[cur_idx]==0)*auc\n",
    "            total_weight_auc+=weight_auc\n",
    "    total_weight_auc/=np.sum(y_true==0)\n",
    "    return total_weight_auc#'weight_rocauc',total_weight_auc,True#名称,值,是否越高越好\n",
    "\n",
    "choose_cols=[col for col in valid.drop(['id','label', 'top_author','author_id'],axis=1).columns]\n",
    "del valid\n",
    "gc.collect()#手动触发垃圾回收,强制回收由垃圾回收器标记为未使用的内存\n",
    "\n",
    "def fit_and_predict(train_feats=train,model=None,num_folds=10,seed=2024,name='lgb'):\n",
    "    X=train_feats[choose_cols].copy()\n",
    "    y=train_feats['label'].copy()\n",
    "    authorid=train_feats['author_id'].values\n",
    "    oof_pred_pro=np.zeros((len(X)))\n",
    "    del train_feats\n",
    "    gc.collect()#手动触发垃圾回收,强制回收由垃圾回收器标记为未使用的内存\n",
    "     \n",
    "    #10折交叉验证\n",
    "    sgkf = StratifiedGroupKFold(n_splits=num_folds,shuffle=True)\n",
    "    for fold, (train_index, valid_index) in (enumerate(sgkf.split(X,y,authorid))):\n",
    "        print(f\"name {name},fold:{fold}\")\n",
    "\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "        \n",
    "        model.fit(X_train,y_train,eval_set=[(X_valid, y_valid)],\n",
    "                      callbacks=[log_evaluation(100),early_stopping(100)]\n",
    "                     )\n",
    "        \n",
    "        oof_pred_pro[valid_index]=model.predict_proba(X_valid)[:,1]\n",
    "        del X_train,X_valid,y_train,y_valid\n",
    "        gc.collect()#手动触发垃圾回收,强制回收由垃圾回收器标记为未使用的内存\n",
    "        \n",
    "    print(f\"weight_roc_auc:{weight_rocauc(authorid,y.values,oof_pred_pro)}\")\n",
    "    \n",
    "    return oof_pred_pro\n",
    "\n",
    "lgb_params={\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"max_depth\": 6,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\":10000,\n",
    "    \"colsample_bytree\": 0.2,\n",
    "    \"colsample_bynode\": 0.2,\n",
    "    \"verbose\": -1,\n",
    "    \"random_state\": 2024,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 10,\n",
    "    \"extra_trees\":True,\n",
    "    'num_leaves':127,\n",
    "    \"verbose\": -1,\n",
    "    \"max_bin\":225,\n",
    "    \"class_weight\":'balanced',\n",
    "    }\n",
    "\n",
    "lgb_oof_pred_pro=fit_and_predict(model=LGBMClassifier(**lgb_params),num_folds=5,seed=2024,name='lgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4893bccf",
   "metadata": {
    "papermill": {
     "duration": 0.081205,
     "end_time": "2024-06-18T09:39:59.516808",
     "exception": false,
     "start_time": "2024-06-18T09:39:59.435603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 我猜测线下CV低的原因可能是作者用了对抗学习,负样本label同时为0和1,由于做了数据增强所以也不知道真实的weight_rocauc.\n",
    "\n",
    "### 如果有机会我会看看top2的方案.https://github.com/LoveFishoO/2024-KDD-WhoIsWho"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5129554,
     "sourceId": 8577770,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11197.108981,
   "end_time": "2024-06-18T09:40:03.309546",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-18T06:33:26.200565",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
